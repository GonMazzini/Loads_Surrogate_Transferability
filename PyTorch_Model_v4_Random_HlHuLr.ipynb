{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GonMazzini/Loads_Surrogate_Transferability/blob/main/PyTorch_Model_v4_Random_HlHuLr.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9P0l_d61zRnQ"
      },
      "source": [
        "GridSearch:\n",
        "\n",
        "-This notebook uses a 3 point GridSearch for learning rate (0.1,0.01,0.001) and number of hidden units (16,256,512).\n",
        "-The objective was set to be the validation error (MSE).\n",
        "-The best result was obtained for 512 units and 0.01 lr."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install parameter-sherpa"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "l9b4V5nszS9-",
        "outputId": "326f9836-0a09-4c15-bf01-4c9494aa484c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting parameter-sherpa\n",
            "  Downloading parameter-sherpa-1.0.6.tar.gz (513 kB)\n",
            "\u001b[?25l\r\u001b[K     |▋                               | 10 kB 20.9 MB/s eta 0:00:01\r\u001b[K     |█▎                              | 20 kB 23.7 MB/s eta 0:00:01\r\u001b[K     |██                              | 30 kB 13.4 MB/s eta 0:00:01\r\u001b[K     |██▌                             | 40 kB 10.3 MB/s eta 0:00:01\r\u001b[K     |███▏                            | 51 kB 8.3 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 61 kB 9.6 MB/s eta 0:00:01\r\u001b[K     |████▌                           | 71 kB 9.7 MB/s eta 0:00:01\r\u001b[K     |█████                           | 81 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 92 kB 10.4 MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 102 kB 10.7 MB/s eta 0:00:01\r\u001b[K     |███████                         | 112 kB 10.7 MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 122 kB 10.7 MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 133 kB 10.7 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 143 kB 10.7 MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 153 kB 10.7 MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 163 kB 10.7 MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 174 kB 10.7 MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 184 kB 10.7 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 194 kB 10.7 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 204 kB 10.7 MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 215 kB 10.7 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 225 kB 10.7 MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 235 kB 10.7 MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 245 kB 10.7 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 256 kB 10.7 MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 266 kB 10.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 276 kB 10.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 286 kB 10.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 296 kB 10.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 307 kB 10.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 317 kB 10.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 327 kB 10.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 337 kB 10.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 348 kB 10.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 358 kB 10.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 368 kB 10.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 378 kB 10.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 389 kB 10.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 399 kB 10.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 409 kB 10.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 419 kB 10.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 430 kB 10.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 440 kB 10.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 450 kB 10.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 460 kB 10.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 471 kB 10.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 481 kB 10.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 491 kB 10.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 501 kB 10.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 512 kB 10.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 513 kB 10.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas>=0.20.3 in /usr/local/lib/python3.7/dist-packages (from parameter-sherpa) (1.3.5)\n",
            "Requirement already satisfied: pymongo>=3.5.1 in /usr/local/lib/python3.7/dist-packages (from parameter-sherpa) (4.0.1)\n",
            "Requirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python3.7/dist-packages (from parameter-sherpa) (1.21.5)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from parameter-sherpa) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from parameter-sherpa) (1.0.2)\n",
            "Requirement already satisfied: flask>=0.12.2 in /usr/local/lib/python3.7/dist-packages (from parameter-sherpa) (1.1.4)\n",
            "Collecting GPyOpt>=1.2.5\n",
            "  Downloading GPyOpt-1.2.6.tar.gz (56 kB)\n",
            "\u001b[K     |████████████████████████████████| 56 kB 4.7 MB/s \n",
            "\u001b[?25hCollecting enum34\n",
            "  Downloading enum34-1.1.10-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from parameter-sherpa) (3.2.2)\n",
            "Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from flask>=0.12.2->parameter-sherpa) (1.1.0)\n",
            "Requirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from flask>=0.12.2->parameter-sherpa) (2.11.3)\n",
            "Requirement already satisfied: click<8.0,>=5.1 in /usr/local/lib/python3.7/dist-packages (from flask>=0.12.2->parameter-sherpa) (7.1.2)\n",
            "Requirement already satisfied: Werkzeug<2.0,>=0.15 in /usr/local/lib/python3.7/dist-packages (from flask>=0.12.2->parameter-sherpa) (1.0.1)\n",
            "Collecting GPy>=1.8\n",
            "  Downloading GPy-1.10.0.tar.gz (959 kB)\n",
            "\u001b[K     |████████████████████████████████| 959 kB 30.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from GPy>=1.8->GPyOpt>=1.2.5->parameter-sherpa) (1.15.0)\n",
            "Collecting paramz>=0.9.0\n",
            "  Downloading paramz-0.9.5.tar.gz (71 kB)\n",
            "\u001b[K     |████████████████████████████████| 71 kB 6.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cython>=0.29 in /usr/local/lib/python3.7/dist-packages (from GPy>=1.8->GPyOpt>=1.2.5->parameter-sherpa) (0.29.28)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2<3.0,>=2.10.1->flask>=0.12.2->parameter-sherpa) (2.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.20.3->parameter-sherpa) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.20.3->parameter-sherpa) (2018.9)\n",
            "Requirement already satisfied: decorator>=4.0.10 in /usr/local/lib/python3.7/dist-packages (from paramz>=0.9.0->GPy>=1.8->GPyOpt>=1.2.5->parameter-sherpa) (4.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.19.1->parameter-sherpa) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.19.1->parameter-sherpa) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->parameter-sherpa) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->parameter-sherpa) (3.0.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->parameter-sherpa) (1.3.2)\n",
            "Building wheels for collected packages: parameter-sherpa, GPyOpt, GPy, paramz\n",
            "  Building wheel for parameter-sherpa (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for parameter-sherpa: filename=parameter_sherpa-1.0.6-py2.py3-none-any.whl size=542134 sha256=fc59a8dd6754c7c7e944d6c06ef807473027d6f8e0f25877c1710cb9bce277ae\n",
            "  Stored in directory: /root/.cache/pip/wheels/96/d9/cb/99569566e5e9b3ef0265ba4cbce3ff16f7692988833aa942f5\n",
            "  Building wheel for GPyOpt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for GPyOpt: filename=GPyOpt-1.2.6-py3-none-any.whl size=83609 sha256=9edd2e33ddc259db6220eb4248bfda4113f3aba3edbd76887f9a71ed5563a908\n",
            "  Stored in directory: /root/.cache/pip/wheels/e6/fa/d1/f9652b5af79f769a0ab74dbead7c7aea9a93c6bc74543fd3ec\n",
            "  Building wheel for GPy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for GPy: filename=GPy-1.10.0-cp37-cp37m-linux_x86_64.whl size=2565102 sha256=a6afd1fd850096c663c295aae8d90afdb389e4a0503b124ffc7a04a12f4c4fc5\n",
            "  Stored in directory: /root/.cache/pip/wheels/f7/18/28/dd1ce0192a81b71a3b086fd952511d088b21e8359ea496860a\n",
            "  Building wheel for paramz (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for paramz: filename=paramz-0.9.5-py3-none-any.whl size=102566 sha256=00aebcb5985779e8fc07b2d2bd9f4651b44d783b3301c69a81bb96c9209c9976\n",
            "  Stored in directory: /root/.cache/pip/wheels/c8/95/f5/ce28482da28162e6028c4b3a32c41d147395825b3cd62bc810\n",
            "Successfully built parameter-sherpa GPyOpt GPy paramz\n",
            "Installing collected packages: paramz, GPy, GPyOpt, enum34, parameter-sherpa\n",
            "Successfully installed GPy-1.10.0 GPyOpt-1.2.6 enum34-1.1.10 parameter-sherpa-1.0.6 paramz-0.9.5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "enum"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "import sherpa\n",
        "from sherpa.algorithms import Genetic\n",
        "import time"
      ],
      "metadata": {
        "id": "Q2oZpZQIzoQT"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "nVCWJUJ2zRnS"
      },
      "outputs": [],
      "source": [
        "import pandas as pd \n",
        "import numpy as np\n",
        "import math\n",
        "from random import shuffle\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zm8YCN4AzRnT",
        "outputId": "bf460a1a-5f40-470e-b2c4-afc977b7055c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU\n"
          ]
        }
      ],
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda:0\")\n",
        "    print(\"GPU\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"CPU\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nF2rjMCtzRnU"
      },
      "source": [
        "# 0- Read the data as a data frame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "W1kffmHZzRnV",
        "outputId": "e4504321-8aee-4a08-ae03-9d59f4d6583a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-26768def-897d-4be2-a0ad-8e767910a389\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>pointno</th>\n",
              "      <th>U</th>\n",
              "      <th>SigmaU</th>\n",
              "      <th>Alpha</th>\n",
              "      <th>MannL</th>\n",
              "      <th>MannGamma</th>\n",
              "      <th>VeerDeltaPhi</th>\n",
              "      <th>TT_Mx_avg</th>\n",
              "      <th>TT_My_avg</th>\n",
              "      <th>TB_Mx_avg</th>\n",
              "      <th>TB_My_avg</th>\n",
              "      <th>TT_Mz_avg</th>\n",
              "      <th>MS_Mz_avg</th>\n",
              "      <th>BR_Mx_avg</th>\n",
              "      <th>BR_My_avg</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>-0.650000</td>\n",
              "      <td>7.500000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-22.250000</td>\n",
              "      <td>747.561872</td>\n",
              "      <td>200.666288</td>\n",
              "      <td>6708.717789</td>\n",
              "      <td>8861.885588</td>\n",
              "      <td>819.209904</td>\n",
              "      <td>63.457528</td>\n",
              "      <td>4253.317748</td>\n",
              "      <td>15006.726860</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>10.150758</td>\n",
              "      <td>1.208656</td>\n",
              "      <td>-0.139692</td>\n",
              "      <td>48.470634</td>\n",
              "      <td>1.363636</td>\n",
              "      <td>-4.771217</td>\n",
              "      <td>3556.031457</td>\n",
              "      <td>676.339081</td>\n",
              "      <td>16692.647572</td>\n",
              "      <td>6329.099515</td>\n",
              "      <td>3746.460605</td>\n",
              "      <td>1354.995442</td>\n",
              "      <td>10409.290476</td>\n",
              "      <td>16289.414152</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-26768def-897d-4be2-a0ad-8e767910a389')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-26768def-897d-4be2-a0ad-8e767910a389 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-26768def-897d-4be2-a0ad-8e767910a389');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   Unnamed: 0  pointno          U    SigmaU     Alpha      MannL  MannGamma  \\\n",
              "0           0        1   4.000000  0.100000 -0.650000   7.500000   1.000000   \n",
              "1           1        2  10.150758  1.208656 -0.139692  48.470634   1.363636   \n",
              "\n",
              "   VeerDeltaPhi    TT_Mx_avg   TT_My_avg     TB_Mx_avg    TB_My_avg  \\\n",
              "0    -22.250000   747.561872  200.666288   6708.717789  8861.885588   \n",
              "1     -4.771217  3556.031457  676.339081  16692.647572  6329.099515   \n",
              "\n",
              "     TT_Mz_avg    MS_Mz_avg     BR_Mx_avg     BR_My_avg  \n",
              "0   819.209904    63.457528   4253.317748  15006.726860  \n",
              "1  3746.460605  1354.995442  10409.290476  16289.414152  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "df = pd.read_excel('LoadsDataBase_6D_Set123_FiltMinMaxCrit.xlsx') # Average the values from Set1,Set2 and Set3.\n",
        "df.head(2)\n",
        "# 0 : TT_Mx_avg\n",
        "# 1 : TT_My_avg\n",
        "# 2 : TB_Mx_avg\n",
        "# 3 : TB_My_avg\n",
        "# 4 : MS_Mz_avg\n",
        "# 5 : BR_Mx_avg\n",
        "# 6 : BR_My_avg\n",
        "# 7 : TT-Mz_avg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVi4tQ09zRnV"
      },
      "source": [
        "# ============================================================\n",
        "# ==========================Section 1 ==========================\n",
        "# ============================================================"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "gTxt7cZJzRnV"
      },
      "outputs": [],
      "source": [
        "X = df.iloc[:,2:8]\n",
        "y = df.iloc[:,8:].drop(['TT_Mx_avg','TT_Mz_avg'], axis = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eX_LPh9wzRnW",
        "outputId": "afba6ca3-246c-426d-ebab-832b0ce38c5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The filtered data set consits on: 7664 entries.\n",
            "A total of 6131 will be used for training and validation.\n",
            "A total of 1533 will be used for testing the final model.\n"
          ]
        }
      ],
      "source": [
        "# Test split:\n",
        "X, X_test, y, y_test = train_test_split(X,y, test_size = 0.2, shuffle = True,  random_state = 101)\n",
        "\n",
        "print(f'The filtered data set consits on: {len(df)} entries.')\n",
        "print(f'A total of {len(X)} will be used for training and validation.')\n",
        "print(f'A total of {len(X_test)} will be used for testing the final model.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6sjTHp4zRnW"
      },
      "source": [
        "### From now on, \"X\" and \"y\" will be used for train-validate the model. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "4nBJ9T1TzRnW"
      },
      "outputs": [],
      "source": [
        "feature_range = (0, 1)\n",
        "scaler_x = preprocessing.MinMaxScaler(feature_range=feature_range).fit(X)\n",
        "X_scaled = scaler_x.transform(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fp5EhbtozRnW"
      },
      "source": [
        "### Separte between train and validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "_kR4yaL0zRnX"
      },
      "outputs": [],
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(X_scaled,y.values, test_size = 0.2, shuffle = True,  random_state = 101)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# printing number of samples for train-validation-test\n",
        "print(f'A total of {y_train.shape[0]} for training, {round(100*y_train.shape[0]/len(df),1)} % of total data')\n",
        "print(f'A total of {y_val.shape[0]} for validation, {round(100*y_val.shape[0]/len(df),1)} % of total data')\n",
        "print(f'A total of {y_test.shape[0]} for testing, {round(100*y_test.shape[0]/len(df),1)} % of total data')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ezp2exU1NR36",
        "outputId": "4681824b-92c4-451e-8a99-18c5d4033f0c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A total of 4904 for training, 64.0 % of total data\n",
            "A total of 1227 for validation, 16.0 % of total data\n",
            "A total of 1533 for testing, 20.0 % of total data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Y6KSzGizRnX"
      },
      "source": [
        "### The PyTorch worfklow can be summarized as follows:\n",
        "- 1) Design model (input, output size, forward pass)\n",
        "- 2) Construct loss and optimizer\n",
        "- 3) Training loop\n",
        "   - forward pass: compute prediction based on the current weights and biases of the net\n",
        "   - backward pass: compute the gradients of the loss function wrt. to model parameters\n",
        "   - update weigths in"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "hVfzqEXbzRnX"
      },
      "outputs": [],
      "source": [
        "input_size = np.shape(X_train)[1]             # np.shape(X_train)[1]\n",
        "output_channels = np.shape(y_train)[1]        # np.shape(y_train)[1]\n",
        "#hidden_size = 50"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "    \"\"\" A feedforward network designed for tuning number of layers and hidden units.\n",
        "    By @GonMazzini\"\"\"\n",
        "    def __init__(self, input_dim, output_dim, n_hidLayers, hidden_size):\n",
        "        super(MLP, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.hidden_size = hidden_size\n",
        "        self.n_hidLayers = n_hidLayers\n",
        "        current_dim = input_dim\n",
        "        self.layers = nn.ModuleList()\n",
        "        \n",
        "        for hdim in [self.hidden_size]*self.n_hidLayers:\n",
        "            self.layers.append(nn.Linear(current_dim, hdim))\n",
        "            current_dim = hdim\n",
        "        self.layers.append(nn.Linear(current_dim, output_dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.layers[:-1]:\n",
        "            x = F.relu(layer(x))\n",
        "        out = F.relu(self.layers[-1](x))\n",
        "        return out "
      ],
      "metadata": {
        "id": "QS60F1Q3vBYi"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test dummy forward\n",
        "testMLP = MLP(input_size,output_channels,4,50) \n",
        "testMLP(torch.tensor(X_train[0]).float())"
      ],
      "metadata": {
        "id": "Hr0F9U6hxA1O",
        "outputId": "7ebb2335-0ae5-4d22-bf63-53e4ee13e545",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.0000, 0.0441, 0.0623, 0.0593, 0.0000, 0.1384],\n",
              "       grad_fn=<ReluBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "kQJG0CJ8zRnY"
      },
      "outputs": [],
      "source": [
        "# instantiate the class\n",
        "#model = Net(hidden_size)  # will be instantiated inside HP loop\n",
        "\n",
        "# Define the loss function (mean square error)\n",
        "loss = nn.MSELoss()\n",
        "\n",
        "# Instantiate optimizer passing the net parameters as argument, and learning rate\n",
        "#optimizer = optim.Adam(model.parameters(), lr = 0.01)  # will be instantiated inside HP loop \n",
        "\n",
        "# list to store results\n",
        "train_losses , val_losses= [],[]\n",
        "\n",
        "# Try a dummy forward\n",
        "#model(torch.tensor(X_train[0]).float())  # beware that need to convert from double to float"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xO8OBaiRzRnY"
      },
      "source": [
        "# 2.1- Use the PyTorch DataLoader and Dataset utils.\n",
        "- DataLoader class combines a dataset and a sampler, and provides an iterable over the given dataset for training the model\n",
        "- Dataset: just an abstract class representing a :class:`Dataset`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "guVFdZgMzRnY"
      },
      "outputs": [],
      "source": [
        "class FatigueLoads_TrainSet(Dataset):\n",
        "\n",
        "    def __init__(self):\n",
        "        self.n_samples = X_train.shape[0]\n",
        "        self.x_data = torch.from_numpy(X_train) # size [n_samples, n_features]\n",
        "        self.y_data = torch.from_numpy(y_train) # size [n_samples, 1]\n",
        "\n",
        "    # support indexing such that dataset[i] can be used to get i-th sample\n",
        "    def __getitem__(self, index):\n",
        "        return self.x_data[index], self.y_data[index]\n",
        "\n",
        "    # we can call len(dataset) to return the size\n",
        "    def __len__(self):\n",
        "        return self.n_samples\n",
        "    \n",
        "class FatigueLoads_ValidationSet(Dataset):\n",
        "\n",
        "    def __init__(self):\n",
        "        self.n_samples = X_val.shape[0]\n",
        "        self.x_data = torch.from_numpy(X_val) # size [n_samples, n_features]\n",
        "        self.y_data = torch.from_numpy(y_val) # size [n_samples, 1]\n",
        "\n",
        "    # support indexing such that dataset[i] can be used to get i-th sample\n",
        "    def __getitem__(self, index):\n",
        "        return self.x_data[index], self.y_data[index]\n",
        "\n",
        "    # we can call len(dataset) to return the size\n",
        "    def __len__(self):\n",
        "        return self.n_samples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sp46xHZVzRnZ"
      },
      "source": [
        "### Get first sample and unpack. Note that the enviromental inputs are normalized using MinMaxScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zALPQBILzRnZ",
        "outputId": "f3e6a7d9-0326-4bb2-a26a-899d6da288db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.9213, 0.5395, 0.2971, 0.1574, 0.7369, 0.3075], dtype=torch.float64) tensor([  808.7441, 21966.9989, 16383.7768,   807.4632, 18567.6736, 16935.8620],\n",
            "       dtype=torch.float64)\n"
          ]
        }
      ],
      "source": [
        "train_dataset = FatigueLoads_TrainSet()\n",
        "valid_dataset = FatigueLoads_ValidationSet()\n",
        "\n",
        "first_data = train_dataset[0]\n",
        "features, loads = first_data\n",
        "print(features, loads)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "323d_UnszRnZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7092765a-7e92-4e33-cb61-a0963ec91544"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batches train: 76\n",
            "batches test:  19\n"
          ]
        }
      ],
      "source": [
        "batch_size = 64\n",
        "num_epochs = 1000\n",
        "\n",
        "num_batches_train = X_train.shape[0] // batch_size\n",
        "num_batches_test = X_val.shape[0] // batch_size\n",
        "print(f'batches train: {num_batches_train}')\n",
        "print(f'batches test:  {num_batches_test}')\n",
        "\n",
        "train_loader = DataLoader(dataset=train_dataset,\n",
        "                          batch_size=batch_size,\n",
        "                          shuffle=True,\n",
        "                          num_workers=0)\n",
        "\n",
        "valid_loader = DataLoader(dataset=valid_dataset,\n",
        "                          batch_size=batch_size,\n",
        "                          shuffle=True,\n",
        "                          num_workers=0)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import repeat\n",
        "#algorithm = sherpa.algorithms.GridSearch(num_grid_points=3)\n",
        "algorithm = sherpa.algorithms.RandomSearch(max_num_trials = 32)\n",
        "# algorithm = sherpa.algorithms.RandomSearch(max_num_trials=15)\n",
        "\n",
        "# parameters = [sherpa.Discrete('hidden_size', [16,512]),\n",
        "#               sherpa.Continuous('lr',[0.001,0.1], scale='log')]\n",
        "\n",
        "parameters = [sherpa.Discrete('n_hidLayers', [2, 6]),\n",
        "              sherpa.Discrete('hidden_size', [16, 64]),\n",
        "              sherpa.Continuous('lr',[0.1,0.001],'log')]"
      ],
      "metadata": {
        "id": "5KXldvsU08MV"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "study = sherpa.Study(parameters=parameters,\n",
        "                     algorithm=algorithm,\n",
        "                     lower_is_better=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXK9eWS81rHZ",
        "outputId": "334d1a02-c6ba-41f4-f9ad-942b51ee9204"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:sherpa.core:\n",
            "-------------------------------------------------------\n",
            "SHERPA Dashboard running. Access via\n",
            "http://172.28.0.2:8880 if on a cluster or\n",
            "http://localhost:8880 if running locally.\n",
            "-------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Serving Flask app \"sherpa.app.app\" (lazy loading)\n",
            " * Environment: production\n",
            "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
            "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
            " * Debug mode: on\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for trial in study:\n",
        "  print(\"Trial {}:\\t{}\".format(trial.id, trial.parameters))\n",
        "  # model = Net(trial.parameters['hidden_size'])\n",
        "  model=MLP(input_dim=np.shape(X_train)[1] , \n",
        "            output_dim=np.shape(y_train)[1] ,\n",
        "            n_hidLayers=trial.parameters['n_hidLayers'],\n",
        "            hidden_size=trial.parameters['hidden_size'],\n",
        "            )    \n",
        "  #model.train()\n",
        "  model.to(device)\n",
        "  optimizer = optim.Adam(model.parameters(), lr = trial.parameters['lr']) # lr = trial.parameters['lr']\n",
        "  \n",
        "  for epoch in range(num_epochs):\n",
        "\n",
        "    epoch_loss = 0\n",
        "    model.train()\n",
        "    \n",
        "    for i, (inputs, loads) in enumerate(train_loader):\n",
        "        #print(features, loads)\n",
        "        \n",
        "        optimizer.zero_grad()                      # zeroize accumulated gradients in parameters             \n",
        "        \n",
        "        output = model(inputs.float().to(device))             # forwards pass       \n",
        "        batch_loss = loss(output, loads.float().to(device))   # compute loss for current batch\n",
        "        \n",
        "        batch_loss.backward()                      # compute the gradient of the loss wrt. model parameters\n",
        "        optimizer.step()                           # update weights according to the comptued gradients\n",
        "        \n",
        "    \n",
        "    epoch_loss_train = 0\n",
        "    epoch_loss_test = 0\n",
        "    model.eval()\n",
        "    \n",
        "    ##### Evaluate training\n",
        "    for i, (inputs, loads) in enumerate(train_loader):\n",
        "        \n",
        "        output = model(inputs.float().to(device))\n",
        "        \n",
        "        batch_loss_train = loss(output, loads.float().to(device))  # compute loss for the current batch\n",
        "        epoch_loss_train += batch_loss_train            # accumulate loss for the current epoch\n",
        "        \n",
        "        #print(f'Epoch: {epoch+1}/{num_epochs}  | Step {i+1}/{n_iterations}')\n",
        "    \n",
        "    ##### Evaluate validation    \n",
        "    for i, (inputs, loads) in enumerate(valid_loader):\n",
        "        \n",
        "        output = model(inputs.float().to(device))\n",
        "        \n",
        "        batch_loss_test = loss(output, loads.float().to(device))  # compute loss for the current batch\n",
        "        epoch_loss_test += batch_loss_test     # accumulate loss for the current epoch\n",
        "        \n",
        "        #print(f'Epoch: {epoch+1}/{num_epochs}  | Step {i+1}/{n_iterations}')\n",
        "    \n",
        "    if epoch % 20 == 0: \n",
        "        print(f'Epoch: {epoch+1}/{num_epochs} | Train loss: {0.001*epoch_loss_train/num_batches_train:.0f}       | Val loss {0.001*epoch_loss_test/num_batches_test:.0f}')\n",
        "\n",
        "        study.add_observation(trial=trial,\n",
        "                              iteration=epoch,\n",
        "                              objective=0.001*epoch_loss_test.cpu().detach().numpy()/num_batches_test)\n",
        "        \n",
        "    if study.should_trial_stop(trial):\n",
        "        break \n",
        "    # store in list for plotting the loss per epoch    \n",
        "    val_losses.append(0.001*epoch_loss_test.cpu().detach().numpy()/num_batches_test)  \n",
        "    train_losses.append(0.001*epoch_loss_train.cpu().detach().numpy()/num_batches_train)  \n",
        "\n",
        "  study.finalize(trial) \n",
        "\n",
        "  #study.get_best_result()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GnOKI0762M9u",
        "outputId": "3dddee67-83f3-49c1-b374-60c9d0968b4e"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 1:\t{'n_hidLayers': 5, 'hidden_size': 47, 'lr': 0.0018399878553151303}\n",
            "Epoch: 1/1000 | Train loss: 157242       | Val loss 166274\n",
            "Epoch: 21/1000 | Train loss: 17047       | Val loss 17233\n",
            "Epoch: 41/1000 | Train loss: 16438       | Val loss 16637\n",
            "Epoch: 61/1000 | Train loss: 16332       | Val loss 16622\n",
            "Epoch: 81/1000 | Train loss: 16414       | Val loss 16650\n",
            "Epoch: 101/1000 | Train loss: 16281       | Val loss 16687\n",
            "Epoch: 121/1000 | Train loss: 16204       | Val loss 16586\n",
            "Epoch: 141/1000 | Train loss: 16126       | Val loss 16680\n",
            "Epoch: 161/1000 | Train loss: 16037       | Val loss 16874\n",
            "Epoch: 181/1000 | Train loss: 6790       | Val loss 7023\n",
            "Epoch: 201/1000 | Train loss: 3109       | Val loss 3267\n",
            "Epoch: 221/1000 | Train loss: 2682       | Val loss 2863\n",
            "Epoch: 241/1000 | Train loss: 2627       | Val loss 2804\n",
            "Epoch: 261/1000 | Train loss: 2485       | Val loss 2633\n",
            "Epoch: 281/1000 | Train loss: 2435       | Val loss 2666\n",
            "Epoch: 301/1000 | Train loss: 2324       | Val loss 2489\n",
            "Epoch: 321/1000 | Train loss: 2126       | Val loss 2331\n",
            "Epoch: 341/1000 | Train loss: 2023       | Val loss 2142\n",
            "Epoch: 361/1000 | Train loss: 1802       | Val loss 1988\n",
            "Epoch: 381/1000 | Train loss: 1330       | Val loss 1469\n",
            "Epoch: 401/1000 | Train loss: 1207       | Val loss 1325\n",
            "Epoch: 421/1000 | Train loss: 1170       | Val loss 1331\n",
            "Epoch: 441/1000 | Train loss: 1104       | Val loss 1270\n",
            "Epoch: 461/1000 | Train loss: 1119       | Val loss 1272\n",
            "Epoch: 481/1000 | Train loss: 1046       | Val loss 1183\n",
            "Epoch: 501/1000 | Train loss: 1038       | Val loss 1220\n",
            "Epoch: 521/1000 | Train loss: 1014       | Val loss 1172\n",
            "Epoch: 541/1000 | Train loss: 997       | Val loss 1175\n",
            "Epoch: 561/1000 | Train loss: 968       | Val loss 1100\n",
            "Epoch: 581/1000 | Train loss: 982       | Val loss 1137\n",
            "Epoch: 601/1000 | Train loss: 938       | Val loss 1105\n",
            "Epoch: 621/1000 | Train loss: 977       | Val loss 1116\n",
            "Epoch: 641/1000 | Train loss: 950       | Val loss 1102\n",
            "Epoch: 661/1000 | Train loss: 955       | Val loss 1108\n",
            "Epoch: 681/1000 | Train loss: 928       | Val loss 1072\n",
            "Epoch: 701/1000 | Train loss: 910       | Val loss 1100\n",
            "Epoch: 721/1000 | Train loss: 893       | Val loss 1061\n",
            "Epoch: 741/1000 | Train loss: 884       | Val loss 1069\n",
            "Epoch: 761/1000 | Train loss: 898       | Val loss 1057\n",
            "Epoch: 781/1000 | Train loss: 877       | Val loss 1051\n",
            "Epoch: 801/1000 | Train loss: 862       | Val loss 1019\n",
            "Epoch: 821/1000 | Train loss: 869       | Val loss 1029\n",
            "Epoch: 841/1000 | Train loss: 841       | Val loss 1002\n",
            "Epoch: 861/1000 | Train loss: 897       | Val loss 1086\n",
            "Epoch: 881/1000 | Train loss: 866       | Val loss 1037\n",
            "Epoch: 901/1000 | Train loss: 849       | Val loss 1011\n",
            "Epoch: 921/1000 | Train loss: 846       | Val loss 1005\n",
            "Epoch: 941/1000 | Train loss: 854       | Val loss 1041\n",
            "Epoch: 961/1000 | Train loss: 852       | Val loss 1035\n",
            "Epoch: 981/1000 | Train loss: 821       | Val loss 977\n",
            "Trial 2:\t{'n_hidLayers': 4, 'hidden_size': 39, 'lr': 0.004586448134125279}\n",
            "Epoch: 1/1000 | Train loss: 161716       | Val loss 166566\n",
            "Epoch: 21/1000 | Train loss: 137219       | Val loss 141540\n",
            "Epoch: 41/1000 | Train loss: 136776       | Val loss 140447\n",
            "Epoch: 61/1000 | Train loss: 136822       | Val loss 141295\n",
            "Epoch: 81/1000 | Train loss: 136765       | Val loss 141162\n",
            "Epoch: 101/1000 | Train loss: 136763       | Val loss 140323\n",
            "Epoch: 121/1000 | Train loss: 136607       | Val loss 140049\n",
            "Epoch: 141/1000 | Train loss: 136616       | Val loss 142438\n",
            "Epoch: 161/1000 | Train loss: 136508       | Val loss 140910\n",
            "Epoch: 181/1000 | Train loss: 136518       | Val loss 139628\n",
            "Epoch: 201/1000 | Train loss: 136414       | Val loss 141213\n",
            "Epoch: 221/1000 | Train loss: 136433       | Val loss 140274\n",
            "Epoch: 241/1000 | Train loss: 136492       | Val loss 140088\n",
            "Epoch: 261/1000 | Train loss: 136452       | Val loss 141630\n",
            "Epoch: 281/1000 | Train loss: 136427       | Val loss 140729\n",
            "Epoch: 301/1000 | Train loss: 136386       | Val loss 138848\n",
            "Epoch: 321/1000 | Train loss: 136492       | Val loss 140449\n",
            "Epoch: 341/1000 | Train loss: 136418       | Val loss 141558\n",
            "Epoch: 361/1000 | Train loss: 136348       | Val loss 140944\n",
            "Epoch: 381/1000 | Train loss: 136388       | Val loss 142125\n",
            "Epoch: 401/1000 | Train loss: 136342       | Val loss 140001\n",
            "Epoch: 421/1000 | Train loss: 136312       | Val loss 139201\n",
            "Epoch: 441/1000 | Train loss: 136223       | Val loss 140439\n",
            "Epoch: 461/1000 | Train loss: 136293       | Val loss 139415\n",
            "Epoch: 481/1000 | Train loss: 136399       | Val loss 141142\n",
            "Epoch: 501/1000 | Train loss: 136295       | Val loss 142147\n",
            "Epoch: 521/1000 | Train loss: 136364       | Val loss 139981\n",
            "Epoch: 541/1000 | Train loss: 136391       | Val loss 140672\n",
            "Epoch: 561/1000 | Train loss: 136303       | Val loss 139673\n",
            "Epoch: 581/1000 | Train loss: 136322       | Val loss 139271\n",
            "Epoch: 601/1000 | Train loss: 136427       | Val loss 139813\n",
            "Epoch: 621/1000 | Train loss: 136361       | Val loss 140654\n",
            "Epoch: 641/1000 | Train loss: 136383       | Val loss 140549\n",
            "Epoch: 661/1000 | Train loss: 136249       | Val loss 141276\n",
            "Epoch: 681/1000 | Train loss: 136306       | Val loss 142977\n",
            "Epoch: 701/1000 | Train loss: 136332       | Val loss 140124\n",
            "Epoch: 721/1000 | Train loss: 136354       | Val loss 142333\n",
            "Epoch: 741/1000 | Train loss: 136263       | Val loss 141420\n",
            "Epoch: 761/1000 | Train loss: 136295       | Val loss 140211\n",
            "Epoch: 781/1000 | Train loss: 136288       | Val loss 141279\n",
            "Epoch: 801/1000 | Train loss: 136499       | Val loss 140477\n",
            "Epoch: 821/1000 | Train loss: 136392       | Val loss 140671\n",
            "Epoch: 841/1000 | Train loss: 136323       | Val loss 140457\n",
            "Epoch: 861/1000 | Train loss: 136353       | Val loss 142110\n",
            "Epoch: 881/1000 | Train loss: 136422       | Val loss 141052\n",
            "Epoch: 901/1000 | Train loss: 136422       | Val loss 140966\n",
            "Epoch: 921/1000 | Train loss: 136280       | Val loss 140815\n",
            "Epoch: 941/1000 | Train loss: 136393       | Val loss 142586\n",
            "Epoch: 961/1000 | Train loss: 136378       | Val loss 141679\n",
            "Epoch: 981/1000 | Train loss: 136380       | Val loss 139625\n",
            "Trial 3:\t{'n_hidLayers': 5, 'hidden_size': 33, 'lr': 0.03872221200928398}\n",
            "Epoch: 1/1000 | Train loss: 62030       | Val loss 63847\n",
            "Epoch: 21/1000 | Train loss: 58665       | Val loss 61154\n",
            "Epoch: 41/1000 | Train loss: 58709       | Val loss 60945\n",
            "Epoch: 61/1000 | Train loss: 5895       | Val loss 6039\n",
            "Epoch: 81/1000 | Train loss: 2879       | Val loss 3019\n",
            "Epoch: 101/1000 | Train loss: 1481       | Val loss 1613\n",
            "Epoch: 121/1000 | Train loss: 1751       | Val loss 1947\n",
            "Epoch: 141/1000 | Train loss: 1185       | Val loss 1370\n",
            "Epoch: 161/1000 | Train loss: 1159       | Val loss 1292\n",
            "Epoch: 181/1000 | Train loss: 1336       | Val loss 1532\n",
            "Epoch: 201/1000 | Train loss: 1044       | Val loss 1163\n",
            "Epoch: 221/1000 | Train loss: 989       | Val loss 1180\n",
            "Epoch: 241/1000 | Train loss: 2287       | Val loss 2505\n",
            "Epoch: 261/1000 | Train loss: 1214       | Val loss 1421\n",
            "Epoch: 281/1000 | Train loss: 1482       | Val loss 1667\n",
            "Epoch: 301/1000 | Train loss: 1028       | Val loss 1188\n",
            "Epoch: 321/1000 | Train loss: 931       | Val loss 1141\n",
            "Epoch: 341/1000 | Train loss: 944       | Val loss 1146\n",
            "Epoch: 361/1000 | Train loss: 988       | Val loss 1224\n",
            "Epoch: 381/1000 | Train loss: 957       | Val loss 1150\n",
            "Epoch: 401/1000 | Train loss: 927       | Val loss 1138\n",
            "Epoch: 421/1000 | Train loss: 857       | Val loss 1084\n",
            "Epoch: 441/1000 | Train loss: 931       | Val loss 1153\n",
            "Epoch: 461/1000 | Train loss: 872       | Val loss 1106\n",
            "Epoch: 481/1000 | Train loss: 923       | Val loss 1157\n",
            "Epoch: 501/1000 | Train loss: 936       | Val loss 1159\n",
            "Epoch: 521/1000 | Train loss: 927       | Val loss 1133\n",
            "Epoch: 541/1000 | Train loss: 997       | Val loss 1235\n",
            "Epoch: 561/1000 | Train loss: 1064       | Val loss 1295\n",
            "Epoch: 581/1000 | Train loss: 879       | Val loss 1177\n",
            "Epoch: 601/1000 | Train loss: 835       | Val loss 1100\n",
            "Epoch: 621/1000 | Train loss: 896       | Val loss 1116\n",
            "Epoch: 641/1000 | Train loss: 863       | Val loss 1066\n",
            "Epoch: 661/1000 | Train loss: 801       | Val loss 1011\n",
            "Epoch: 681/1000 | Train loss: 998       | Val loss 1209\n",
            "Epoch: 701/1000 | Train loss: 838       | Val loss 1088\n",
            "Epoch: 721/1000 | Train loss: 903       | Val loss 1142\n",
            "Epoch: 741/1000 | Train loss: 815       | Val loss 1049\n",
            "Epoch: 761/1000 | Train loss: 970       | Val loss 1128\n",
            "Epoch: 781/1000 | Train loss: 1137       | Val loss 1319\n",
            "Epoch: 801/1000 | Train loss: 882       | Val loss 1125\n",
            "Epoch: 821/1000 | Train loss: 865       | Val loss 1102\n",
            "Epoch: 841/1000 | Train loss: 1071       | Val loss 1311\n",
            "Epoch: 861/1000 | Train loss: 905       | Val loss 1210\n",
            "Epoch: 881/1000 | Train loss: 820       | Val loss 1091\n",
            "Epoch: 901/1000 | Train loss: 832       | Val loss 1069\n",
            "Epoch: 921/1000 | Train loss: 842       | Val loss 1086\n",
            "Epoch: 941/1000 | Train loss: 841       | Val loss 1075\n",
            "Epoch: 961/1000 | Train loss: 814       | Val loss 1036\n",
            "Epoch: 981/1000 | Train loss: 846       | Val loss 1082\n",
            "Trial 4:\t{'n_hidLayers': 3, 'hidden_size': 59, 'lr': 0.0019344822624495688}\n",
            "Epoch: 1/1000 | Train loss: 233191       | Val loss 244972\n",
            "Epoch: 21/1000 | Train loss: 18189       | Val loss 18385\n",
            "Epoch: 41/1000 | Train loss: 17383       | Val loss 18012\n",
            "Epoch: 61/1000 | Train loss: 15664       | Val loss 16348\n",
            "Epoch: 81/1000 | Train loss: 8773       | Val loss 9132\n",
            "Epoch: 101/1000 | Train loss: 8149       | Val loss 8315\n",
            "Epoch: 121/1000 | Train loss: 5631       | Val loss 5607\n",
            "Epoch: 141/1000 | Train loss: 4778       | Val loss 4939\n",
            "Epoch: 161/1000 | Train loss: 4374       | Val loss 4388\n",
            "Epoch: 181/1000 | Train loss: 4187       | Val loss 4239\n",
            "Epoch: 201/1000 | Train loss: 3964       | Val loss 4197\n",
            "Epoch: 221/1000 | Train loss: 3461       | Val loss 3460\n",
            "Epoch: 241/1000 | Train loss: 2735       | Val loss 2773\n",
            "Epoch: 261/1000 | Train loss: 2341       | Val loss 2409\n",
            "Epoch: 281/1000 | Train loss: 2087       | Val loss 2260\n",
            "Epoch: 301/1000 | Train loss: 1947       | Val loss 2076\n",
            "Epoch: 321/1000 | Train loss: 1801       | Val loss 1880\n",
            "Epoch: 341/1000 | Train loss: 1635       | Val loss 1713\n",
            "Epoch: 361/1000 | Train loss: 1518       | Val loss 1545\n",
            "Epoch: 381/1000 | Train loss: 1412       | Val loss 1421\n",
            "Epoch: 401/1000 | Train loss: 1383       | Val loss 1421\n",
            "Epoch: 421/1000 | Train loss: 1287       | Val loss 1327\n",
            "Epoch: 441/1000 | Train loss: 1261       | Val loss 1308\n",
            "Epoch: 461/1000 | Train loss: 1236       | Val loss 1243\n",
            "Epoch: 481/1000 | Train loss: 1208       | Val loss 1265\n",
            "Epoch: 501/1000 | Train loss: 1181       | Val loss 1231\n",
            "Epoch: 521/1000 | Train loss: 1160       | Val loss 1194\n",
            "Epoch: 541/1000 | Train loss: 1151       | Val loss 1182\n",
            "Epoch: 561/1000 | Train loss: 1150       | Val loss 1190\n",
            "Epoch: 581/1000 | Train loss: 1117       | Val loss 1153\n",
            "Epoch: 601/1000 | Train loss: 1134       | Val loss 1174\n",
            "Epoch: 621/1000 | Train loss: 1085       | Val loss 1145\n",
            "Epoch: 641/1000 | Train loss: 1092       | Val loss 1246\n",
            "Epoch: 661/1000 | Train loss: 1058       | Val loss 1121\n",
            "Epoch: 681/1000 | Train loss: 1060       | Val loss 1169\n",
            "Epoch: 701/1000 | Train loss: 993       | Val loss 1092\n",
            "Epoch: 721/1000 | Train loss: 969       | Val loss 1068\n",
            "Epoch: 741/1000 | Train loss: 958       | Val loss 1062\n",
            "Epoch: 761/1000 | Train loss: 925       | Val loss 1021\n",
            "Epoch: 781/1000 | Train loss: 887       | Val loss 995\n",
            "Epoch: 801/1000 | Train loss: 853       | Val loss 943\n",
            "Epoch: 821/1000 | Train loss: 837       | Val loss 950\n",
            "Epoch: 841/1000 | Train loss: 825       | Val loss 952\n",
            "Epoch: 861/1000 | Train loss: 787       | Val loss 908\n",
            "Epoch: 881/1000 | Train loss: 773       | Val loss 879\n",
            "Epoch: 901/1000 | Train loss: 752       | Val loss 863\n",
            "Epoch: 921/1000 | Train loss: 741       | Val loss 938\n",
            "Epoch: 941/1000 | Train loss: 716       | Val loss 837\n",
            "Epoch: 961/1000 | Train loss: 694       | Val loss 854\n",
            "Epoch: 981/1000 | Train loss: 686       | Val loss 795\n",
            "Trial 5:\t{'n_hidLayers': 3, 'hidden_size': 58, 'lr': 0.0729085596914963}\n",
            "Epoch: 1/1000 | Train loss: 19988       | Val loss 22177\n",
            "Epoch: 21/1000 | Train loss: 5644       | Val loss 5841\n",
            "Epoch: 41/1000 | Train loss: 2945       | Val loss 3092\n",
            "Epoch: 61/1000 | Train loss: 1806       | Val loss 1941\n",
            "Epoch: 81/1000 | Train loss: 1495       | Val loss 1618\n",
            "Epoch: 101/1000 | Train loss: 1145       | Val loss 1289\n",
            "Epoch: 121/1000 | Train loss: 1057       | Val loss 1213\n",
            "Epoch: 141/1000 | Train loss: 1168       | Val loss 1316\n",
            "Epoch: 161/1000 | Train loss: 979       | Val loss 1174\n",
            "Epoch: 181/1000 | Train loss: 1020       | Val loss 1162\n",
            "Epoch: 201/1000 | Train loss: 958       | Val loss 1122\n",
            "Epoch: 221/1000 | Train loss: 895       | Val loss 1098\n",
            "Epoch: 241/1000 | Train loss: 1011       | Val loss 1186\n",
            "Epoch: 261/1000 | Train loss: 884       | Val loss 1139\n",
            "Epoch: 281/1000 | Train loss: 899       | Val loss 1102\n",
            "Epoch: 301/1000 | Train loss: 839       | Val loss 1078\n",
            "Epoch: 321/1000 | Train loss: 876       | Val loss 1090\n",
            "Epoch: 341/1000 | Train loss: 845       | Val loss 1073\n",
            "Epoch: 361/1000 | Train loss: 851       | Val loss 1082\n",
            "Epoch: 381/1000 | Train loss: 850       | Val loss 1066\n",
            "Epoch: 401/1000 | Train loss: 873       | Val loss 1085\n",
            "Epoch: 421/1000 | Train loss: 817       | Val loss 1144\n",
            "Epoch: 441/1000 | Train loss: 807       | Val loss 1055\n",
            "Epoch: 461/1000 | Train loss: 823       | Val loss 1112\n",
            "Epoch: 481/1000 | Train loss: 791       | Val loss 1031\n",
            "Epoch: 501/1000 | Train loss: 768       | Val loss 990\n",
            "Epoch: 521/1000 | Train loss: 885       | Val loss 1145\n",
            "Epoch: 541/1000 | Train loss: 801       | Val loss 1090\n",
            "Epoch: 561/1000 | Train loss: 1022       | Val loss 1275\n",
            "Epoch: 581/1000 | Train loss: 758       | Val loss 1003\n",
            "Epoch: 601/1000 | Train loss: 841       | Val loss 1064\n",
            "Epoch: 621/1000 | Train loss: 752       | Val loss 985\n",
            "Epoch: 641/1000 | Train loss: 784       | Val loss 1034\n",
            "Epoch: 661/1000 | Train loss: 766       | Val loss 987\n",
            "Epoch: 681/1000 | Train loss: 777       | Val loss 1023\n",
            "Epoch: 701/1000 | Train loss: 784       | Val loss 1037\n",
            "Epoch: 721/1000 | Train loss: 771       | Val loss 1075\n",
            "Epoch: 741/1000 | Train loss: 742       | Val loss 987\n",
            "Epoch: 761/1000 | Train loss: 750       | Val loss 967\n",
            "Epoch: 781/1000 | Train loss: 749       | Val loss 993\n",
            "Epoch: 801/1000 | Train loss: 731       | Val loss 976\n",
            "Epoch: 821/1000 | Train loss: 805       | Val loss 1074\n",
            "Epoch: 841/1000 | Train loss: 733       | Val loss 995\n",
            "Epoch: 861/1000 | Train loss: 725       | Val loss 991\n",
            "Epoch: 881/1000 | Train loss: 760       | Val loss 1006\n",
            "Epoch: 901/1000 | Train loss: 773       | Val loss 1016\n",
            "Epoch: 921/1000 | Train loss: 761       | Val loss 1032\n",
            "Epoch: 941/1000 | Train loss: 731       | Val loss 987\n",
            "Epoch: 961/1000 | Train loss: 770       | Val loss 1010\n",
            "Epoch: 981/1000 | Train loss: 755       | Val loss 1057\n",
            "Trial 6:\t{'n_hidLayers': 3, 'hidden_size': 20, 'lr': 0.0012577950274039135}\n",
            "Epoch: 1/1000 | Train loss: 240300       | Val loss 249193\n",
            "Epoch: 21/1000 | Train loss: 31534       | Val loss 32657\n",
            "Epoch: 41/1000 | Train loss: 19593       | Val loss 20666\n",
            "Epoch: 61/1000 | Train loss: 18332       | Val loss 18772\n",
            "Epoch: 81/1000 | Train loss: 18189       | Val loss 18693\n",
            "Epoch: 101/1000 | Train loss: 18035       | Val loss 18252\n",
            "Epoch: 121/1000 | Train loss: 17642       | Val loss 18601\n",
            "Epoch: 141/1000 | Train loss: 17201       | Val loss 17479\n",
            "Epoch: 161/1000 | Train loss: 16625       | Val loss 16796\n",
            "Epoch: 181/1000 | Train loss: 13228       | Val loss 13659\n",
            "Epoch: 201/1000 | Train loss: 9432       | Val loss 9599\n",
            "Epoch: 221/1000 | Train loss: 8959       | Val loss 9199\n",
            "Epoch: 241/1000 | Train loss: 8780       | Val loss 9084\n",
            "Epoch: 261/1000 | Train loss: 8652       | Val loss 8913\n",
            "Epoch: 281/1000 | Train loss: 8478       | Val loss 8675\n",
            "Epoch: 301/1000 | Train loss: 8216       | Val loss 8424\n",
            "Epoch: 321/1000 | Train loss: 7815       | Val loss 8244\n",
            "Epoch: 341/1000 | Train loss: 6944       | Val loss 7125\n",
            "Epoch: 361/1000 | Train loss: 6493       | Val loss 6465\n",
            "Epoch: 381/1000 | Train loss: 6344       | Val loss 6474\n",
            "Epoch: 401/1000 | Train loss: 6198       | Val loss 6198\n",
            "Epoch: 421/1000 | Train loss: 6062       | Val loss 6079\n",
            "Epoch: 441/1000 | Train loss: 5831       | Val loss 5872\n",
            "Epoch: 461/1000 | Train loss: 5656       | Val loss 5789\n",
            "Epoch: 481/1000 | Train loss: 5559       | Val loss 5706\n",
            "Epoch: 501/1000 | Train loss: 5519       | Val loss 5595\n",
            "Epoch: 521/1000 | Train loss: 5478       | Val loss 5749\n",
            "Epoch: 541/1000 | Train loss: 5422       | Val loss 5720\n",
            "Epoch: 561/1000 | Train loss: 5404       | Val loss 5474\n",
            "Epoch: 581/1000 | Train loss: 5348       | Val loss 5577\n",
            "Epoch: 601/1000 | Train loss: 5299       | Val loss 5401\n",
            "Epoch: 621/1000 | Train loss: 5252       | Val loss 5285\n",
            "Epoch: 641/1000 | Train loss: 5211       | Val loss 5292\n",
            "Epoch: 661/1000 | Train loss: 5190       | Val loss 5508\n",
            "Epoch: 681/1000 | Train loss: 5149       | Val loss 5257\n",
            "Epoch: 701/1000 | Train loss: 5083       | Val loss 5233\n",
            "Epoch: 721/1000 | Train loss: 5036       | Val loss 5462\n",
            "Epoch: 741/1000 | Train loss: 4986       | Val loss 5063\n",
            "Epoch: 761/1000 | Train loss: 4940       | Val loss 4988\n",
            "Epoch: 781/1000 | Train loss: 4886       | Val loss 5033\n",
            "Epoch: 801/1000 | Train loss: 4831       | Val loss 5017\n",
            "Epoch: 821/1000 | Train loss: 4786       | Val loss 4944\n",
            "Epoch: 841/1000 | Train loss: 4730       | Val loss 4890\n",
            "Epoch: 861/1000 | Train loss: 4709       | Val loss 4885\n",
            "Epoch: 881/1000 | Train loss: 4633       | Val loss 4806\n",
            "Epoch: 901/1000 | Train loss: 4587       | Val loss 4689\n",
            "Epoch: 921/1000 | Train loss: 4543       | Val loss 4739\n",
            "Epoch: 941/1000 | Train loss: 4486       | Val loss 4667\n",
            "Epoch: 961/1000 | Train loss: 4450       | Val loss 4524\n",
            "Epoch: 981/1000 | Train loss: 4393       | Val loss 4585\n",
            "Trial 7:\t{'n_hidLayers': 4, 'hidden_size': 60, 'lr': 0.02337891566862916}\n",
            "Epoch: 1/1000 | Train loss: 143710       | Val loss 153740\n",
            "Epoch: 21/1000 | Train loss: 143484       | Val loss 150879\n",
            "Epoch: 41/1000 | Train loss: 142588       | Val loss 150027\n",
            "Epoch: 61/1000 | Train loss: 139304       | Val loss 148344\n",
            "Epoch: 81/1000 | Train loss: 139328       | Val loss 145909\n",
            "Epoch: 101/1000 | Train loss: 139403       | Val loss 148182\n",
            "Epoch: 121/1000 | Train loss: 35497       | Val loss 36992\n",
            "Epoch: 141/1000 | Train loss: 35222       | Val loss 36432\n",
            "Epoch: 161/1000 | Train loss: 35284       | Val loss 37154\n",
            "Epoch: 181/1000 | Train loss: 35189       | Val loss 37459\n",
            "Epoch: 201/1000 | Train loss: 35241       | Val loss 36768\n",
            "Epoch: 221/1000 | Train loss: 1418       | Val loss 1553\n",
            "Epoch: 241/1000 | Train loss: 847       | Val loss 982\n",
            "Epoch: 261/1000 | Train loss: 842       | Val loss 985\n",
            "Epoch: 281/1000 | Train loss: 753       | Val loss 912\n",
            "Epoch: 301/1000 | Train loss: 772       | Val loss 940\n",
            "Epoch: 321/1000 | Train loss: 757       | Val loss 933\n",
            "Epoch: 341/1000 | Train loss: 820       | Val loss 991\n",
            "Epoch: 361/1000 | Train loss: 731       | Val loss 891\n",
            "Epoch: 381/1000 | Train loss: 668       | Val loss 818\n",
            "Epoch: 401/1000 | Train loss: 741       | Val loss 930\n",
            "Epoch: 421/1000 | Train loss: 675       | Val loss 878\n",
            "Epoch: 441/1000 | Train loss: 839       | Val loss 1000\n",
            "Epoch: 461/1000 | Train loss: 638       | Val loss 848\n",
            "Epoch: 481/1000 | Train loss: 666       | Val loss 856\n",
            "Epoch: 501/1000 | Train loss: 658       | Val loss 863\n",
            "Epoch: 521/1000 | Train loss: 643       | Val loss 867\n",
            "Epoch: 541/1000 | Train loss: 723       | Val loss 954\n",
            "Epoch: 561/1000 | Train loss: 616       | Val loss 829\n",
            "Epoch: 581/1000 | Train loss: 619       | Val loss 867\n",
            "Epoch: 601/1000 | Train loss: 608       | Val loss 847\n",
            "Epoch: 621/1000 | Train loss: 605       | Val loss 803\n",
            "Epoch: 641/1000 | Train loss: 608       | Val loss 836\n",
            "Epoch: 661/1000 | Train loss: 685       | Val loss 892\n",
            "Epoch: 681/1000 | Train loss: 636       | Val loss 889\n",
            "Epoch: 701/1000 | Train loss: 672       | Val loss 918\n",
            "Epoch: 721/1000 | Train loss: 570       | Val loss 803\n",
            "Epoch: 741/1000 | Train loss: 562       | Val loss 817\n",
            "Epoch: 761/1000 | Train loss: 784       | Val loss 1047\n",
            "Epoch: 781/1000 | Train loss: 601       | Val loss 855\n",
            "Epoch: 801/1000 | Train loss: 622       | Val loss 859\n",
            "Epoch: 821/1000 | Train loss: 598       | Val loss 832\n",
            "Epoch: 841/1000 | Train loss: 587       | Val loss 823\n",
            "Epoch: 861/1000 | Train loss: 565       | Val loss 810\n",
            "Epoch: 881/1000 | Train loss: 629       | Val loss 894\n",
            "Epoch: 901/1000 | Train loss: 630       | Val loss 875\n",
            "Epoch: 921/1000 | Train loss: 591       | Val loss 856\n",
            "Epoch: 941/1000 | Train loss: 573       | Val loss 842\n",
            "Epoch: 961/1000 | Train loss: 565       | Val loss 851\n",
            "Epoch: 981/1000 | Train loss: 571       | Val loss 856\n",
            "Trial 8:\t{'n_hidLayers': 5, 'hidden_size': 34, 'lr': 0.04175839322512321}\n",
            "Epoch: 1/1000 | Train loss: 20375       | Val loss 20867\n",
            "Epoch: 21/1000 | Train loss: 17015       | Val loss 17403\n",
            "Epoch: 41/1000 | Train loss: 17029       | Val loss 17423\n",
            "Epoch: 61/1000 | Train loss: 3668       | Val loss 3993\n",
            "Epoch: 81/1000 | Train loss: 3140       | Val loss 3368\n",
            "Epoch: 101/1000 | Train loss: 3209       | Val loss 3455\n",
            "Epoch: 121/1000 | Train loss: 1348       | Val loss 1569\n",
            "Epoch: 141/1000 | Train loss: 1404       | Val loss 1632\n",
            "Epoch: 161/1000 | Train loss: 1169       | Val loss 1357\n",
            "Epoch: 181/1000 | Train loss: 1052       | Val loss 1254\n",
            "Epoch: 201/1000 | Train loss: 1171       | Val loss 1381\n",
            "Epoch: 221/1000 | Train loss: 1174       | Val loss 1367\n",
            "Epoch: 241/1000 | Train loss: 1190       | Val loss 1386\n",
            "Epoch: 261/1000 | Train loss: 1469       | Val loss 1630\n",
            "Epoch: 281/1000 | Train loss: 937       | Val loss 1128\n",
            "Epoch: 301/1000 | Train loss: 1010       | Val loss 1204\n",
            "Epoch: 321/1000 | Train loss: 976       | Val loss 1195\n",
            "Epoch: 341/1000 | Train loss: 953       | Val loss 1178\n",
            "Epoch: 361/1000 | Train loss: 918       | Val loss 1180\n",
            "Epoch: 381/1000 | Train loss: 1051       | Val loss 1197\n",
            "Epoch: 401/1000 | Train loss: 1006       | Val loss 1226\n",
            "Epoch: 421/1000 | Train loss: 972       | Val loss 1187\n",
            "Epoch: 441/1000 | Train loss: 1218       | Val loss 1384\n",
            "Epoch: 461/1000 | Train loss: 1037       | Val loss 1239\n",
            "Epoch: 481/1000 | Train loss: 899       | Val loss 1137\n",
            "Epoch: 501/1000 | Train loss: 977       | Val loss 1184\n",
            "Epoch: 521/1000 | Train loss: 947       | Val loss 1124\n",
            "Epoch: 541/1000 | Train loss: 850       | Val loss 1069\n",
            "Epoch: 561/1000 | Train loss: 848       | Val loss 1068\n",
            "Epoch: 581/1000 | Train loss: 860       | Val loss 1058\n",
            "Epoch: 601/1000 | Train loss: 981       | Val loss 1188\n",
            "Epoch: 621/1000 | Train loss: 904       | Val loss 1125\n",
            "Epoch: 641/1000 | Train loss: 898       | Val loss 1151\n",
            "Epoch: 661/1000 | Train loss: 986       | Val loss 1184\n",
            "Epoch: 681/1000 | Train loss: 909       | Val loss 1145\n",
            "Epoch: 701/1000 | Train loss: 899       | Val loss 1119\n",
            "Epoch: 721/1000 | Train loss: 854       | Val loss 1105\n",
            "Epoch: 741/1000 | Train loss: 868       | Val loss 1098\n",
            "Epoch: 761/1000 | Train loss: 879       | Val loss 1107\n",
            "Epoch: 781/1000 | Train loss: 833       | Val loss 1059\n",
            "Epoch: 801/1000 | Train loss: 1021       | Val loss 1292\n",
            "Epoch: 821/1000 | Train loss: 866       | Val loss 1074\n",
            "Epoch: 841/1000 | Train loss: 896       | Val loss 1088\n",
            "Epoch: 861/1000 | Train loss: 860       | Val loss 1046\n",
            "Epoch: 881/1000 | Train loss: 1055       | Val loss 1228\n",
            "Epoch: 901/1000 | Train loss: 833       | Val loss 1051\n",
            "Epoch: 921/1000 | Train loss: 855       | Val loss 1060\n",
            "Epoch: 941/1000 | Train loss: 851       | Val loss 1087\n",
            "Epoch: 961/1000 | Train loss: 846       | Val loss 1081\n",
            "Epoch: 981/1000 | Train loss: 891       | Val loss 1104\n",
            "Trial 9:\t{'n_hidLayers': 3, 'hidden_size': 42, 'lr': 0.02147548450186802}\n",
            "Epoch: 1/1000 | Train loss: 121048       | Val loss 127012\n",
            "Epoch: 21/1000 | Train loss: 116259       | Val loss 123696\n",
            "Epoch: 41/1000 | Train loss: 10545       | Val loss 10650\n",
            "Epoch: 61/1000 | Train loss: 3929       | Val loss 4321\n",
            "Epoch: 81/1000 | Train loss: 2379       | Val loss 2498\n",
            "Epoch: 101/1000 | Train loss: 1840       | Val loss 1911\n",
            "Epoch: 121/1000 | Train loss: 1593       | Val loss 1733\n",
            "Epoch: 141/1000 | Train loss: 1485       | Val loss 1629\n",
            "Epoch: 161/1000 | Train loss: 1345       | Val loss 1450\n",
            "Epoch: 181/1000 | Train loss: 1299       | Val loss 1438\n",
            "Epoch: 201/1000 | Train loss: 1238       | Val loss 1395\n",
            "Epoch: 221/1000 | Train loss: 1296       | Val loss 1440\n",
            "Epoch: 241/1000 | Train loss: 1211       | Val loss 1372\n",
            "Epoch: 261/1000 | Train loss: 1166       | Val loss 1313\n",
            "Epoch: 281/1000 | Train loss: 1114       | Val loss 1230\n",
            "Epoch: 301/1000 | Train loss: 1077       | Val loss 1266\n",
            "Epoch: 321/1000 | Train loss: 1080       | Val loss 1210\n",
            "Epoch: 341/1000 | Train loss: 1071       | Val loss 1206\n",
            "Epoch: 361/1000 | Train loss: 1042       | Val loss 1200\n",
            "Epoch: 381/1000 | Train loss: 1090       | Val loss 1200\n",
            "Epoch: 401/1000 | Train loss: 1060       | Val loss 1231\n",
            "Epoch: 421/1000 | Train loss: 953       | Val loss 1115\n",
            "Epoch: 441/1000 | Train loss: 976       | Val loss 1156\n",
            "Epoch: 461/1000 | Train loss: 1039       | Val loss 1213\n",
            "Epoch: 481/1000 | Train loss: 963       | Val loss 1145\n",
            "Epoch: 501/1000 | Train loss: 959       | Val loss 1111\n",
            "Epoch: 521/1000 | Train loss: 946       | Val loss 1101\n",
            "Epoch: 541/1000 | Train loss: 932       | Val loss 1108\n",
            "Epoch: 561/1000 | Train loss: 1000       | Val loss 1170\n",
            "Epoch: 581/1000 | Train loss: 948       | Val loss 1120\n",
            "Epoch: 601/1000 | Train loss: 925       | Val loss 1062\n",
            "Epoch: 621/1000 | Train loss: 897       | Val loss 1045\n",
            "Epoch: 641/1000 | Train loss: 907       | Val loss 1094\n",
            "Epoch: 661/1000 | Train loss: 916       | Val loss 1115\n",
            "Epoch: 681/1000 | Train loss: 908       | Val loss 1095\n",
            "Epoch: 701/1000 | Train loss: 901       | Val loss 1058\n",
            "Epoch: 721/1000 | Train loss: 889       | Val loss 1074\n",
            "Epoch: 741/1000 | Train loss: 902       | Val loss 1076\n",
            "Epoch: 761/1000 | Train loss: 981       | Val loss 1150\n",
            "Epoch: 781/1000 | Train loss: 909       | Val loss 1050\n",
            "Epoch: 801/1000 | Train loss: 966       | Val loss 1139\n",
            "Epoch: 821/1000 | Train loss: 901       | Val loss 1087\n",
            "Epoch: 841/1000 | Train loss: 981       | Val loss 1168\n",
            "Epoch: 861/1000 | Train loss: 932       | Val loss 1110\n",
            "Epoch: 881/1000 | Train loss: 884       | Val loss 1038\n",
            "Epoch: 901/1000 | Train loss: 865       | Val loss 1046\n",
            "Epoch: 921/1000 | Train loss: 918       | Val loss 1096\n",
            "Epoch: 941/1000 | Train loss: 885       | Val loss 1052\n",
            "Epoch: 961/1000 | Train loss: 988       | Val loss 1159\n",
            "Epoch: 981/1000 | Train loss: 879       | Val loss 1076\n",
            "Trial 10:\t{'n_hidLayers': 5, 'hidden_size': 60, 'lr': 0.008330449459227422}\n",
            "Epoch: 1/1000 | Train loss: 105517       | Val loss 106977\n",
            "Epoch: 21/1000 | Train loss: 95345       | Val loss 97874\n",
            "Epoch: 41/1000 | Train loss: 95162       | Val loss 97864\n",
            "Epoch: 61/1000 | Train loss: 95151       | Val loss 99363\n",
            "Epoch: 81/1000 | Train loss: 4822       | Val loss 4987\n",
            "Epoch: 101/1000 | Train loss: 2743       | Val loss 2995\n",
            "Epoch: 121/1000 | Train loss: 2688       | Val loss 2889\n",
            "Epoch: 141/1000 | Train loss: 2764       | Val loss 3018\n",
            "Epoch: 161/1000 | Train loss: 1470       | Val loss 1655\n",
            "Epoch: 181/1000 | Train loss: 1094       | Val loss 1274\n",
            "Epoch: 201/1000 | Train loss: 1094       | Val loss 1243\n",
            "Epoch: 221/1000 | Train loss: 1013       | Val loss 1166\n",
            "Epoch: 241/1000 | Train loss: 1017       | Val loss 1175\n",
            "Epoch: 261/1000 | Train loss: 982       | Val loss 1107\n",
            "Epoch: 281/1000 | Train loss: 995       | Val loss 1189\n",
            "Epoch: 301/1000 | Train loss: 1139       | Val loss 1288\n",
            "Epoch: 321/1000 | Train loss: 974       | Val loss 1121\n",
            "Epoch: 341/1000 | Train loss: 1012       | Val loss 1213\n",
            "Epoch: 361/1000 | Train loss: 924       | Val loss 1101\n",
            "Epoch: 381/1000 | Train loss: 906       | Val loss 1093\n",
            "Epoch: 401/1000 | Train loss: 1003       | Val loss 1175\n",
            "Epoch: 421/1000 | Train loss: 835       | Val loss 1022\n",
            "Epoch: 441/1000 | Train loss: 839       | Val loss 1039\n",
            "Epoch: 461/1000 | Train loss: 851       | Val loss 1052\n",
            "Epoch: 481/1000 | Train loss: 818       | Val loss 988\n",
            "Epoch: 501/1000 | Train loss: 832       | Val loss 1015\n",
            "Epoch: 521/1000 | Train loss: 832       | Val loss 1043\n",
            "Epoch: 541/1000 | Train loss: 827       | Val loss 1032\n",
            "Epoch: 561/1000 | Train loss: 793       | Val loss 1013\n",
            "Epoch: 581/1000 | Train loss: 796       | Val loss 986\n",
            "Epoch: 601/1000 | Train loss: 769       | Val loss 968\n",
            "Epoch: 621/1000 | Train loss: 799       | Val loss 1015\n",
            "Epoch: 641/1000 | Train loss: 802       | Val loss 1000\n",
            "Epoch: 661/1000 | Train loss: 764       | Val loss 955\n",
            "Epoch: 681/1000 | Train loss: 799       | Val loss 1001\n",
            "Epoch: 701/1000 | Train loss: 761       | Val loss 979\n",
            "Epoch: 721/1000 | Train loss: 745       | Val loss 927\n",
            "Epoch: 741/1000 | Train loss: 834       | Val loss 1062\n",
            "Epoch: 761/1000 | Train loss: 739       | Val loss 954\n",
            "Epoch: 781/1000 | Train loss: 748       | Val loss 980\n",
            "Epoch: 801/1000 | Train loss: 735       | Val loss 941\n",
            "Epoch: 821/1000 | Train loss: 761       | Val loss 973\n",
            "Epoch: 841/1000 | Train loss: 754       | Val loss 1012\n",
            "Epoch: 861/1000 | Train loss: 762       | Val loss 985\n",
            "Epoch: 881/1000 | Train loss: 732       | Val loss 971\n",
            "Epoch: 901/1000 | Train loss: 725       | Val loss 950\n",
            "Epoch: 921/1000 | Train loss: 797       | Val loss 1015\n",
            "Epoch: 941/1000 | Train loss: 752       | Val loss 964\n",
            "Epoch: 961/1000 | Train loss: 774       | Val loss 1005\n",
            "Epoch: 981/1000 | Train loss: 765       | Val loss 984\n",
            "Trial 11:\t{'n_hidLayers': 5, 'hidden_size': 57, 'lr': 0.003738419574330746}\n",
            "Epoch: 1/1000 | Train loss: 86563       | Val loss 87377\n",
            "Epoch: 21/1000 | Train loss: 65154       | Val loss 66343\n",
            "Epoch: 41/1000 | Train loss: 64866       | Val loss 67810\n",
            "Epoch: 61/1000 | Train loss: 64698       | Val loss 65321\n",
            "Epoch: 81/1000 | Train loss: 64541       | Val loss 65714\n",
            "Epoch: 101/1000 | Train loss: 63043       | Val loss 64053\n",
            "Epoch: 121/1000 | Train loss: 5763       | Val loss 6260\n",
            "Epoch: 141/1000 | Train loss: 5363       | Val loss 5727\n",
            "Epoch: 161/1000 | Train loss: 2950       | Val loss 3141\n",
            "Epoch: 181/1000 | Train loss: 2592       | Val loss 2743\n",
            "Epoch: 201/1000 | Train loss: 2564       | Val loss 2778\n",
            "Epoch: 221/1000 | Train loss: 2281       | Val loss 2486\n",
            "Epoch: 241/1000 | Train loss: 1473       | Val loss 1670\n",
            "Epoch: 261/1000 | Train loss: 1192       | Val loss 1394\n",
            "Epoch: 281/1000 | Train loss: 1151       | Val loss 1338\n",
            "Epoch: 301/1000 | Train loss: 1091       | Val loss 1267\n",
            "Epoch: 321/1000 | Train loss: 1108       | Val loss 1293\n",
            "Epoch: 341/1000 | Train loss: 1186       | Val loss 1353\n",
            "Epoch: 361/1000 | Train loss: 1040       | Val loss 1258\n",
            "Epoch: 381/1000 | Train loss: 1008       | Val loss 1194\n",
            "Epoch: 401/1000 | Train loss: 1155       | Val loss 1346\n",
            "Epoch: 421/1000 | Train loss: 1084       | Val loss 1297\n",
            "Epoch: 441/1000 | Train loss: 1070       | Val loss 1262\n",
            "Epoch: 461/1000 | Train loss: 1049       | Val loss 1261\n",
            "Epoch: 481/1000 | Train loss: 1050       | Val loss 1258\n",
            "Epoch: 501/1000 | Train loss: 956       | Val loss 1178\n",
            "Epoch: 521/1000 | Train loss: 1091       | Val loss 1328\n",
            "Epoch: 541/1000 | Train loss: 925       | Val loss 1138\n",
            "Epoch: 561/1000 | Train loss: 927       | Val loss 1203\n",
            "Epoch: 581/1000 | Train loss: 896       | Val loss 1121\n",
            "Epoch: 601/1000 | Train loss: 937       | Val loss 1139\n",
            "Epoch: 621/1000 | Train loss: 911       | Val loss 1126\n",
            "Epoch: 641/1000 | Train loss: 941       | Val loss 1159\n",
            "Epoch: 661/1000 | Train loss: 926       | Val loss 1110\n",
            "Epoch: 681/1000 | Train loss: 921       | Val loss 1118\n",
            "Epoch: 701/1000 | Train loss: 920       | Val loss 1141\n",
            "Epoch: 721/1000 | Train loss: 891       | Val loss 1090\n",
            "Epoch: 741/1000 | Train loss: 875       | Val loss 1113\n",
            "Epoch: 761/1000 | Train loss: 876       | Val loss 1102\n",
            "Epoch: 781/1000 | Train loss: 872       | Val loss 1126\n",
            "Epoch: 801/1000 | Train loss: 866       | Val loss 1073\n",
            "Epoch: 821/1000 | Train loss: 842       | Val loss 1110\n",
            "Epoch: 841/1000 | Train loss: 868       | Val loss 1066\n",
            "Epoch: 861/1000 | Train loss: 852       | Val loss 1087\n",
            "Epoch: 881/1000 | Train loss: 838       | Val loss 1071\n",
            "Epoch: 901/1000 | Train loss: 818       | Val loss 1050\n",
            "Epoch: 921/1000 | Train loss: 836       | Val loss 1065\n",
            "Epoch: 941/1000 | Train loss: 857       | Val loss 1108\n",
            "Epoch: 961/1000 | Train loss: 840       | Val loss 1071\n",
            "Epoch: 981/1000 | Train loss: 812       | Val loss 1028\n",
            "Trial 12:\t{'n_hidLayers': 2, 'hidden_size': 29, 'lr': 0.06373182639453308}\n",
            "Epoch: 1/1000 | Train loss: 166595       | Val loss 176044\n",
            "Epoch: 21/1000 | Train loss: 160481       | Val loss 169585\n",
            "Epoch: 41/1000 | Train loss: 4570       | Val loss 4566\n",
            "Epoch: 61/1000 | Train loss: 2233       | Val loss 2216\n",
            "Epoch: 81/1000 | Train loss: 1814       | Val loss 1856\n",
            "Epoch: 101/1000 | Train loss: 1690       | Val loss 1705\n",
            "Epoch: 121/1000 | Train loss: 1515       | Val loss 1551\n",
            "Epoch: 141/1000 | Train loss: 1470       | Val loss 1520\n",
            "Epoch: 161/1000 | Train loss: 1489       | Val loss 1539\n",
            "Epoch: 181/1000 | Train loss: 1421       | Val loss 1541\n",
            "Epoch: 201/1000 | Train loss: 1483       | Val loss 1568\n",
            "Epoch: 221/1000 | Train loss: 1300       | Val loss 1325\n",
            "Epoch: 241/1000 | Train loss: 1159       | Val loss 1222\n",
            "Epoch: 261/1000 | Train loss: 1054       | Val loss 1131\n",
            "Epoch: 281/1000 | Train loss: 1040       | Val loss 1134\n",
            "Epoch: 301/1000 | Train loss: 1017       | Val loss 1127\n",
            "Epoch: 321/1000 | Train loss: 1003       | Val loss 1129\n",
            "Epoch: 341/1000 | Train loss: 971       | Val loss 1138\n",
            "Epoch: 361/1000 | Train loss: 899       | Val loss 1069\n",
            "Epoch: 381/1000 | Train loss: 864       | Val loss 1023\n",
            "Epoch: 401/1000 | Train loss: 992       | Val loss 1116\n",
            "Epoch: 421/1000 | Train loss: 846       | Val loss 966\n",
            "Epoch: 441/1000 | Train loss: 874       | Val loss 1009\n",
            "Epoch: 461/1000 | Train loss: 834       | Val loss 949\n",
            "Epoch: 481/1000 | Train loss: 824       | Val loss 956\n",
            "Epoch: 501/1000 | Train loss: 839       | Val loss 958\n",
            "Epoch: 521/1000 | Train loss: 843       | Val loss 948\n",
            "Epoch: 541/1000 | Train loss: 831       | Val loss 939\n",
            "Epoch: 561/1000 | Train loss: 970       | Val loss 1116\n",
            "Epoch: 581/1000 | Train loss: 799       | Val loss 938\n",
            "Epoch: 601/1000 | Train loss: 790       | Val loss 917\n",
            "Epoch: 621/1000 | Train loss: 784       | Val loss 917\n",
            "Epoch: 641/1000 | Train loss: 806       | Val loss 965\n",
            "Epoch: 661/1000 | Train loss: 762       | Val loss 911\n",
            "Epoch: 681/1000 | Train loss: 777       | Val loss 891\n",
            "Epoch: 701/1000 | Train loss: 755       | Val loss 930\n",
            "Epoch: 721/1000 | Train loss: 861       | Val loss 1019\n",
            "Epoch: 741/1000 | Train loss: 864       | Val loss 970\n",
            "Epoch: 761/1000 | Train loss: 732       | Val loss 850\n",
            "Epoch: 781/1000 | Train loss: 766       | Val loss 870\n",
            "Epoch: 801/1000 | Train loss: 685       | Val loss 810\n",
            "Epoch: 821/1000 | Train loss: 714       | Val loss 838\n",
            "Epoch: 841/1000 | Train loss: 685       | Val loss 840\n",
            "Epoch: 861/1000 | Train loss: 736       | Val loss 850\n",
            "Epoch: 881/1000 | Train loss: 761       | Val loss 897\n",
            "Epoch: 901/1000 | Train loss: 723       | Val loss 875\n",
            "Epoch: 921/1000 | Train loss: 697       | Val loss 816\n",
            "Epoch: 941/1000 | Train loss: 707       | Val loss 862\n",
            "Epoch: 961/1000 | Train loss: 681       | Val loss 840\n",
            "Epoch: 981/1000 | Train loss: 710       | Val loss 849\n",
            "Trial 13:\t{'n_hidLayers': 2, 'hidden_size': 29, 'lr': 0.08145779641052157}\n",
            "Epoch: 1/1000 | Train loss: 81083       | Val loss 83624\n",
            "Epoch: 21/1000 | Train loss: 58014       | Val loss 59185\n",
            "Epoch: 41/1000 | Train loss: 56690       | Val loss 57140\n",
            "Epoch: 61/1000 | Train loss: 56636       | Val loss 57544\n",
            "Epoch: 81/1000 | Train loss: 56651       | Val loss 57073\n",
            "Epoch: 101/1000 | Train loss: 56553       | Val loss 57072\n",
            "Epoch: 121/1000 | Train loss: 56523       | Val loss 57071\n",
            "Epoch: 141/1000 | Train loss: 56650       | Val loss 57480\n",
            "Epoch: 161/1000 | Train loss: 56273       | Val loss 57349\n",
            "Epoch: 181/1000 | Train loss: 56306       | Val loss 56972\n",
            "Epoch: 201/1000 | Train loss: 56279       | Val loss 57209\n",
            "Epoch: 221/1000 | Train loss: 55959       | Val loss 56977\n",
            "Epoch: 241/1000 | Train loss: 55692       | Val loss 56471\n",
            "Epoch: 261/1000 | Train loss: 55389       | Val loss 55821\n",
            "Epoch: 281/1000 | Train loss: 5299       | Val loss 5472\n",
            "Epoch: 301/1000 | Train loss: 1773       | Val loss 1823\n",
            "Epoch: 321/1000 | Train loss: 1615       | Val loss 1772\n",
            "Epoch: 341/1000 | Train loss: 1382       | Val loss 1512\n",
            "Epoch: 361/1000 | Train loss: 1311       | Val loss 1471\n",
            "Epoch: 381/1000 | Train loss: 1361       | Val loss 1541\n",
            "Epoch: 401/1000 | Train loss: 1377       | Val loss 1517\n",
            "Epoch: 421/1000 | Train loss: 1377       | Val loss 1521\n",
            "Epoch: 441/1000 | Train loss: 1221       | Val loss 1355\n",
            "Epoch: 461/1000 | Train loss: 1233       | Val loss 1378\n",
            "Epoch: 481/1000 | Train loss: 1202       | Val loss 1389\n",
            "Epoch: 501/1000 | Train loss: 1197       | Val loss 1340\n",
            "Epoch: 521/1000 | Train loss: 1194       | Val loss 1443\n",
            "Epoch: 541/1000 | Train loss: 1169       | Val loss 1347\n",
            "Epoch: 561/1000 | Train loss: 1116       | Val loss 1274\n",
            "Epoch: 581/1000 | Train loss: 1138       | Val loss 1296\n",
            "Epoch: 601/1000 | Train loss: 1126       | Val loss 1299\n",
            "Epoch: 621/1000 | Train loss: 1089       | Val loss 1237\n",
            "Epoch: 641/1000 | Train loss: 1092       | Val loss 1274\n",
            "Epoch: 661/1000 | Train loss: 1147       | Val loss 1307\n",
            "Epoch: 681/1000 | Train loss: 1041       | Val loss 1203\n",
            "Epoch: 701/1000 | Train loss: 1066       | Val loss 1231\n",
            "Epoch: 721/1000 | Train loss: 1054       | Val loss 1214\n",
            "Epoch: 741/1000 | Train loss: 1012       | Val loss 1158\n",
            "Epoch: 761/1000 | Train loss: 1037       | Val loss 1203\n",
            "Epoch: 781/1000 | Train loss: 1055       | Val loss 1216\n",
            "Epoch: 801/1000 | Train loss: 1000       | Val loss 1151\n",
            "Epoch: 821/1000 | Train loss: 1009       | Val loss 1198\n",
            "Epoch: 841/1000 | Train loss: 1023       | Val loss 1181\n",
            "Epoch: 861/1000 | Train loss: 1058       | Val loss 1224\n",
            "Epoch: 881/1000 | Train loss: 1106       | Val loss 1273\n",
            "Epoch: 901/1000 | Train loss: 974       | Val loss 1112\n",
            "Epoch: 921/1000 | Train loss: 1041       | Val loss 1220\n",
            "Epoch: 941/1000 | Train loss: 1000       | Val loss 1151\n",
            "Epoch: 961/1000 | Train loss: 1018       | Val loss 1182\n",
            "Epoch: 981/1000 | Train loss: 1024       | Val loss 1209\n",
            "Trial 14:\t{'n_hidLayers': 5, 'hidden_size': 45, 'lr': 0.03395862127842433}\n",
            "Epoch: 1/1000 | Train loss: 116935       | Val loss 123803\n",
            "Epoch: 21/1000 | Train loss: 115781       | Val loss 124544\n",
            "Epoch: 41/1000 | Train loss: 115470       | Val loss 119841\n",
            "Epoch: 61/1000 | Train loss: 4140       | Val loss 4412\n",
            "Epoch: 81/1000 | Train loss: 1438       | Val loss 1775\n",
            "Epoch: 101/1000 | Train loss: 1296       | Val loss 1394\n",
            "Epoch: 121/1000 | Train loss: 1131       | Val loss 1282\n",
            "Epoch: 141/1000 | Train loss: 1000       | Val loss 1112\n",
            "Epoch: 161/1000 | Train loss: 1356       | Val loss 1480\n",
            "Epoch: 181/1000 | Train loss: 1035       | Val loss 1210\n",
            "Epoch: 201/1000 | Train loss: 991       | Val loss 1106\n",
            "Epoch: 221/1000 | Train loss: 900       | Val loss 1057\n",
            "Epoch: 241/1000 | Train loss: 895       | Val loss 1159\n",
            "Epoch: 261/1000 | Train loss: 947       | Val loss 1144\n",
            "Epoch: 281/1000 | Train loss: 853       | Val loss 1064\n",
            "Epoch: 301/1000 | Train loss: 905       | Val loss 1107\n",
            "Epoch: 321/1000 | Train loss: 822       | Val loss 991\n",
            "Epoch: 341/1000 | Train loss: 814       | Val loss 1038\n",
            "Epoch: 361/1000 | Train loss: 799       | Val loss 1010\n",
            "Epoch: 381/1000 | Train loss: 889       | Val loss 1150\n",
            "Epoch: 401/1000 | Train loss: 797       | Val loss 999\n",
            "Epoch: 421/1000 | Train loss: 1005       | Val loss 1267\n",
            "Epoch: 441/1000 | Train loss: 846       | Val loss 1045\n",
            "Epoch: 461/1000 | Train loss: 859       | Val loss 1162\n",
            "Epoch: 481/1000 | Train loss: 842       | Val loss 1023\n",
            "Epoch: 501/1000 | Train loss: 808       | Val loss 1047\n",
            "Epoch: 521/1000 | Train loss: 889       | Val loss 1113\n",
            "Epoch: 541/1000 | Train loss: 812       | Val loss 1088\n",
            "Epoch: 561/1000 | Train loss: 784       | Val loss 1011\n",
            "Epoch: 581/1000 | Train loss: 877       | Val loss 1110\n",
            "Epoch: 601/1000 | Train loss: 830       | Val loss 1044\n",
            "Epoch: 621/1000 | Train loss: 797       | Val loss 1024\n",
            "Epoch: 641/1000 | Train loss: 816       | Val loss 1062\n",
            "Epoch: 661/1000 | Train loss: 1023       | Val loss 1313\n",
            "Epoch: 681/1000 | Train loss: 890       | Val loss 1210\n",
            "Epoch: 701/1000 | Train loss: 794       | Val loss 1059\n",
            "Epoch: 721/1000 | Train loss: 811       | Val loss 1070\n",
            "Epoch: 741/1000 | Train loss: 818       | Val loss 1106\n",
            "Epoch: 761/1000 | Train loss: 876       | Val loss 1121\n",
            "Epoch: 781/1000 | Train loss: 724       | Val loss 944\n",
            "Epoch: 801/1000 | Train loss: 848       | Val loss 1126\n",
            "Epoch: 821/1000 | Train loss: 734       | Val loss 1015\n",
            "Epoch: 841/1000 | Train loss: 814       | Val loss 1073\n",
            "Epoch: 861/1000 | Train loss: 800       | Val loss 1055\n",
            "Epoch: 881/1000 | Train loss: 769       | Val loss 1022\n",
            "Epoch: 901/1000 | Train loss: 778       | Val loss 1018\n",
            "Epoch: 921/1000 | Train loss: 771       | Val loss 999\n",
            "Epoch: 941/1000 | Train loss: 798       | Val loss 1095\n",
            "Epoch: 961/1000 | Train loss: 769       | Val loss 1022\n",
            "Epoch: 981/1000 | Train loss: 766       | Val loss 1100\n",
            "Trial 15:\t{'n_hidLayers': 5, 'hidden_size': 53, 'lr': 0.04078152864919068}\n",
            "Epoch: 1/1000 | Train loss: 18849       | Val loss 19278\n",
            "Epoch: 21/1000 | Train loss: 17337       | Val loss 18865\n",
            "Epoch: 41/1000 | Train loss: 2727       | Val loss 2930\n",
            "Epoch: 61/1000 | Train loss: 1274       | Val loss 1447\n",
            "Epoch: 81/1000 | Train loss: 894       | Val loss 1041\n",
            "Epoch: 101/1000 | Train loss: 808       | Val loss 975\n",
            "Epoch: 121/1000 | Train loss: 639       | Val loss 791\n",
            "Epoch: 141/1000 | Train loss: 682       | Val loss 804\n",
            "Epoch: 161/1000 | Train loss: 705       | Val loss 836\n",
            "Epoch: 181/1000 | Train loss: 505       | Val loss 651\n",
            "Epoch: 201/1000 | Train loss: 731       | Val loss 905\n",
            "Epoch: 221/1000 | Train loss: 695       | Val loss 846\n",
            "Epoch: 241/1000 | Train loss: 524       | Val loss 658\n",
            "Epoch: 261/1000 | Train loss: 525       | Val loss 709\n",
            "Epoch: 281/1000 | Train loss: 545       | Val loss 735\n",
            "Epoch: 301/1000 | Train loss: 447       | Val loss 642\n",
            "Epoch: 321/1000 | Train loss: 664       | Val loss 812\n",
            "Epoch: 341/1000 | Train loss: 419       | Val loss 580\n",
            "Epoch: 361/1000 | Train loss: 504       | Val loss 717\n",
            "Epoch: 381/1000 | Train loss: 462       | Val loss 660\n",
            "Epoch: 401/1000 | Train loss: 673       | Val loss 910\n",
            "Epoch: 421/1000 | Train loss: 541       | Val loss 748\n",
            "Epoch: 441/1000 | Train loss: 412       | Val loss 566\n",
            "Epoch: 461/1000 | Train loss: 584       | Val loss 811\n",
            "Epoch: 481/1000 | Train loss: 450       | Val loss 655\n",
            "Epoch: 501/1000 | Train loss: 439       | Val loss 645\n",
            "Epoch: 521/1000 | Train loss: 703       | Val loss 896\n",
            "Epoch: 541/1000 | Train loss: 418       | Val loss 719\n",
            "Epoch: 561/1000 | Train loss: 420       | Val loss 625\n",
            "Epoch: 581/1000 | Train loss: 480       | Val loss 697\n",
            "Epoch: 601/1000 | Train loss: 606       | Val loss 804\n",
            "Epoch: 621/1000 | Train loss: 406       | Val loss 628\n",
            "Epoch: 641/1000 | Train loss: 424       | Val loss 628\n",
            "Epoch: 661/1000 | Train loss: 507       | Val loss 730\n",
            "Epoch: 681/1000 | Train loss: 407       | Val loss 584\n",
            "Epoch: 701/1000 | Train loss: 486       | Val loss 668\n",
            "Epoch: 721/1000 | Train loss: 494       | Val loss 694\n",
            "Epoch: 741/1000 | Train loss: 490       | Val loss 662\n",
            "Epoch: 761/1000 | Train loss: 424       | Val loss 651\n",
            "Epoch: 781/1000 | Train loss: 520       | Val loss 753\n",
            "Epoch: 801/1000 | Train loss: 404       | Val loss 609\n",
            "Epoch: 821/1000 | Train loss: 420       | Val loss 628\n",
            "Epoch: 841/1000 | Train loss: 558       | Val loss 758\n",
            "Epoch: 861/1000 | Train loss: 467       | Val loss 697\n",
            "Epoch: 881/1000 | Train loss: 399       | Val loss 638\n",
            "Epoch: 901/1000 | Train loss: 439       | Val loss 642\n",
            "Epoch: 921/1000 | Train loss: 467       | Val loss 655\n",
            "Epoch: 941/1000 | Train loss: 548       | Val loss 777\n",
            "Epoch: 961/1000 | Train loss: 525       | Val loss 767\n",
            "Epoch: 981/1000 | Train loss: 403       | Val loss 716\n",
            "Trial 16:\t{'n_hidLayers': 4, 'hidden_size': 60, 'lr': 0.06792008605860615}\n",
            "Epoch: 1/1000 | Train loss: 90242       | Val loss 94304\n",
            "Epoch: 21/1000 | Train loss: 88771       | Val loss 92155\n",
            "Epoch: 41/1000 | Train loss: 38046       | Val loss 39698\n",
            "Epoch: 61/1000 | Train loss: 1935       | Val loss 2279\n",
            "Epoch: 81/1000 | Train loss: 1306       | Val loss 1507\n",
            "Epoch: 101/1000 | Train loss: 1146       | Val loss 1337\n",
            "Epoch: 121/1000 | Train loss: 1153       | Val loss 1333\n",
            "Epoch: 141/1000 | Train loss: 1196       | Val loss 1327\n",
            "Epoch: 161/1000 | Train loss: 1047       | Val loss 1271\n",
            "Epoch: 181/1000 | Train loss: 1091       | Val loss 1281\n",
            "Epoch: 201/1000 | Train loss: 1015       | Val loss 1236\n",
            "Epoch: 221/1000 | Train loss: 1149       | Val loss 1366\n",
            "Epoch: 241/1000 | Train loss: 892       | Val loss 1115\n",
            "Epoch: 261/1000 | Train loss: 1013       | Val loss 1214\n",
            "Epoch: 281/1000 | Train loss: 889       | Val loss 1114\n",
            "Epoch: 301/1000 | Train loss: 1126       | Val loss 1386\n",
            "Epoch: 321/1000 | Train loss: 818       | Val loss 1061\n",
            "Epoch: 341/1000 | Train loss: 914       | Val loss 1164\n",
            "Epoch: 361/1000 | Train loss: 867       | Val loss 1113\n",
            "Epoch: 381/1000 | Train loss: 887       | Val loss 1061\n",
            "Epoch: 401/1000 | Train loss: 923       | Val loss 1132\n",
            "Epoch: 421/1000 | Train loss: 842       | Val loss 1069\n",
            "Epoch: 441/1000 | Train loss: 846       | Val loss 1074\n",
            "Epoch: 461/1000 | Train loss: 902       | Val loss 1152\n",
            "Epoch: 481/1000 | Train loss: 904       | Val loss 1194\n",
            "Epoch: 501/1000 | Train loss: 793       | Val loss 1074\n",
            "Epoch: 521/1000 | Train loss: 910       | Val loss 1120\n",
            "Epoch: 541/1000 | Train loss: 909       | Val loss 1175\n",
            "Epoch: 561/1000 | Train loss: 1021       | Val loss 1213\n",
            "Epoch: 581/1000 | Train loss: 910       | Val loss 1175\n",
            "Epoch: 601/1000 | Train loss: 831       | Val loss 1099\n",
            "Epoch: 621/1000 | Train loss: 849       | Val loss 1151\n",
            "Epoch: 641/1000 | Train loss: 896       | Val loss 1188\n",
            "Epoch: 661/1000 | Train loss: 888       | Val loss 1143\n",
            "Epoch: 681/1000 | Train loss: 891       | Val loss 1113\n",
            "Epoch: 701/1000 | Train loss: 918       | Val loss 1198\n",
            "Epoch: 721/1000 | Train loss: 868       | Val loss 1109\n",
            "Epoch: 741/1000 | Train loss: 866       | Val loss 1102\n",
            "Epoch: 761/1000 | Train loss: 801       | Val loss 1053\n",
            "Epoch: 781/1000 | Train loss: 919       | Val loss 1201\n",
            "Epoch: 801/1000 | Train loss: 788       | Val loss 1037\n",
            "Epoch: 821/1000 | Train loss: 786       | Val loss 1036\n",
            "Epoch: 841/1000 | Train loss: 889       | Val loss 1120\n",
            "Epoch: 861/1000 | Train loss: 806       | Val loss 1075\n",
            "Epoch: 881/1000 | Train loss: 851       | Val loss 1113\n",
            "Epoch: 901/1000 | Train loss: 886       | Val loss 1135\n",
            "Epoch: 921/1000 | Train loss: 760       | Val loss 1009\n",
            "Epoch: 941/1000 | Train loss: 806       | Val loss 1089\n",
            "Epoch: 961/1000 | Train loss: 905       | Val loss 1142\n",
            "Epoch: 981/1000 | Train loss: 836       | Val loss 1094\n",
            "Trial 17:\t{'n_hidLayers': 5, 'hidden_size': 51, 'lr': 0.0010438924357788332}\n",
            "Epoch: 1/1000 | Train loss: 238475       | Val loss 252357\n",
            "Epoch: 21/1000 | Train loss: 165293       | Val loss 175944\n",
            "Epoch: 41/1000 | Train loss: 164846       | Val loss 173989\n",
            "Epoch: 61/1000 | Train loss: 164881       | Val loss 172685\n",
            "Epoch: 81/1000 | Train loss: 164980       | Val loss 171441\n",
            "Epoch: 101/1000 | Train loss: 165011       | Val loss 172048\n",
            "Epoch: 121/1000 | Train loss: 164958       | Val loss 170651\n",
            "Epoch: 141/1000 | Train loss: 164740       | Val loss 171118\n",
            "Epoch: 161/1000 | Train loss: 159011       | Val loss 167100\n",
            "Epoch: 181/1000 | Train loss: 159039       | Val loss 164582\n",
            "Epoch: 201/1000 | Train loss: 158958       | Val loss 167580\n",
            "Epoch: 221/1000 | Train loss: 158849       | Val loss 164987\n",
            "Epoch: 241/1000 | Train loss: 158952       | Val loss 164761\n",
            "Epoch: 261/1000 | Train loss: 158777       | Val loss 165615\n",
            "Epoch: 281/1000 | Train loss: 158690       | Val loss 165240\n",
            "Epoch: 301/1000 | Train loss: 158528       | Val loss 164000\n",
            "Epoch: 321/1000 | Train loss: 158628       | Val loss 167317\n",
            "Epoch: 341/1000 | Train loss: 158436       | Val loss 163136\n",
            "Epoch: 361/1000 | Train loss: 158316       | Val loss 165646\n",
            "Epoch: 381/1000 | Train loss: 158290       | Val loss 165059\n",
            "Epoch: 401/1000 | Train loss: 158086       | Val loss 165802\n",
            "Epoch: 421/1000 | Train loss: 158177       | Val loss 164762\n",
            "Epoch: 441/1000 | Train loss: 158154       | Val loss 163523\n",
            "Epoch: 461/1000 | Train loss: 157981       | Val loss 168959\n",
            "Epoch: 481/1000 | Train loss: 158206       | Val loss 165688\n",
            "Epoch: 501/1000 | Train loss: 158096       | Val loss 163180\n",
            "Epoch: 521/1000 | Train loss: 158224       | Val loss 167307\n",
            "Epoch: 541/1000 | Train loss: 158145       | Val loss 164827\n",
            "Epoch: 561/1000 | Train loss: 158140       | Val loss 162702\n",
            "Epoch: 581/1000 | Train loss: 158079       | Val loss 164121\n",
            "Epoch: 601/1000 | Train loss: 158048       | Val loss 166968\n",
            "Epoch: 621/1000 | Train loss: 158056       | Val loss 162559\n",
            "Epoch: 641/1000 | Train loss: 157997       | Val loss 163568\n",
            "Epoch: 661/1000 | Train loss: 158203       | Val loss 167105\n",
            "Epoch: 681/1000 | Train loss: 158175       | Val loss 164239\n",
            "Epoch: 701/1000 | Train loss: 158025       | Val loss 163744\n",
            "Epoch: 721/1000 | Train loss: 158147       | Val loss 166109\n",
            "Epoch: 741/1000 | Train loss: 158032       | Val loss 161596\n",
            "Epoch: 761/1000 | Train loss: 158034       | Val loss 163880\n",
            "Epoch: 781/1000 | Train loss: 158041       | Val loss 162979\n",
            "Epoch: 801/1000 | Train loss: 157994       | Val loss 165626\n",
            "Epoch: 821/1000 | Train loss: 158096       | Val loss 164477\n",
            "Epoch: 841/1000 | Train loss: 158001       | Val loss 165327\n",
            "Epoch: 861/1000 | Train loss: 158090       | Val loss 163919\n",
            "Epoch: 881/1000 | Train loss: 157998       | Val loss 166927\n",
            "Epoch: 901/1000 | Train loss: 157997       | Val loss 162923\n",
            "Epoch: 921/1000 | Train loss: 158028       | Val loss 166238\n",
            "Epoch: 941/1000 | Train loss: 157974       | Val loss 164513\n",
            "Epoch: 961/1000 | Train loss: 158017       | Val loss 164409\n",
            "Epoch: 981/1000 | Train loss: 158155       | Val loss 168039\n",
            "Trial 18:\t{'n_hidLayers': 5, 'hidden_size': 37, 'lr': 0.03266473993859904}\n",
            "Epoch: 1/1000 | Train loss: 117388       | Val loss 124411\n",
            "Epoch: 21/1000 | Train loss: 116605       | Val loss 121660\n",
            "Epoch: 41/1000 | Train loss: 116225       | Val loss 122506\n",
            "Epoch: 61/1000 | Train loss: 3850       | Val loss 3977\n",
            "Epoch: 81/1000 | Train loss: 2990       | Val loss 3146\n",
            "Epoch: 101/1000 | Train loss: 1652       | Val loss 1995\n",
            "Epoch: 121/1000 | Train loss: 1222       | Val loss 1350\n",
            "Epoch: 141/1000 | Train loss: 1304       | Val loss 1518\n",
            "Epoch: 161/1000 | Train loss: 1148       | Val loss 1299\n",
            "Epoch: 181/1000 | Train loss: 1156       | Val loss 1317\n",
            "Epoch: 201/1000 | Train loss: 1308       | Val loss 1460\n",
            "Epoch: 221/1000 | Train loss: 1013       | Val loss 1215\n",
            "Epoch: 241/1000 | Train loss: 1025       | Val loss 1175\n",
            "Epoch: 261/1000 | Train loss: 972       | Val loss 1150\n",
            "Epoch: 281/1000 | Train loss: 1050       | Val loss 1191\n",
            "Epoch: 301/1000 | Train loss: 918       | Val loss 1071\n",
            "Epoch: 321/1000 | Train loss: 951       | Val loss 1149\n",
            "Epoch: 341/1000 | Train loss: 1433       | Val loss 1676\n",
            "Epoch: 361/1000 | Train loss: 888       | Val loss 1099\n",
            "Epoch: 381/1000 | Train loss: 934       | Val loss 1145\n",
            "Epoch: 401/1000 | Train loss: 959       | Val loss 1191\n",
            "Epoch: 421/1000 | Train loss: 1077       | Val loss 1271\n",
            "Epoch: 441/1000 | Train loss: 848       | Val loss 1046\n",
            "Epoch: 461/1000 | Train loss: 924       | Val loss 1102\n",
            "Epoch: 481/1000 | Train loss: 915       | Val loss 1092\n",
            "Epoch: 501/1000 | Train loss: 834       | Val loss 1014\n",
            "Epoch: 521/1000 | Train loss: 909       | Val loss 1102\n",
            "Epoch: 541/1000 | Train loss: 1013       | Val loss 1285\n",
            "Epoch: 561/1000 | Train loss: 974       | Val loss 1195\n",
            "Epoch: 581/1000 | Train loss: 865       | Val loss 1074\n",
            "Epoch: 601/1000 | Train loss: 805       | Val loss 1028\n",
            "Epoch: 621/1000 | Train loss: 838       | Val loss 1048\n",
            "Epoch: 641/1000 | Train loss: 861       | Val loss 1067\n",
            "Epoch: 661/1000 | Train loss: 905       | Val loss 1186\n",
            "Epoch: 681/1000 | Train loss: 895       | Val loss 1099\n",
            "Epoch: 701/1000 | Train loss: 831       | Val loss 1034\n",
            "Epoch: 721/1000 | Train loss: 895       | Val loss 1118\n",
            "Epoch: 741/1000 | Train loss: 881       | Val loss 1123\n",
            "Epoch: 761/1000 | Train loss: 814       | Val loss 1056\n",
            "Epoch: 781/1000 | Train loss: 795       | Val loss 1012\n",
            "Epoch: 801/1000 | Train loss: 838       | Val loss 1090\n",
            "Epoch: 821/1000 | Train loss: 775       | Val loss 1016\n",
            "Epoch: 841/1000 | Train loss: 853       | Val loss 1079\n",
            "Epoch: 861/1000 | Train loss: 865       | Val loss 1094\n",
            "Epoch: 881/1000 | Train loss: 935       | Val loss 1165\n",
            "Epoch: 901/1000 | Train loss: 953       | Val loss 1139\n",
            "Epoch: 921/1000 | Train loss: 789       | Val loss 1002\n",
            "Epoch: 941/1000 | Train loss: 1019       | Val loss 1279\n",
            "Epoch: 961/1000 | Train loss: 942       | Val loss 1170\n",
            "Epoch: 981/1000 | Train loss: 776       | Val loss 1020\n",
            "Trial 19:\t{'n_hidLayers': 3, 'hidden_size': 18, 'lr': 0.0015831574766812114}\n",
            "Epoch: 1/1000 | Train loss: 240311       | Val loss 252212\n",
            "Epoch: 21/1000 | Train loss: 74966       | Val loss 77935\n",
            "Epoch: 41/1000 | Train loss: 67718       | Val loss 69055\n",
            "Epoch: 61/1000 | Train loss: 67281       | Val loss 68535\n",
            "Epoch: 81/1000 | Train loss: 66771       | Val loss 68752\n",
            "Epoch: 101/1000 | Train loss: 65897       | Val loss 67433\n",
            "Epoch: 121/1000 | Train loss: 65403       | Val loss 66188\n",
            "Epoch: 141/1000 | Train loss: 65302       | Val loss 66989\n",
            "Epoch: 161/1000 | Train loss: 65261       | Val loss 67166\n",
            "Epoch: 181/1000 | Train loss: 65217       | Val loss 66884\n",
            "Epoch: 201/1000 | Train loss: 65205       | Val loss 66330\n",
            "Epoch: 221/1000 | Train loss: 65225       | Val loss 65895\n",
            "Epoch: 241/1000 | Train loss: 65164       | Val loss 66327\n",
            "Epoch: 261/1000 | Train loss: 65180       | Val loss 66868\n",
            "Epoch: 281/1000 | Train loss: 65151       | Val loss 66171\n",
            "Epoch: 301/1000 | Train loss: 65122       | Val loss 66109\n",
            "Epoch: 321/1000 | Train loss: 65053       | Val loss 65926\n",
            "Epoch: 341/1000 | Train loss: 64900       | Val loss 65950\n",
            "Epoch: 361/1000 | Train loss: 64372       | Val loss 66239\n",
            "Epoch: 381/1000 | Train loss: 58898       | Val loss 59478\n",
            "Epoch: 401/1000 | Train loss: 58350       | Val loss 59350\n",
            "Epoch: 421/1000 | Train loss: 58160       | Val loss 58858\n",
            "Epoch: 441/1000 | Train loss: 58227       | Val loss 59057\n",
            "Epoch: 461/1000 | Train loss: 58207       | Val loss 59279\n",
            "Epoch: 481/1000 | Train loss: 58156       | Val loss 59478\n",
            "Epoch: 501/1000 | Train loss: 58135       | Val loss 58122\n",
            "Epoch: 521/1000 | Train loss: 58161       | Val loss 59966\n",
            "Epoch: 541/1000 | Train loss: 58149       | Val loss 58912\n",
            "Epoch: 561/1000 | Train loss: 58239       | Val loss 59213\n",
            "Epoch: 581/1000 | Train loss: 58118       | Val loss 58907\n",
            "Epoch: 601/1000 | Train loss: 58102       | Val loss 59316\n",
            "Epoch: 621/1000 | Train loss: 58048       | Val loss 59141\n",
            "Epoch: 641/1000 | Train loss: 57807       | Val loss 58547\n",
            "Epoch: 661/1000 | Train loss: 57334       | Val loss 59623\n",
            "Epoch: 681/1000 | Train loss: 56926       | Val loss 58180\n",
            "Epoch: 701/1000 | Train loss: 56777       | Val loss 57255\n",
            "Epoch: 721/1000 | Train loss: 56748       | Val loss 57616\n",
            "Epoch: 741/1000 | Train loss: 56778       | Val loss 57349\n",
            "Epoch: 761/1000 | Train loss: 56696       | Val loss 57011\n",
            "Epoch: 781/1000 | Train loss: 56646       | Val loss 57395\n",
            "Epoch: 801/1000 | Train loss: 56765       | Val loss 57813\n",
            "Epoch: 821/1000 | Train loss: 56688       | Val loss 57767\n",
            "Epoch: 841/1000 | Train loss: 56670       | Val loss 57864\n",
            "Epoch: 861/1000 | Train loss: 56584       | Val loss 56873\n",
            "Epoch: 881/1000 | Train loss: 56505       | Val loss 57076\n",
            "Epoch: 901/1000 | Train loss: 56336       | Val loss 57385\n",
            "Epoch: 921/1000 | Train loss: 56115       | Val loss 56594\n",
            "Epoch: 941/1000 | Train loss: 55956       | Val loss 57750\n",
            "Epoch: 961/1000 | Train loss: 55828       | Val loss 56717\n",
            "Epoch: 981/1000 | Train loss: 55828       | Val loss 56803\n",
            "Trial 20:\t{'n_hidLayers': 5, 'hidden_size': 48, 'lr': 0.002709620569004853}\n",
            "Epoch: 1/1000 | Train loss: 171312       | Val loss 179191\n",
            "Epoch: 21/1000 | Train loss: 164776       | Val loss 171352\n",
            "Epoch: 41/1000 | Train loss: 164906       | Val loss 173534\n",
            "Epoch: 61/1000 | Train loss: 164823       | Val loss 169628\n",
            "Epoch: 81/1000 | Train loss: 164971       | Val loss 177198\n",
            "Epoch: 101/1000 | Train loss: 164834       | Val loss 170909\n",
            "Epoch: 121/1000 | Train loss: 164967       | Val loss 171276\n",
            "Epoch: 141/1000 | Train loss: 158988       | Val loss 163399\n",
            "Epoch: 161/1000 | Train loss: 159263       | Val loss 166650\n",
            "Epoch: 181/1000 | Train loss: 159142       | Val loss 167278\n",
            "Epoch: 201/1000 | Train loss: 158999       | Val loss 163857\n",
            "Epoch: 221/1000 | Train loss: 158493       | Val loss 164302\n",
            "Epoch: 241/1000 | Train loss: 158044       | Val loss 167026\n",
            "Epoch: 261/1000 | Train loss: 158086       | Val loss 167379\n",
            "Epoch: 281/1000 | Train loss: 158143       | Val loss 168179\n",
            "Epoch: 301/1000 | Train loss: 158154       | Val loss 166491\n",
            "Epoch: 321/1000 | Train loss: 157966       | Val loss 164749\n",
            "Epoch: 341/1000 | Train loss: 158115       | Val loss 161635\n",
            "Epoch: 361/1000 | Train loss: 157958       | Val loss 165461\n",
            "Epoch: 381/1000 | Train loss: 157928       | Val loss 165731\n",
            "Epoch: 401/1000 | Train loss: 158006       | Val loss 169317\n",
            "Epoch: 421/1000 | Train loss: 157924       | Val loss 162600\n",
            "Epoch: 441/1000 | Train loss: 158055       | Val loss 168252\n",
            "Epoch: 461/1000 | Train loss: 158065       | Val loss 162329\n",
            "Epoch: 481/1000 | Train loss: 158179       | Val loss 164643\n",
            "Epoch: 501/1000 | Train loss: 157949       | Val loss 164638\n",
            "Epoch: 521/1000 | Train loss: 157990       | Val loss 164294\n",
            "Epoch: 541/1000 | Train loss: 157972       | Val loss 163533\n",
            "Epoch: 561/1000 | Train loss: 158101       | Val loss 164755\n",
            "Epoch: 581/1000 | Train loss: 157821       | Val loss 164554\n",
            "Epoch: 601/1000 | Train loss: 158000       | Val loss 161963\n",
            "Epoch: 621/1000 | Train loss: 157828       | Val loss 166642\n",
            "Epoch: 641/1000 | Train loss: 157968       | Val loss 165734\n",
            "Epoch: 661/1000 | Train loss: 157964       | Val loss 162898\n",
            "Epoch: 681/1000 | Train loss: 157818       | Val loss 166605\n",
            "Epoch: 701/1000 | Train loss: 157865       | Val loss 166016\n",
            "Epoch: 721/1000 | Train loss: 157842       | Val loss 163681\n",
            "Epoch: 741/1000 | Train loss: 158030       | Val loss 165789\n",
            "Epoch: 761/1000 | Train loss: 157844       | Val loss 164363\n",
            "Epoch: 781/1000 | Train loss: 158074       | Val loss 164787\n",
            "Epoch: 801/1000 | Train loss: 157797       | Val loss 167276\n",
            "Epoch: 821/1000 | Train loss: 157857       | Val loss 167908\n",
            "Epoch: 841/1000 | Train loss: 157731       | Val loss 164338\n",
            "Epoch: 861/1000 | Train loss: 157949       | Val loss 164422\n",
            "Epoch: 881/1000 | Train loss: 157879       | Val loss 162937\n",
            "Epoch: 901/1000 | Train loss: 157927       | Val loss 166734\n",
            "Epoch: 921/1000 | Train loss: 157989       | Val loss 166800\n",
            "Epoch: 941/1000 | Train loss: 157805       | Val loss 167393\n",
            "Epoch: 961/1000 | Train loss: 157997       | Val loss 162872\n",
            "Epoch: 981/1000 | Train loss: 157742       | Val loss 165319\n",
            "Trial 21:\t{'n_hidLayers': 5, 'hidden_size': 17, 'lr': 0.021561446534815225}\n",
            "Epoch: 1/1000 | Train loss: 79853       | Val loss 81860\n",
            "Epoch: 21/1000 | Train loss: 65418       | Val loss 66783\n",
            "Epoch: 41/1000 | Train loss: 65314       | Val loss 67868\n",
            "Epoch: 61/1000 | Train loss: 58475       | Val loss 59037\n",
            "Epoch: 81/1000 | Train loss: 58474       | Val loss 58763\n",
            "Epoch: 101/1000 | Train loss: 58680       | Val loss 59757\n",
            "Epoch: 121/1000 | Train loss: 58462       | Val loss 58937\n",
            "Epoch: 141/1000 | Train loss: 58407       | Val loss 58820\n",
            "Epoch: 161/1000 | Train loss: 58304       | Val loss 59222\n",
            "Epoch: 181/1000 | Train loss: 57943       | Val loss 58560\n",
            "Epoch: 201/1000 | Train loss: 57858       | Val loss 58897\n",
            "Epoch: 221/1000 | Train loss: 57447       | Val loss 58379\n",
            "Epoch: 241/1000 | Train loss: 57211       | Val loss 58180\n",
            "Epoch: 261/1000 | Train loss: 55397       | Val loss 56612\n",
            "Epoch: 281/1000 | Train loss: 55153       | Val loss 55139\n",
            "Epoch: 301/1000 | Train loss: 54937       | Val loss 55851\n",
            "Epoch: 321/1000 | Train loss: 54914       | Val loss 55694\n",
            "Epoch: 341/1000 | Train loss: 54846       | Val loss 55404\n",
            "Epoch: 361/1000 | Train loss: 54725       | Val loss 55294\n",
            "Epoch: 381/1000 | Train loss: 1817       | Val loss 2082\n",
            "Epoch: 401/1000 | Train loss: 1484       | Val loss 1682\n",
            "Epoch: 421/1000 | Train loss: 1499       | Val loss 1754\n",
            "Epoch: 441/1000 | Train loss: 1275       | Val loss 1416\n",
            "Epoch: 461/1000 | Train loss: 1243       | Val loss 1383\n",
            "Epoch: 481/1000 | Train loss: 1233       | Val loss 1358\n",
            "Epoch: 501/1000 | Train loss: 1345       | Val loss 1595\n",
            "Epoch: 521/1000 | Train loss: 1247       | Val loss 1450\n",
            "Epoch: 541/1000 | Train loss: 1262       | Val loss 1437\n",
            "Epoch: 561/1000 | Train loss: 1236       | Val loss 1424\n",
            "Epoch: 581/1000 | Train loss: 1234       | Val loss 1380\n",
            "Epoch: 601/1000 | Train loss: 1221       | Val loss 1363\n",
            "Epoch: 621/1000 | Train loss: 1391       | Val loss 1578\n",
            "Epoch: 641/1000 | Train loss: 1181       | Val loss 1324\n",
            "Epoch: 661/1000 | Train loss: 1111       | Val loss 1287\n",
            "Epoch: 681/1000 | Train loss: 1132       | Val loss 1357\n",
            "Epoch: 701/1000 | Train loss: 1255       | Val loss 1402\n",
            "Epoch: 721/1000 | Train loss: 1094       | Val loss 1247\n",
            "Epoch: 741/1000 | Train loss: 1153       | Val loss 1350\n",
            "Epoch: 761/1000 | Train loss: 1220       | Val loss 1408\n",
            "Epoch: 781/1000 | Train loss: 1091       | Val loss 1254\n",
            "Epoch: 801/1000 | Train loss: 1036       | Val loss 1179\n",
            "Epoch: 821/1000 | Train loss: 1127       | Val loss 1302\n",
            "Epoch: 841/1000 | Train loss: 1101       | Val loss 1262\n",
            "Epoch: 861/1000 | Train loss: 1103       | Val loss 1262\n",
            "Epoch: 881/1000 | Train loss: 1094       | Val loss 1242\n",
            "Epoch: 901/1000 | Train loss: 1183       | Val loss 1415\n",
            "Epoch: 921/1000 | Train loss: 1042       | Val loss 1226\n",
            "Epoch: 941/1000 | Train loss: 1040       | Val loss 1175\n",
            "Epoch: 961/1000 | Train loss: 1025       | Val loss 1160\n",
            "Epoch: 981/1000 | Train loss: 1012       | Val loss 1194\n",
            "Trial 22:\t{'n_hidLayers': 5, 'hidden_size': 19, 'lr': 0.005914293101519497}\n",
            "Epoch: 1/1000 | Train loss: 68865       | Val loss 71128\n",
            "Epoch: 21/1000 | Train loss: 17025       | Val loss 17176\n",
            "Epoch: 41/1000 | Train loss: 17114       | Val loss 17618\n",
            "Epoch: 61/1000 | Train loss: 16715       | Val loss 17030\n",
            "Epoch: 81/1000 | Train loss: 16763       | Val loss 17922\n",
            "Epoch: 101/1000 | Train loss: 16766       | Val loss 17091\n",
            "Epoch: 121/1000 | Train loss: 16735       | Val loss 17298\n",
            "Epoch: 141/1000 | Train loss: 16584       | Val loss 17104\n",
            "Epoch: 161/1000 | Train loss: 16546       | Val loss 17020\n",
            "Epoch: 181/1000 | Train loss: 16387       | Val loss 16647\n",
            "Epoch: 201/1000 | Train loss: 7744       | Val loss 7894\n",
            "Epoch: 221/1000 | Train loss: 6844       | Val loss 6991\n",
            "Epoch: 241/1000 | Train loss: 6455       | Val loss 6774\n",
            "Epoch: 261/1000 | Train loss: 5872       | Val loss 5995\n",
            "Epoch: 281/1000 | Train loss: 5309       | Val loss 5441\n",
            "Epoch: 301/1000 | Train loss: 3507       | Val loss 3610\n",
            "Epoch: 321/1000 | Train loss: 3344       | Val loss 3442\n",
            "Epoch: 341/1000 | Train loss: 2900       | Val loss 2971\n",
            "Epoch: 361/1000 | Train loss: 2767       | Val loss 2860\n",
            "Epoch: 381/1000 | Train loss: 2716       | Val loss 2742\n",
            "Epoch: 401/1000 | Train loss: 2792       | Val loss 2822\n",
            "Epoch: 421/1000 | Train loss: 2659       | Val loss 2727\n",
            "Epoch: 441/1000 | Train loss: 2711       | Val loss 2773\n",
            "Epoch: 461/1000 | Train loss: 2666       | Val loss 2775\n",
            "Epoch: 481/1000 | Train loss: 2716       | Val loss 2861\n",
            "Epoch: 501/1000 | Train loss: 2670       | Val loss 2829\n",
            "Epoch: 521/1000 | Train loss: 2610       | Val loss 2848\n",
            "Epoch: 541/1000 | Train loss: 2526       | Val loss 2573\n",
            "Epoch: 561/1000 | Train loss: 2542       | Val loss 2643\n",
            "Epoch: 581/1000 | Train loss: 2552       | Val loss 2649\n",
            "Epoch: 601/1000 | Train loss: 2447       | Val loss 2562\n",
            "Epoch: 621/1000 | Train loss: 2393       | Val loss 2526\n",
            "Epoch: 641/1000 | Train loss: 1770       | Val loss 1954\n",
            "Epoch: 661/1000 | Train loss: 1547       | Val loss 1708\n",
            "Epoch: 681/1000 | Train loss: 1507       | Val loss 1611\n",
            "Epoch: 701/1000 | Train loss: 1352       | Val loss 1488\n",
            "Epoch: 721/1000 | Train loss: 1305       | Val loss 1396\n",
            "Epoch: 741/1000 | Train loss: 1268       | Val loss 1346\n",
            "Epoch: 761/1000 | Train loss: 1364       | Val loss 1569\n",
            "Epoch: 781/1000 | Train loss: 1264       | Val loss 1555\n",
            "Epoch: 801/1000 | Train loss: 1169       | Val loss 1310\n",
            "Epoch: 821/1000 | Train loss: 1136       | Val loss 1365\n",
            "Epoch: 841/1000 | Train loss: 1156       | Val loss 1326\n",
            "Epoch: 861/1000 | Train loss: 1121       | Val loss 1276\n",
            "Epoch: 881/1000 | Train loss: 1066       | Val loss 1202\n",
            "Epoch: 901/1000 | Train loss: 1100       | Val loss 1265\n",
            "Epoch: 921/1000 | Train loss: 1064       | Val loss 1253\n",
            "Epoch: 941/1000 | Train loss: 1099       | Val loss 1312\n",
            "Epoch: 961/1000 | Train loss: 1062       | Val loss 1213\n",
            "Epoch: 981/1000 | Train loss: 996       | Val loss 1142\n",
            "Trial 23:\t{'n_hidLayers': 4, 'hidden_size': 34, 'lr': 0.002835018797629078}\n",
            "Epoch: 1/1000 | Train loss: 240475       | Val loss 250597\n",
            "Epoch: 21/1000 | Train loss: 240377       | Val loss 247029\n",
            "Epoch: 41/1000 | Train loss: 240126       | Val loss 248364\n",
            "Epoch: 61/1000 | Train loss: 240316       | Val loss 255360\n",
            "Epoch: 81/1000 | Train loss: 240299       | Val loss 251206\n",
            "Epoch: 101/1000 | Train loss: 240367       | Val loss 253543\n",
            "Epoch: 121/1000 | Train loss: 240373       | Val loss 250701\n",
            "Epoch: 141/1000 | Train loss: 240583       | Val loss 252678\n",
            "Epoch: 161/1000 | Train loss: 240318       | Val loss 250806\n",
            "Epoch: 181/1000 | Train loss: 240146       | Val loss 247370\n",
            "Epoch: 201/1000 | Train loss: 240442       | Val loss 252473\n",
            "Epoch: 221/1000 | Train loss: 240253       | Val loss 249036\n",
            "Epoch: 241/1000 | Train loss: 240265       | Val loss 250794\n",
            "Epoch: 261/1000 | Train loss: 240369       | Val loss 251293\n",
            "Epoch: 281/1000 | Train loss: 240368       | Val loss 253277\n",
            "Epoch: 301/1000 | Train loss: 240228       | Val loss 249315\n",
            "Epoch: 321/1000 | Train loss: 240146       | Val loss 248392\n",
            "Epoch: 341/1000 | Train loss: 240327       | Val loss 253489\n",
            "Epoch: 361/1000 | Train loss: 240466       | Val loss 247219\n",
            "Epoch: 381/1000 | Train loss: 240452       | Val loss 247281\n",
            "Epoch: 401/1000 | Train loss: 240567       | Val loss 248046\n",
            "Epoch: 421/1000 | Train loss: 240358       | Val loss 248502\n",
            "Epoch: 441/1000 | Train loss: 240518       | Val loss 249769\n",
            "Epoch: 461/1000 | Train loss: 240288       | Val loss 250505\n",
            "Epoch: 481/1000 | Train loss: 240234       | Val loss 248944\n",
            "Epoch: 501/1000 | Train loss: 240404       | Val loss 248932\n",
            "Epoch: 521/1000 | Train loss: 240433       | Val loss 253691\n",
            "Epoch: 541/1000 | Train loss: 240641       | Val loss 249821\n",
            "Epoch: 561/1000 | Train loss: 240458       | Val loss 254419\n",
            "Epoch: 581/1000 | Train loss: 240396       | Val loss 250743\n",
            "Epoch: 601/1000 | Train loss: 240630       | Val loss 247518\n",
            "Epoch: 621/1000 | Train loss: 240369       | Val loss 250610\n",
            "Epoch: 641/1000 | Train loss: 240638       | Val loss 247555\n",
            "Epoch: 661/1000 | Train loss: 240416       | Val loss 253120\n",
            "Epoch: 681/1000 | Train loss: 240600       | Val loss 250176\n",
            "Epoch: 701/1000 | Train loss: 240337       | Val loss 251586\n",
            "Epoch: 721/1000 | Train loss: 240288       | Val loss 249666\n",
            "Epoch: 741/1000 | Train loss: 240456       | Val loss 249874\n",
            "Epoch: 761/1000 | Train loss: 240222       | Val loss 248798\n",
            "Epoch: 781/1000 | Train loss: 240279       | Val loss 251474\n",
            "Epoch: 801/1000 | Train loss: 240246       | Val loss 248975\n",
            "Epoch: 821/1000 | Train loss: 240278       | Val loss 252043\n",
            "Epoch: 841/1000 | Train loss: 240374       | Val loss 249346\n",
            "Epoch: 861/1000 | Train loss: 240372       | Val loss 248988\n",
            "Epoch: 881/1000 | Train loss: 240408       | Val loss 252763\n",
            "Epoch: 901/1000 | Train loss: 240309       | Val loss 249758\n",
            "Epoch: 921/1000 | Train loss: 240466       | Val loss 249832\n",
            "Epoch: 941/1000 | Train loss: 240245       | Val loss 247365\n",
            "Epoch: 961/1000 | Train loss: 240373       | Val loss 251639\n",
            "Epoch: 981/1000 | Train loss: 240396       | Val loss 249860\n",
            "Trial 24:\t{'n_hidLayers': 3, 'hidden_size': 18, 'lr': 0.07673577155733873}\n",
            "Epoch: 1/1000 | Train loss: 78143       | Val loss 81812\n",
            "Epoch: 21/1000 | Train loss: 8805       | Val loss 9126\n",
            "Epoch: 41/1000 | Train loss: 6763       | Val loss 7042\n",
            "Epoch: 61/1000 | Train loss: 3763       | Val loss 3942\n",
            "Epoch: 81/1000 | Train loss: 2472       | Val loss 2596\n",
            "Epoch: 101/1000 | Train loss: 2355       | Val loss 2463\n",
            "Epoch: 121/1000 | Train loss: 2245       | Val loss 2275\n",
            "Epoch: 141/1000 | Train loss: 2080       | Val loss 2103\n",
            "Epoch: 161/1000 | Train loss: 1991       | Val loss 2130\n",
            "Epoch: 181/1000 | Train loss: 2067       | Val loss 2171\n",
            "Epoch: 201/1000 | Train loss: 1961       | Val loss 2020\n",
            "Epoch: 221/1000 | Train loss: 1819       | Val loss 1879\n",
            "Epoch: 241/1000 | Train loss: 2122       | Val loss 2252\n",
            "Epoch: 261/1000 | Train loss: 1888       | Val loss 1941\n",
            "Epoch: 281/1000 | Train loss: 1800       | Val loss 1835\n",
            "Epoch: 301/1000 | Train loss: 2022       | Val loss 2096\n",
            "Epoch: 321/1000 | Train loss: 1810       | Val loss 1886\n",
            "Epoch: 341/1000 | Train loss: 1766       | Val loss 1827\n",
            "Epoch: 361/1000 | Train loss: 2255       | Val loss 2314\n",
            "Epoch: 381/1000 | Train loss: 1888       | Val loss 2002\n",
            "Epoch: 401/1000 | Train loss: 1863       | Val loss 1942\n",
            "Epoch: 421/1000 | Train loss: 1990       | Val loss 2101\n",
            "Epoch: 441/1000 | Train loss: 1754       | Val loss 1871\n",
            "Epoch: 461/1000 | Train loss: 2024       | Val loss 2094\n",
            "Epoch: 481/1000 | Train loss: 1789       | Val loss 1826\n",
            "Epoch: 501/1000 | Train loss: 1854       | Val loss 1997\n",
            "Epoch: 521/1000 | Train loss: 1901       | Val loss 2018\n",
            "Epoch: 541/1000 | Train loss: 1781       | Val loss 1899\n",
            "Epoch: 561/1000 | Train loss: 1738       | Val loss 1820\n",
            "Epoch: 581/1000 | Train loss: 1775       | Val loss 1840\n",
            "Epoch: 601/1000 | Train loss: 1731       | Val loss 1823\n",
            "Epoch: 621/1000 | Train loss: 2000       | Val loss 2075\n",
            "Epoch: 641/1000 | Train loss: 1742       | Val loss 1805\n",
            "Epoch: 661/1000 | Train loss: 1827       | Val loss 1932\n",
            "Epoch: 681/1000 | Train loss: 2019       | Val loss 2117\n",
            "Epoch: 701/1000 | Train loss: 1719       | Val loss 1813\n",
            "Epoch: 721/1000 | Train loss: 1744       | Val loss 1847\n",
            "Epoch: 741/1000 | Train loss: 1751       | Val loss 1870\n",
            "Epoch: 761/1000 | Train loss: 1690       | Val loss 1791\n",
            "Epoch: 781/1000 | Train loss: 1688       | Val loss 1790\n",
            "Epoch: 801/1000 | Train loss: 1782       | Val loss 1870\n",
            "Epoch: 821/1000 | Train loss: 1643       | Val loss 1736\n",
            "Epoch: 841/1000 | Train loss: 1721       | Val loss 1839\n",
            "Epoch: 861/1000 | Train loss: 1709       | Val loss 1814\n",
            "Epoch: 881/1000 | Train loss: 1731       | Val loss 1806\n",
            "Epoch: 901/1000 | Train loss: 1937       | Val loss 2072\n",
            "Epoch: 921/1000 | Train loss: 1786       | Val loss 1947\n",
            "Epoch: 941/1000 | Train loss: 1619       | Val loss 1698\n",
            "Epoch: 961/1000 | Train loss: 1702       | Val loss 1838\n",
            "Epoch: 981/1000 | Train loss: 1620       | Val loss 1753\n",
            "Trial 25:\t{'n_hidLayers': 2, 'hidden_size': 18, 'lr': 0.0015698868068679385}\n",
            "Epoch: 1/1000 | Train loss: 240440       | Val loss 252779\n",
            "Epoch: 21/1000 | Train loss: 39951       | Val loss 41065\n",
            "Epoch: 41/1000 | Train loss: 33634       | Val loss 35179\n",
            "Epoch: 61/1000 | Train loss: 26724       | Val loss 27643\n",
            "Epoch: 81/1000 | Train loss: 20654       | Val loss 23028\n",
            "Epoch: 101/1000 | Train loss: 18754       | Val loss 18938\n",
            "Epoch: 121/1000 | Train loss: 18458       | Val loss 19313\n",
            "Epoch: 141/1000 | Train loss: 18446       | Val loss 18730\n",
            "Epoch: 161/1000 | Train loss: 18388       | Val loss 18957\n",
            "Epoch: 181/1000 | Train loss: 18338       | Val loss 18457\n",
            "Epoch: 201/1000 | Train loss: 18279       | Val loss 18688\n",
            "Epoch: 221/1000 | Train loss: 18284       | Val loss 18671\n",
            "Epoch: 241/1000 | Train loss: 18195       | Val loss 19457\n",
            "Epoch: 261/1000 | Train loss: 18126       | Val loss 18611\n",
            "Epoch: 281/1000 | Train loss: 17987       | Val loss 18230\n",
            "Epoch: 301/1000 | Train loss: 17846       | Val loss 19165\n",
            "Epoch: 321/1000 | Train loss: 17620       | Val loss 17979\n",
            "Epoch: 341/1000 | Train loss: 17429       | Val loss 17575\n",
            "Epoch: 361/1000 | Train loss: 17158       | Val loss 17685\n",
            "Epoch: 381/1000 | Train loss: 16247       | Val loss 16800\n",
            "Epoch: 401/1000 | Train loss: 13808       | Val loss 14553\n",
            "Epoch: 421/1000 | Train loss: 10655       | Val loss 10931\n",
            "Epoch: 441/1000 | Train loss: 9571       | Val loss 9827\n",
            "Epoch: 461/1000 | Train loss: 8937       | Val loss 8930\n",
            "Epoch: 481/1000 | Train loss: 8263       | Val loss 8426\n",
            "Epoch: 501/1000 | Train loss: 7563       | Val loss 8237\n",
            "Epoch: 521/1000 | Train loss: 7102       | Val loss 7239\n",
            "Epoch: 541/1000 | Train loss: 6845       | Val loss 7001\n",
            "Epoch: 561/1000 | Train loss: 6669       | Val loss 6722\n",
            "Epoch: 581/1000 | Train loss: 6479       | Val loss 6634\n",
            "Epoch: 601/1000 | Train loss: 6335       | Val loss 6348\n",
            "Epoch: 621/1000 | Train loss: 6181       | Val loss 6595\n",
            "Epoch: 641/1000 | Train loss: 6049       | Val loss 6001\n",
            "Epoch: 661/1000 | Train loss: 5891       | Val loss 5961\n",
            "Epoch: 681/1000 | Train loss: 5744       | Val loss 5863\n",
            "Epoch: 701/1000 | Train loss: 5559       | Val loss 5649\n",
            "Epoch: 721/1000 | Train loss: 5394       | Val loss 5623\n",
            "Epoch: 741/1000 | Train loss: 5264       | Val loss 5835\n",
            "Epoch: 761/1000 | Train loss: 5126       | Val loss 5277\n",
            "Epoch: 781/1000 | Train loss: 4994       | Val loss 5128\n",
            "Epoch: 801/1000 | Train loss: 4868       | Val loss 5114\n",
            "Epoch: 821/1000 | Train loss: 4752       | Val loss 4948\n",
            "Epoch: 841/1000 | Train loss: 4653       | Val loss 4836\n",
            "Epoch: 861/1000 | Train loss: 4558       | Val loss 4590\n",
            "Epoch: 881/1000 | Train loss: 4452       | Val loss 4731\n",
            "Epoch: 901/1000 | Train loss: 4355       | Val loss 4566\n",
            "Epoch: 921/1000 | Train loss: 4254       | Val loss 4774\n",
            "Epoch: 941/1000 | Train loss: 4133       | Val loss 4193\n",
            "Epoch: 961/1000 | Train loss: 4028       | Val loss 4177\n",
            "Epoch: 981/1000 | Train loss: 3937       | Val loss 3998\n",
            "Trial 26:\t{'n_hidLayers': 3, 'hidden_size': 22, 'lr': 0.0024272940877493533}\n",
            "Epoch: 1/1000 | Train loss: 238740       | Val loss 248601\n",
            "Epoch: 21/1000 | Train loss: 46929       | Val loss 51164\n",
            "Epoch: 41/1000 | Train loss: 46508       | Val loss 48177\n",
            "Epoch: 61/1000 | Train loss: 46307       | Val loss 50032\n",
            "Epoch: 81/1000 | Train loss: 41116       | Val loss 42527\n",
            "Epoch: 101/1000 | Train loss: 40313       | Val loss 42773\n",
            "Epoch: 121/1000 | Train loss: 10052       | Val loss 10419\n",
            "Epoch: 141/1000 | Train loss: 9057       | Val loss 9270\n",
            "Epoch: 161/1000 | Train loss: 8719       | Val loss 8842\n",
            "Epoch: 181/1000 | Train loss: 8378       | Val loss 8559\n",
            "Epoch: 201/1000 | Train loss: 6608       | Val loss 6646\n",
            "Epoch: 221/1000 | Train loss: 6172       | Val loss 6375\n",
            "Epoch: 241/1000 | Train loss: 5606       | Val loss 5760\n",
            "Epoch: 261/1000 | Train loss: 5390       | Val loss 5594\n",
            "Epoch: 281/1000 | Train loss: 5113       | Val loss 5252\n",
            "Epoch: 301/1000 | Train loss: 4992       | Val loss 5136\n",
            "Epoch: 321/1000 | Train loss: 4800       | Val loss 4886\n",
            "Epoch: 341/1000 | Train loss: 4749       | Val loss 4907\n",
            "Epoch: 361/1000 | Train loss: 4632       | Val loss 4826\n",
            "Epoch: 381/1000 | Train loss: 4603       | Val loss 4801\n",
            "Epoch: 401/1000 | Train loss: 4477       | Val loss 4542\n",
            "Epoch: 421/1000 | Train loss: 4410       | Val loss 4585\n",
            "Epoch: 441/1000 | Train loss: 4362       | Val loss 4586\n",
            "Epoch: 461/1000 | Train loss: 4281       | Val loss 4493\n",
            "Epoch: 481/1000 | Train loss: 4196       | Val loss 4273\n",
            "Epoch: 501/1000 | Train loss: 4056       | Val loss 4083\n",
            "Epoch: 521/1000 | Train loss: 3875       | Val loss 4006\n",
            "Epoch: 541/1000 | Train loss: 3642       | Val loss 3712\n",
            "Epoch: 561/1000 | Train loss: 3106       | Val loss 3156\n",
            "Epoch: 581/1000 | Train loss: 2663       | Val loss 2720\n",
            "Epoch: 601/1000 | Train loss: 2327       | Val loss 2379\n",
            "Epoch: 621/1000 | Train loss: 2079       | Val loss 2162\n",
            "Epoch: 641/1000 | Train loss: 1934       | Val loss 2027\n",
            "Epoch: 661/1000 | Train loss: 1820       | Val loss 1900\n",
            "Epoch: 681/1000 | Train loss: 1745       | Val loss 1819\n",
            "Epoch: 701/1000 | Train loss: 1687       | Val loss 1706\n",
            "Epoch: 721/1000 | Train loss: 1633       | Val loss 1694\n",
            "Epoch: 741/1000 | Train loss: 1591       | Val loss 1635\n",
            "Epoch: 761/1000 | Train loss: 1575       | Val loss 1593\n",
            "Epoch: 781/1000 | Train loss: 1533       | Val loss 1579\n",
            "Epoch: 801/1000 | Train loss: 1515       | Val loss 1582\n",
            "Epoch: 821/1000 | Train loss: 1476       | Val loss 1528\n",
            "Epoch: 841/1000 | Train loss: 1461       | Val loss 1490\n",
            "Epoch: 861/1000 | Train loss: 1457       | Val loss 1495\n",
            "Epoch: 881/1000 | Train loss: 1426       | Val loss 1455\n",
            "Epoch: 901/1000 | Train loss: 1439       | Val loss 1511\n",
            "Epoch: 921/1000 | Train loss: 1449       | Val loss 1482\n",
            "Epoch: 941/1000 | Train loss: 1386       | Val loss 1444\n",
            "Epoch: 961/1000 | Train loss: 1370       | Val loss 1466\n",
            "Epoch: 981/1000 | Train loss: 1374       | Val loss 1422\n",
            "Trial 27:\t{'n_hidLayers': 5, 'hidden_size': 50, 'lr': 0.03887454529870162}\n",
            "Epoch: 1/1000 | Train loss: 161938       | Val loss 169403\n",
            "Epoch: 21/1000 | Train loss: 159734       | Val loss 167062\n",
            "Epoch: 41/1000 | Train loss: 159826       | Val loss 168865\n",
            "Epoch: 61/1000 | Train loss: 5870       | Val loss 6202\n",
            "Epoch: 81/1000 | Train loss: 1555       | Val loss 1767\n",
            "Epoch: 101/1000 | Train loss: 1322       | Val loss 1503\n",
            "Epoch: 121/1000 | Train loss: 1517       | Val loss 1648\n",
            "Epoch: 141/1000 | Train loss: 1326       | Val loss 1567\n",
            "Epoch: 161/1000 | Train loss: 1380       | Val loss 1534\n",
            "Epoch: 181/1000 | Train loss: 1060       | Val loss 1241\n",
            "Epoch: 201/1000 | Train loss: 1084       | Val loss 1334\n",
            "Epoch: 221/1000 | Train loss: 1263       | Val loss 1399\n",
            "Epoch: 241/1000 | Train loss: 1003       | Val loss 1210\n",
            "Epoch: 261/1000 | Train loss: 942       | Val loss 1139\n",
            "Epoch: 281/1000 | Train loss: 1068       | Val loss 1291\n",
            "Epoch: 301/1000 | Train loss: 964       | Val loss 1159\n",
            "Epoch: 321/1000 | Train loss: 854       | Val loss 1059\n",
            "Epoch: 341/1000 | Train loss: 991       | Val loss 1208\n",
            "Epoch: 361/1000 | Train loss: 910       | Val loss 1126\n",
            "Epoch: 381/1000 | Train loss: 984       | Val loss 1193\n",
            "Epoch: 401/1000 | Train loss: 968       | Val loss 1230\n",
            "Epoch: 421/1000 | Train loss: 957       | Val loss 1178\n",
            "Epoch: 441/1000 | Train loss: 893       | Val loss 1134\n",
            "Epoch: 461/1000 | Train loss: 934       | Val loss 1311\n",
            "Epoch: 481/1000 | Train loss: 935       | Val loss 1200\n",
            "Epoch: 501/1000 | Train loss: 870       | Val loss 1083\n",
            "Epoch: 521/1000 | Train loss: 918       | Val loss 1130\n",
            "Epoch: 541/1000 | Train loss: 854       | Val loss 1083\n",
            "Epoch: 561/1000 | Train loss: 915       | Val loss 1109\n",
            "Epoch: 581/1000 | Train loss: 886       | Val loss 1112\n",
            "Epoch: 601/1000 | Train loss: 876       | Val loss 1125\n",
            "Epoch: 621/1000 | Train loss: 864       | Val loss 1067\n",
            "Epoch: 641/1000 | Train loss: 896       | Val loss 1115\n",
            "Epoch: 661/1000 | Train loss: 951       | Val loss 1212\n",
            "Epoch: 681/1000 | Train loss: 1055       | Val loss 1259\n",
            "Epoch: 701/1000 | Train loss: 833       | Val loss 1058\n",
            "Epoch: 721/1000 | Train loss: 922       | Val loss 1180\n",
            "Epoch: 741/1000 | Train loss: 823       | Val loss 1085\n",
            "Epoch: 761/1000 | Train loss: 815       | Val loss 1072\n",
            "Epoch: 781/1000 | Train loss: 1009       | Val loss 1185\n",
            "Epoch: 801/1000 | Train loss: 862       | Val loss 1146\n",
            "Epoch: 821/1000 | Train loss: 967       | Val loss 1204\n",
            "Epoch: 841/1000 | Train loss: 856       | Val loss 1114\n",
            "Epoch: 861/1000 | Train loss: 863       | Val loss 1106\n",
            "Epoch: 881/1000 | Train loss: 798       | Val loss 1065\n",
            "Epoch: 901/1000 | Train loss: 813       | Val loss 1047\n",
            "Epoch: 921/1000 | Train loss: 828       | Val loss 1060\n",
            "Epoch: 941/1000 | Train loss: 955       | Val loss 1257\n",
            "Epoch: 961/1000 | Train loss: 856       | Val loss 1095\n",
            "Epoch: 981/1000 | Train loss: 803       | Val loss 1094\n",
            "Trial 28:\t{'n_hidLayers': 2, 'hidden_size': 29, 'lr': 0.05887375210280041}\n",
            "Epoch: 1/1000 | Train loss: 82377       | Val loss 84875\n",
            "Epoch: 21/1000 | Train loss: 53705       | Val loss 55547\n",
            "Epoch: 41/1000 | Train loss: 51740       | Val loss 53720\n",
            "Epoch: 61/1000 | Train loss: 50622       | Val loss 52406\n",
            "Epoch: 81/1000 | Train loss: 50444       | Val loss 52276\n",
            "Epoch: 101/1000 | Train loss: 50307       | Val loss 52150\n",
            "Epoch: 121/1000 | Train loss: 50173       | Val loss 52110\n",
            "Epoch: 141/1000 | Train loss: 6183       | Val loss 6585\n",
            "Epoch: 161/1000 | Train loss: 2129       | Val loss 2259\n",
            "Epoch: 181/1000 | Train loss: 2057       | Val loss 2282\n",
            "Epoch: 201/1000 | Train loss: 2050       | Val loss 2212\n",
            "Epoch: 221/1000 | Train loss: 1932       | Val loss 2064\n",
            "Epoch: 241/1000 | Train loss: 1924       | Val loss 2073\n",
            "Epoch: 261/1000 | Train loss: 1730       | Val loss 1873\n",
            "Epoch: 281/1000 | Train loss: 1780       | Val loss 1970\n",
            "Epoch: 301/1000 | Train loss: 1688       | Val loss 1815\n",
            "Epoch: 321/1000 | Train loss: 1609       | Val loss 1707\n",
            "Epoch: 341/1000 | Train loss: 1687       | Val loss 1784\n",
            "Epoch: 361/1000 | Train loss: 1591       | Val loss 1672\n",
            "Epoch: 381/1000 | Train loss: 1493       | Val loss 1541\n",
            "Epoch: 401/1000 | Train loss: 1595       | Val loss 1842\n",
            "Epoch: 421/1000 | Train loss: 1360       | Val loss 1455\n",
            "Epoch: 441/1000 | Train loss: 1323       | Val loss 1394\n",
            "Epoch: 461/1000 | Train loss: 1599       | Val loss 1745\n",
            "Epoch: 481/1000 | Train loss: 1355       | Val loss 1426\n",
            "Epoch: 501/1000 | Train loss: 1321       | Val loss 1397\n",
            "Epoch: 521/1000 | Train loss: 1647       | Val loss 1760\n",
            "Epoch: 541/1000 | Train loss: 1331       | Val loss 1461\n",
            "Epoch: 561/1000 | Train loss: 1212       | Val loss 1284\n",
            "Epoch: 581/1000 | Train loss: 1345       | Val loss 1430\n",
            "Epoch: 601/1000 | Train loss: 1285       | Val loss 1362\n",
            "Epoch: 621/1000 | Train loss: 1303       | Val loss 1345\n",
            "Epoch: 641/1000 | Train loss: 1295       | Val loss 1357\n",
            "Epoch: 661/1000 | Train loss: 1259       | Val loss 1429\n",
            "Epoch: 681/1000 | Train loss: 1359       | Val loss 1438\n",
            "Epoch: 701/1000 | Train loss: 1262       | Val loss 1398\n",
            "Epoch: 721/1000 | Train loss: 1199       | Val loss 1274\n",
            "Epoch: 741/1000 | Train loss: 1202       | Val loss 1269\n",
            "Epoch: 761/1000 | Train loss: 1193       | Val loss 1246\n",
            "Epoch: 781/1000 | Train loss: 1231       | Val loss 1304\n",
            "Epoch: 801/1000 | Train loss: 1191       | Val loss 1276\n",
            "Epoch: 821/1000 | Train loss: 1358       | Val loss 1426\n",
            "Epoch: 841/1000 | Train loss: 1200       | Val loss 1307\n",
            "Epoch: 861/1000 | Train loss: 1335       | Val loss 1410\n",
            "Epoch: 881/1000 | Train loss: 1141       | Val loss 1338\n",
            "Epoch: 901/1000 | Train loss: 1162       | Val loss 1246\n",
            "Epoch: 921/1000 | Train loss: 1154       | Val loss 1198\n",
            "Epoch: 941/1000 | Train loss: 1147       | Val loss 1194\n",
            "Epoch: 961/1000 | Train loss: 1191       | Val loss 1266\n",
            "Epoch: 981/1000 | Train loss: 1156       | Val loss 1205\n",
            "Trial 29:\t{'n_hidLayers': 5, 'hidden_size': 46, 'lr': 0.05608803141270856}\n",
            "Epoch: 1/1000 | Train loss: 97373       | Val loss 100445\n",
            "Epoch: 21/1000 | Train loss: 95167       | Val loss 99534\n",
            "Epoch: 41/1000 | Train loss: 89011       | Val loss 90856\n",
            "Epoch: 61/1000 | Train loss: 89023       | Val loss 90796\n",
            "Epoch: 81/1000 | Train loss: 88844       | Val loss 92195\n",
            "Epoch: 101/1000 | Train loss: 2165       | Val loss 2369\n",
            "Epoch: 121/1000 | Train loss: 1324       | Val loss 1505\n",
            "Epoch: 141/1000 | Train loss: 1095       | Val loss 1240\n",
            "Epoch: 161/1000 | Train loss: 1172       | Val loss 1293\n",
            "Epoch: 181/1000 | Train loss: 1336       | Val loss 1569\n",
            "Epoch: 201/1000 | Train loss: 939       | Val loss 1176\n",
            "Epoch: 221/1000 | Train loss: 1045       | Val loss 1212\n",
            "Epoch: 241/1000 | Train loss: 964       | Val loss 1161\n",
            "Epoch: 261/1000 | Train loss: 1007       | Val loss 1222\n",
            "Epoch: 281/1000 | Train loss: 1009       | Val loss 1165\n",
            "Epoch: 301/1000 | Train loss: 1150       | Val loss 1376\n",
            "Epoch: 321/1000 | Train loss: 900       | Val loss 1108\n",
            "Epoch: 341/1000 | Train loss: 854       | Val loss 1036\n",
            "Epoch: 361/1000 | Train loss: 939       | Val loss 1181\n",
            "Epoch: 381/1000 | Train loss: 879       | Val loss 1115\n",
            "Epoch: 401/1000 | Train loss: 1468       | Val loss 1691\n",
            "Epoch: 421/1000 | Train loss: 852       | Val loss 1085\n",
            "Epoch: 441/1000 | Train loss: 901       | Val loss 1103\n",
            "Epoch: 461/1000 | Train loss: 965       | Val loss 1247\n",
            "Epoch: 481/1000 | Train loss: 907       | Val loss 1096\n",
            "Epoch: 501/1000 | Train loss: 862       | Val loss 1095\n",
            "Epoch: 521/1000 | Train loss: 906       | Val loss 1072\n",
            "Epoch: 541/1000 | Train loss: 887       | Val loss 1097\n",
            "Epoch: 561/1000 | Train loss: 844       | Val loss 1066\n",
            "Epoch: 581/1000 | Train loss: 840       | Val loss 1054\n",
            "Epoch: 601/1000 | Train loss: 860       | Val loss 1169\n",
            "Epoch: 621/1000 | Train loss: 875       | Val loss 1121\n",
            "Epoch: 641/1000 | Train loss: 925       | Val loss 1144\n",
            "Epoch: 661/1000 | Train loss: 900       | Val loss 1142\n",
            "Epoch: 681/1000 | Train loss: 893       | Val loss 1114\n",
            "Epoch: 701/1000 | Train loss: 999       | Val loss 1246\n",
            "Epoch: 721/1000 | Train loss: 925       | Val loss 1200\n",
            "Epoch: 741/1000 | Train loss: 893       | Val loss 1168\n",
            "Epoch: 761/1000 | Train loss: 1129       | Val loss 1400\n",
            "Epoch: 781/1000 | Train loss: 918       | Val loss 1174\n",
            "Epoch: 801/1000 | Train loss: 949       | Val loss 1154\n",
            "Epoch: 821/1000 | Train loss: 898       | Val loss 1165\n",
            "Epoch: 841/1000 | Train loss: 883       | Val loss 1131\n",
            "Epoch: 861/1000 | Train loss: 1152       | Val loss 1432\n",
            "Epoch: 881/1000 | Train loss: 913       | Val loss 1260\n",
            "Epoch: 901/1000 | Train loss: 809       | Val loss 1099\n",
            "Epoch: 921/1000 | Train loss: 1135       | Val loss 1445\n",
            "Epoch: 941/1000 | Train loss: 995       | Val loss 1272\n",
            "Epoch: 961/1000 | Train loss: 804       | Val loss 1033\n",
            "Epoch: 981/1000 | Train loss: 943       | Val loss 1217\n",
            "Trial 30:\t{'n_hidLayers': 5, 'hidden_size': 48, 'lr': 0.005416826640186919}\n",
            "Epoch: 1/1000 | Train loss: 34196       | Val loss 35377\n",
            "Epoch: 21/1000 | Train loss: 16983       | Val loss 18162\n",
            "Epoch: 41/1000 | Train loss: 16968       | Val loss 17238\n",
            "Epoch: 61/1000 | Train loss: 16940       | Val loss 17281\n",
            "Epoch: 81/1000 | Train loss: 16841       | Val loss 17399\n",
            "Epoch: 101/1000 | Train loss: 16351       | Val loss 16886\n",
            "Epoch: 121/1000 | Train loss: 6492       | Val loss 6799\n",
            "Epoch: 141/1000 | Train loss: 5694       | Val loss 5938\n",
            "Epoch: 161/1000 | Train loss: 3027       | Val loss 3221\n",
            "Epoch: 181/1000 | Train loss: 2750       | Val loss 2934\n",
            "Epoch: 201/1000 | Train loss: 2699       | Val loss 2895\n",
            "Epoch: 221/1000 | Train loss: 2629       | Val loss 2803\n",
            "Epoch: 241/1000 | Train loss: 1258       | Val loss 1384\n",
            "Epoch: 261/1000 | Train loss: 1126       | Val loss 1260\n",
            "Epoch: 281/1000 | Train loss: 1092       | Val loss 1239\n",
            "Epoch: 301/1000 | Train loss: 1135       | Val loss 1309\n",
            "Epoch: 321/1000 | Train loss: 968       | Val loss 1128\n",
            "Epoch: 341/1000 | Train loss: 1019       | Val loss 1210\n",
            "Epoch: 361/1000 | Train loss: 985       | Val loss 1173\n",
            "Epoch: 381/1000 | Train loss: 983       | Val loss 1179\n",
            "Epoch: 401/1000 | Train loss: 931       | Val loss 1098\n",
            "Epoch: 421/1000 | Train loss: 943       | Val loss 1137\n",
            "Epoch: 441/1000 | Train loss: 942       | Val loss 1110\n",
            "Epoch: 461/1000 | Train loss: 991       | Val loss 1153\n",
            "Epoch: 481/1000 | Train loss: 998       | Val loss 1193\n",
            "Epoch: 501/1000 | Train loss: 937       | Val loss 1112\n",
            "Epoch: 521/1000 | Train loss: 931       | Val loss 1107\n",
            "Epoch: 541/1000 | Train loss: 927       | Val loss 1121\n",
            "Epoch: 561/1000 | Train loss: 891       | Val loss 1059\n",
            "Epoch: 581/1000 | Train loss: 874       | Val loss 1037\n",
            "Epoch: 601/1000 | Train loss: 937       | Val loss 1106\n",
            "Epoch: 621/1000 | Train loss: 903       | Val loss 1079\n",
            "Epoch: 641/1000 | Train loss: 925       | Val loss 1131\n",
            "Epoch: 661/1000 | Train loss: 953       | Val loss 1126\n",
            "Epoch: 681/1000 | Train loss: 896       | Val loss 1095\n",
            "Epoch: 701/1000 | Train loss: 853       | Val loss 1031\n",
            "Epoch: 721/1000 | Train loss: 919       | Val loss 1108\n",
            "Epoch: 741/1000 | Train loss: 866       | Val loss 1029\n",
            "Epoch: 761/1000 | Train loss: 914       | Val loss 1075\n",
            "Epoch: 781/1000 | Train loss: 837       | Val loss 1000\n",
            "Epoch: 801/1000 | Train loss: 847       | Val loss 1042\n",
            "Epoch: 821/1000 | Train loss: 835       | Val loss 1034\n",
            "Epoch: 841/1000 | Train loss: 860       | Val loss 1030\n",
            "Epoch: 861/1000 | Train loss: 1047       | Val loss 1236\n",
            "Epoch: 881/1000 | Train loss: 819       | Val loss 1060\n",
            "Epoch: 901/1000 | Train loss: 836       | Val loss 1042\n",
            "Epoch: 921/1000 | Train loss: 817       | Val loss 1029\n",
            "Epoch: 941/1000 | Train loss: 834       | Val loss 1009\n",
            "Epoch: 961/1000 | Train loss: 851       | Val loss 1060\n",
            "Epoch: 981/1000 | Train loss: 860       | Val loss 1058\n",
            "Trial 31:\t{'n_hidLayers': 2, 'hidden_size': 53, 'lr': 0.03484476303851063}\n",
            "Epoch: 1/1000 | Train loss: 145970       | Val loss 155376\n",
            "Epoch: 21/1000 | Train loss: 143231       | Val loss 150277\n",
            "Epoch: 41/1000 | Train loss: 140758       | Val loss 146829\n",
            "Epoch: 61/1000 | Train loss: 140601       | Val loss 151645\n",
            "Epoch: 81/1000 | Train loss: 140419       | Val loss 149006\n",
            "Epoch: 101/1000 | Train loss: 139889       | Val loss 146028\n",
            "Epoch: 121/1000 | Train loss: 2916       | Val loss 2956\n",
            "Epoch: 141/1000 | Train loss: 1894       | Val loss 1971\n",
            "Epoch: 161/1000 | Train loss: 1532       | Val loss 1595\n",
            "Epoch: 181/1000 | Train loss: 1277       | Val loss 1413\n",
            "Epoch: 201/1000 | Train loss: 1111       | Val loss 1279\n",
            "Epoch: 221/1000 | Train loss: 1030       | Val loss 1189\n",
            "Epoch: 241/1000 | Train loss: 1048       | Val loss 1191\n",
            "Epoch: 261/1000 | Train loss: 927       | Val loss 1066\n",
            "Epoch: 281/1000 | Train loss: 891       | Val loss 1040\n",
            "Epoch: 301/1000 | Train loss: 861       | Val loss 986\n",
            "Epoch: 321/1000 | Train loss: 898       | Val loss 1043\n",
            "Epoch: 341/1000 | Train loss: 917       | Val loss 1055\n",
            "Epoch: 361/1000 | Train loss: 790       | Val loss 890\n",
            "Epoch: 381/1000 | Train loss: 792       | Val loss 953\n",
            "Epoch: 401/1000 | Train loss: 776       | Val loss 860\n",
            "Epoch: 421/1000 | Train loss: 743       | Val loss 856\n",
            "Epoch: 441/1000 | Train loss: 773       | Val loss 945\n",
            "Epoch: 461/1000 | Train loss: 742       | Val loss 853\n",
            "Epoch: 481/1000 | Train loss: 708       | Val loss 817\n",
            "Epoch: 501/1000 | Train loss: 690       | Val loss 830\n",
            "Epoch: 521/1000 | Train loss: 700       | Val loss 820\n",
            "Epoch: 541/1000 | Train loss: 671       | Val loss 801\n",
            "Epoch: 561/1000 | Train loss: 652       | Val loss 770\n",
            "Epoch: 581/1000 | Train loss: 675       | Val loss 820\n",
            "Epoch: 601/1000 | Train loss: 627       | Val loss 757\n",
            "Epoch: 621/1000 | Train loss: 633       | Val loss 815\n",
            "Epoch: 641/1000 | Train loss: 641       | Val loss 764\n",
            "Epoch: 661/1000 | Train loss: 612       | Val loss 740\n",
            "Epoch: 681/1000 | Train loss: 621       | Val loss 749\n",
            "Epoch: 701/1000 | Train loss: 601       | Val loss 740\n",
            "Epoch: 721/1000 | Train loss: 619       | Val loss 759\n",
            "Epoch: 741/1000 | Train loss: 541       | Val loss 692\n",
            "Epoch: 761/1000 | Train loss: 582       | Val loss 741\n",
            "Epoch: 781/1000 | Train loss: 591       | Val loss 762\n",
            "Epoch: 801/1000 | Train loss: 519       | Val loss 667\n",
            "Epoch: 821/1000 | Train loss: 518       | Val loss 675\n",
            "Epoch: 841/1000 | Train loss: 517       | Val loss 682\n",
            "Epoch: 861/1000 | Train loss: 523       | Val loss 661\n",
            "Epoch: 881/1000 | Train loss: 540       | Val loss 672\n",
            "Epoch: 901/1000 | Train loss: 514       | Val loss 658\n",
            "Epoch: 921/1000 | Train loss: 558       | Val loss 699\n",
            "Epoch: 941/1000 | Train loss: 514       | Val loss 670\n",
            "Epoch: 961/1000 | Train loss: 517       | Val loss 672\n",
            "Epoch: 981/1000 | Train loss: 538       | Val loss 699\n",
            "Trial 32:\t{'n_hidLayers': 2, 'hidden_size': 35, 'lr': 0.05525157724699159}\n",
            "Epoch: 1/1000 | Train loss: 36226       | Val loss 36770\n",
            "Epoch: 21/1000 | Train loss: 6500       | Val loss 6520\n",
            "Epoch: 41/1000 | Train loss: 5032       | Val loss 5128\n",
            "Epoch: 61/1000 | Train loss: 3486       | Val loss 3763\n",
            "Epoch: 81/1000 | Train loss: 3002       | Val loss 3039\n",
            "Epoch: 101/1000 | Train loss: 2895       | Val loss 2959\n",
            "Epoch: 121/1000 | Train loss: 2906       | Val loss 2977\n",
            "Epoch: 141/1000 | Train loss: 2690       | Val loss 2774\n",
            "Epoch: 161/1000 | Train loss: 2400       | Val loss 2466\n",
            "Epoch: 181/1000 | Train loss: 1889       | Val loss 1970\n",
            "Epoch: 201/1000 | Train loss: 1814       | Val loss 1931\n",
            "Epoch: 221/1000 | Train loss: 1394       | Val loss 1482\n",
            "Epoch: 241/1000 | Train loss: 1132       | Val loss 1246\n",
            "Epoch: 261/1000 | Train loss: 1031       | Val loss 1118\n",
            "Epoch: 281/1000 | Train loss: 960       | Val loss 1133\n",
            "Epoch: 301/1000 | Train loss: 891       | Val loss 1012\n",
            "Epoch: 321/1000 | Train loss: 953       | Val loss 1084\n",
            "Epoch: 341/1000 | Train loss: 788       | Val loss 894\n",
            "Epoch: 361/1000 | Train loss: 828       | Val loss 1042\n",
            "Epoch: 381/1000 | Train loss: 768       | Val loss 947\n",
            "Epoch: 401/1000 | Train loss: 797       | Val loss 926\n",
            "Epoch: 421/1000 | Train loss: 747       | Val loss 876\n",
            "Epoch: 441/1000 | Train loss: 781       | Val loss 993\n",
            "Epoch: 461/1000 | Train loss: 840       | Val loss 986\n",
            "Epoch: 481/1000 | Train loss: 793       | Val loss 945\n",
            "Epoch: 501/1000 | Train loss: 763       | Val loss 908\n",
            "Epoch: 521/1000 | Train loss: 720       | Val loss 868\n",
            "Epoch: 541/1000 | Train loss: 726       | Val loss 910\n",
            "Epoch: 561/1000 | Train loss: 695       | Val loss 874\n",
            "Epoch: 581/1000 | Train loss: 714       | Val loss 864\n",
            "Epoch: 601/1000 | Train loss: 676       | Val loss 862\n",
            "Epoch: 621/1000 | Train loss: 725       | Val loss 910\n",
            "Epoch: 641/1000 | Train loss: 706       | Val loss 872\n",
            "Epoch: 661/1000 | Train loss: 654       | Val loss 819\n",
            "Epoch: 681/1000 | Train loss: 660       | Val loss 804\n",
            "Epoch: 701/1000 | Train loss: 633       | Val loss 787\n",
            "Epoch: 721/1000 | Train loss: 657       | Val loss 782\n",
            "Epoch: 741/1000 | Train loss: 614       | Val loss 768\n",
            "Epoch: 761/1000 | Train loss: 626       | Val loss 779\n",
            "Epoch: 781/1000 | Train loss: 611       | Val loss 769\n",
            "Epoch: 801/1000 | Train loss: 647       | Val loss 789\n",
            "Epoch: 821/1000 | Train loss: 633       | Val loss 815\n",
            "Epoch: 841/1000 | Train loss: 623       | Val loss 779\n",
            "Epoch: 861/1000 | Train loss: 591       | Val loss 733\n",
            "Epoch: 881/1000 | Train loss: 596       | Val loss 745\n",
            "Epoch: 901/1000 | Train loss: 582       | Val loss 725\n",
            "Epoch: 921/1000 | Train loss: 563       | Val loss 708\n",
            "Epoch: 941/1000 | Train loss: 618       | Val loss 774\n",
            "Epoch: 961/1000 | Train loss: 604       | Val loss 754\n",
            "Epoch: 981/1000 | Train loss: 551       | Val loss 693\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = study.results"
      ],
      "metadata": {
        "id": "rpdUjrJQ0lcJ"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "df.to_csv('filename.csv') \n",
        "files.download('filename.csv')"
      ],
      "metadata": {
        "id": "MyFIkShRyPpH",
        "outputId": "d41c50ae-6cb6-4421-9a14-ac57ecb064f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 16
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_62c1a9b9-247d-4e06-b0e5-0f6b9f652cc5\", \"filename.csv\", 110491)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "study.get_best_result()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mzhkLlRv29u0",
        "outputId": "4f182b52-859a-4610-9a2c-517b629ec1f5"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Iteration': 440,\n",
              " 'Objective': 566.0530526315789,\n",
              " 'Trial-ID': 15,\n",
              " 'hidden_size': 53,\n",
              " 'lr': 0.04078152864919068,\n",
              " 'n_hidLayers': 5}"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "study.get_best_result()"
      ],
      "metadata": {
        "id": "CSx8TyoulAS0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "study.results.Objective.describe()"
      ],
      "metadata": {
        "id": "3TJI1oV0jZnd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "study.get_best_result()"
      ],
      "metadata": {
        "id": "z_uF07KUG5uX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "study.results"
      ],
      "metadata": {
        "id": "yrrA661lhyJW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "diZ_MjPfzRna"
      },
      "source": [
        "# 3.1- Start training loop "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f6-pIKVRzRna"
      },
      "outputs": [],
      "source": [
        "for epoch in range(num_epochs):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    model.train()\n",
        "    \n",
        "    for i, (inputs, loads) in enumerate(train_loader):\n",
        "        #print(features, loads)\n",
        "        \n",
        "        optimizer.zero_grad()                      # zeroize accumulated gradients in parameters             \n",
        "        \n",
        "        output = model(inputs.float())             # forwards pass       \n",
        "        batch_loss = loss(output, loads.float())   # compute loss for current batch\n",
        "        \n",
        "        batch_loss.backward()                      # compute the gradient of the loss wrt. model parameters\n",
        "        optimizer.step()                           # update weights according to the comptued gradients\n",
        "        \n",
        "    \n",
        "    epoch_loss_train = 0\n",
        "    epoch_loss_test = 0\n",
        "    model.eval()\n",
        "    \n",
        "    ##### Evaluate training\n",
        "    for i, (inputs, loads) in enumerate(train_loader):\n",
        "        \n",
        "        output = model(inputs.float())\n",
        "        \n",
        "        batch_loss_train = loss(output, loads.float())  # compute loss for the current batch\n",
        "        epoch_loss_train += batch_loss_train            # accumulate loss for the current epoch\n",
        "        \n",
        "        #print(f'Epoch: {epoch+1}/{num_epochs}  | Step {i+1}/{n_iterations}')\n",
        "    \n",
        "    ##### Evaluate validation    \n",
        "    for i, (inputs, loads) in enumerate(valid_loader):\n",
        "        \n",
        "        output = model(inputs.float())\n",
        "        \n",
        "        batch_loss_test = loss(output, loads.float())  # compute loss for the current batch\n",
        "        epoch_loss_test += batch_loss_test     # accumulate loss for the current epoch\n",
        "        \n",
        "        #print(f'Epoch: {epoch+1}/{num_epochs}  | Step {i+1}/{n_iterations}')\n",
        "    \n",
        "    if epoch % 100 == 0: \n",
        "        print(f'Epoch: {epoch+1}/{num_epochs} | Train loss: {epoch_loss_train/num_batches_train}       | Val loss {epoch_loss_test/num_batches_test}')\n",
        "        \n",
        "    # store in list for plotting the loss per epoch    \n",
        "    val_losses.append(epoch_loss_test/num_batches_test)  \n",
        "    train_losses.append(epoch_loss_train/num_batches_train)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vgCFr4fIzRna"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize = (8,6))\n",
        "plt.plot(train_losses[50:])\n",
        "plt.plot(val_losses[50:], '-k')\n",
        "plt.title('Training-Validation loss', fontsize = 16)\n",
        "plt.grid()\n",
        "plt.legend(['Training loss','Validation loss'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSkpKZ2HzRna"
      },
      "source": [
        "# Evalute on the test data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6lcj4lLzRnb"
      },
      "source": [
        "### Scale the test data before evaluating the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e-TPXw0rzRnb"
      },
      "outputs": [],
      "source": [
        "Test_scaler_x = preprocessing.MinMaxScaler(feature_range=feature_range).fit(X_test) # maybe try another for X?\n",
        "X_Test_scaled = Test_scaler_x.transform(X_test)\n",
        "\n",
        "predictions = model(torch.tensor(X_Test_scaled).float()).detach().numpy()\n",
        "test_targets = y_test.values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ySZ9om3tzRnb"
      },
      "outputs": [],
      "source": [
        "for idx, ch in enumerate(df.columns.tolist()[8:]):\n",
        "  # issues with 2 and 7\n",
        "    plt.figure(idx, figsize = (8,6))\n",
        "    sns.scatterplot(predictions[:,idx],test_targets[:,idx])\n",
        "    plt.title(f'DELs: {ch}', fontsize = 20)\n",
        "    plt.xlabel('Predictions [kNm]', fontsize = 12)\n",
        "    plt.ylabel('Targets [kNm]')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Py30VR4zRnb"
      },
      "outputs": [],
      "source": [
        "mse_list = []\n",
        "r2_score_list = []\n",
        "for i in range(len(df.columns.tolist()[8:])):\n",
        "    #print(f'MSE {AllTargetData.columns[i]} Channel : \\n {mean_squared_error(AllTargetData.values[:,i], Yout[:,i])}')\n",
        "    mse_list.append(mean_squared_error(test_targets[:,i], predictions[:,i]))\n",
        "    r2_score_list.append(r2_score(test_targets[:,i], predictions[:,i]))\n",
        " \n",
        "\n",
        " # Compute the normalized mean square error:\n",
        "Norm_RMSE = np.sqrt(np.array(mse_list)) / y_test.describe().loc['mean'].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0MOqWtwxzRnb"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize= (14,4)) \n",
        "sns.barplot(x= df.columns.tolist()[8:], y= r2_score_list) \n",
        "plt.ylim([min(r2_score_list)*0.92,max(r2_score_list)*1.02])\n",
        "plt.ylabel('R2 score', fontsize = 20)\n",
        "plt.grid()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZR2NNMLYzRnc"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize= (10,4)) \n",
        "sns.barplot(x= df.columns.tolist()[8:], y= Norm_RMSE) \n",
        "plt.ylim([0,max(Norm_RMSE)*1.02])\n",
        "plt.ylabel('Normalized Root Mean Square Error', fontsize = 20)\n",
        "plt.grid()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7bJWGF9zRnc"
      },
      "source": [
        "# ============================================================\n",
        "# ==========================Section 2 ==========================\n",
        "# ============================================================\n",
        "### Compare versus the wind2loads neural net"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HdQbK_iozRnc"
      },
      "outputs": [],
      "source": [
        "error on purpose"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kvVzwFAqzRnc"
      },
      "outputs": [],
      "source": [
        "from w2l import neuralnets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qz34TBv6zRnc"
      },
      "outputs": [],
      "source": [
        "# define the model using the same net architecture, and train for the same number of epochs using the previously batch size as well.\n",
        "w2l_net = neuralnets.ann(layersizes = [input_size, num_hid_1, num_hid_2, output_channels],\n",
        "                       params = {'minibatchsize':64, 'nepochs':500}, \n",
        "                       output_style = 'None',\n",
        "                        testratios = [0.7, 0.3, 0.])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8f_LQU-gzRnd"
      },
      "outputs": [],
      "source": [
        "# train using the data from the first split\n",
        "Outdata = w2l_net.train(X.values,y.values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zKsFN84lzRnd"
      },
      "outputs": [],
      "source": [
        "w2l_net.params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8oSktNjXzRnd"
      },
      "outputs": [],
      "source": [
        "Outdata.keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4JWEI6izRnd"
      },
      "source": [
        "$\\color{red}{\\text{Why 500 epochs tho}}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nsafzKKMzRnd"
      },
      "outputs": [],
      "source": [
        "plt.plot(Outdata['Jhist'][50:])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k75YNlcxzRnd"
      },
      "outputs": [],
      "source": [
        "Yout = w2l_net.predict(X_test.values) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R5m1sXI1zRne"
      },
      "outputs": [],
      "source": [
        "for idx, ch in enumerate(df.columns.tolist()[8:]):\n",
        "  # issues with 2 and 7\n",
        "    plt.figure(idx, figsize = (8,6))\n",
        "    sns.scatterplot(Yout[:,idx],test_targets[:,idx])\n",
        "    sns.scatterplot(predictions[:,idx],test_targets[:,idx])\n",
        "    plt.title(f'DELs: {ch}', fontsize = 20)\n",
        "    plt.legend(['W2L','PyTorch'])\n",
        "    plt.xlabel('Predictions [kNm]', fontsize = 12)\n",
        "    plt.ylabel('Targets [kNm]')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uGzKSU3dzRne"
      },
      "outputs": [],
      "source": [
        "W2L_mse_list = []\n",
        "W2L_r2_score_list = []\n",
        "for i in range(len(df.columns.tolist()[8:])):\n",
        "    #print(f'MSE {AllTargetData.columns[i]} Channel : \\n {mean_squared_error(AllTargetData.values[:,i], Yout[:,i])}')\n",
        "    W2L_mse_list.append(mean_squared_error(test_targets[:,i], Yout[:,i]))\n",
        "    W2L_r2_score_list.append(r2_score(test_targets[:,i], Yout[:,i]))\n",
        "    \n",
        "W2L_Norm_RMSE = np.sqrt(np.array(W2L_mse_list)) / y_test.describe().loc['mean'].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OWxxa9MSzRne"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize= (12,6)) \n",
        "sns.barplot(x= df.columns.tolist()[8:], y= W2L_r2_score_list) \n",
        "plt.ylim([min(W2L_r2_score_list)*0.92,max(W2L_r2_score_list)*1.02])\n",
        "plt.ylabel('R2 score', fontsize = 20)\n",
        "plt.grid()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mczhs3xmzRne"
      },
      "source": [
        "# Compare Wind2Loads net versus PyTorch implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5k0G9KK1zRne"
      },
      "outputs": [],
      "source": [
        "barplot_lst = []\n",
        "for i,ch in enumerate(df.columns.tolist()[8:]):\n",
        "    barplot_lst.append(['pytorch',ch,r2_score_list[i]])\n",
        "for i,ch in enumerate(df.columns.tolist()[8:]):\n",
        "    barplot_lst.append(['W2L',ch,W2L_r2_score_list[i]])  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zCZCFJffzRne"
      },
      "outputs": [],
      "source": [
        "df_comparison = pd.DataFrame(barplot_lst,\n",
        "                  columns=['Model','channel','r2'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IIlsLG7FzRne"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12,6))\n",
        "df_comparison.pivot(\"channel\", \"Model\", \"r2\").plot(kind='bar')\n",
        "plt.ylim([min(r2_score_list)*0.98,max(r2_score_list)*1.02])\n",
        "plt.title('R2', fontsize = 18)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VA-jwSfqzRnf"
      },
      "outputs": [],
      "source": [
        "barplot_lst_NMSE = []\n",
        "for i,ch in enumerate(df.columns.tolist()[8:]):\n",
        "    barplot_lst_NMSE.append(['pytorch',ch , Norm_RMSE[i]])\n",
        "for i,ch in enumerate(df.columns.tolist()[8:]):\n",
        "    barplot_lst_NMSE.append(['W2L', ch, W2L_Norm_RMSE[i]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qsiebtcqzRnf"
      },
      "outputs": [],
      "source": [
        "df_NRMSE_comparison = pd.DataFrame(barplot_lst_NMSE,\n",
        "                  columns=['Model','channel','NRMSE'])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kZT_Jd4uzRnf"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(14,6))\n",
        "df_NRMSE_comparison.pivot(\"channel\", \"Model\", \"NRMSE\").plot(kind='bar')\n",
        "plt.ylim([0,max(W2L_Norm_RMSE)*1.02])\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "PyTorch_Model_v4_Sherpa.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "g6lcj4lLzRnb"
      ],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}