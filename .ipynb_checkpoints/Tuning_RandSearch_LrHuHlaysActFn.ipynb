{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GonMazzini/Loads_Surrogate_Transferability/blob/main/.ipynb_checkpoints/Tuning_RandSearch_LrHuHlaysActFn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9P0l_d61zRnQ"
      },
      "source": [
        "## This notebooks contains the model designed for tuning number of layers and hidden units.\n",
        "\n",
        "### Summary of tuning :\n",
        "Run for just 6 channels:\n",
        "    y = df.iloc[:,8:].drop(['TT_Mx_avg','TT_Mz_avg'], axis = 1)  \n",
        "\n",
        "algorithm \n",
        "    - sherpa.algorithms.RandomSearch(max_num_trials = 32)\n",
        "\n",
        "Parameters = \n",
        "    - sherpa.Discrete('n_hidLayers', [2, 6]),\n",
        "    - sherpa.Discrete('hidden_size', [16, 64]),\n",
        "    - sherpa.Continuous('lr',[0.1,0.001],'log')]\n",
        "## Note\n",
        "    The results are in 6ch_RndmHlHuLr and view Analysis_randomSearch.ipynb for the analysis.\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "l9b4V5nszS9-",
        "outputId": "58b11988-c987-486b-def1-a403d969f4c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting parameter-sherpa\n",
            "  Downloading parameter-sherpa-1.0.6.tar.gz (513 kB)\n",
            "\u001b[K     |████████████████████████████████| 513 kB 4.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas>=0.20.3 in /usr/local/lib/python3.7/dist-packages (from parameter-sherpa) (1.3.5)\n",
            "Requirement already satisfied: pymongo>=3.5.1 in /usr/local/lib/python3.7/dist-packages (from parameter-sherpa) (4.0.2)\n",
            "Requirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python3.7/dist-packages (from parameter-sherpa) (1.21.5)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from parameter-sherpa) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from parameter-sherpa) (1.0.2)\n",
            "Requirement already satisfied: flask>=0.12.2 in /usr/local/lib/python3.7/dist-packages (from parameter-sherpa) (1.1.4)\n",
            "Collecting GPyOpt>=1.2.5\n",
            "  Downloading GPyOpt-1.2.6.tar.gz (56 kB)\n",
            "\u001b[K     |████████████████████████████████| 56 kB 3.6 MB/s \n",
            "\u001b[?25hCollecting enum34\n",
            "  Downloading enum34-1.1.10-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from parameter-sherpa) (3.2.2)\n",
            "Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from flask>=0.12.2->parameter-sherpa) (1.1.0)\n",
            "Requirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from flask>=0.12.2->parameter-sherpa) (2.11.3)\n",
            "Requirement already satisfied: click<8.0,>=5.1 in /usr/local/lib/python3.7/dist-packages (from flask>=0.12.2->parameter-sherpa) (7.1.2)\n",
            "Requirement already satisfied: Werkzeug<2.0,>=0.15 in /usr/local/lib/python3.7/dist-packages (from flask>=0.12.2->parameter-sherpa) (1.0.1)\n",
            "Collecting GPy>=1.8\n",
            "  Downloading GPy-1.10.0.tar.gz (959 kB)\n",
            "\u001b[K     |████████████████████████████████| 959 kB 44.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from GPy>=1.8->GPyOpt>=1.2.5->parameter-sherpa) (1.15.0)\n",
            "Collecting paramz>=0.9.0\n",
            "  Downloading paramz-0.9.5.tar.gz (71 kB)\n",
            "\u001b[K     |████████████████████████████████| 71 kB 6.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cython>=0.29 in /usr/local/lib/python3.7/dist-packages (from GPy>=1.8->GPyOpt>=1.2.5->parameter-sherpa) (0.29.28)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2<3.0,>=2.10.1->flask>=0.12.2->parameter-sherpa) (2.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.20.3->parameter-sherpa) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.20.3->parameter-sherpa) (2018.9)\n",
            "Requirement already satisfied: decorator>=4.0.10 in /usr/local/lib/python3.7/dist-packages (from paramz>=0.9.0->GPy>=1.8->GPyOpt>=1.2.5->parameter-sherpa) (4.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.19.1->parameter-sherpa) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.19.1->parameter-sherpa) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->parameter-sherpa) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->parameter-sherpa) (3.0.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->parameter-sherpa) (1.3.2)\n",
            "Building wheels for collected packages: parameter-sherpa, GPyOpt, GPy, paramz\n",
            "  Building wheel for parameter-sherpa (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for parameter-sherpa: filename=parameter_sherpa-1.0.6-py2.py3-none-any.whl size=542134 sha256=ee43e5e3f08ee5f146fd661615c36e093c36bb3881783e82b707e2df6a425e55\n",
            "  Stored in directory: /root/.cache/pip/wheels/96/d9/cb/99569566e5e9b3ef0265ba4cbce3ff16f7692988833aa942f5\n",
            "  Building wheel for GPyOpt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for GPyOpt: filename=GPyOpt-1.2.6-py3-none-any.whl size=83609 sha256=2577bed854a5904ed248d5c081f5bae823dce45de7cf7f058c4d54a376a48416\n",
            "  Stored in directory: /root/.cache/pip/wheels/e6/fa/d1/f9652b5af79f769a0ab74dbead7c7aea9a93c6bc74543fd3ec\n",
            "  Building wheel for GPy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for GPy: filename=GPy-1.10.0-cp37-cp37m-linux_x86_64.whl size=2565097 sha256=dbc4c0bb4b262d6bc23430291e4f90ca95a8b593ee2b78333e46811dd9f2fc65\n",
            "  Stored in directory: /root/.cache/pip/wheels/f7/18/28/dd1ce0192a81b71a3b086fd952511d088b21e8359ea496860a\n",
            "  Building wheel for paramz (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for paramz: filename=paramz-0.9.5-py3-none-any.whl size=102566 sha256=5cab5ad98658ede85bd3313405484c9d492ab46992fa16f5f80edc27d860d937\n",
            "  Stored in directory: /root/.cache/pip/wheels/c8/95/f5/ce28482da28162e6028c4b3a32c41d147395825b3cd62bc810\n",
            "Successfully built parameter-sherpa GPyOpt GPy paramz\n",
            "Installing collected packages: paramz, GPy, GPyOpt, enum34, parameter-sherpa\n",
            "Successfully installed GPy-1.10.0 GPyOpt-1.2.6 enum34-1.1.10 parameter-sherpa-1.0.6 paramz-0.9.5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "enum"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install parameter-sherpa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Q2oZpZQIzoQT"
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function\n",
        "import sherpa\n",
        "from sherpa.algorithms import Genetic\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "nVCWJUJ2zRnS"
      },
      "outputs": [],
      "source": [
        "import pandas as pd \n",
        "import numpy as np\n",
        "import math\n",
        "from random import shuffle\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zm8YCN4AzRnT",
        "outputId": "1e04d8e1-f8f8-45a7-a030-9cfe2da91a65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU\n"
          ]
        }
      ],
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda:0\")\n",
        "    print(\"GPU\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"CPU\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nF2rjMCtzRnU"
      },
      "source": [
        "# 0- Read the data as a data frame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "id": "W1kffmHZzRnV",
        "outputId": "fa6aec72-54e1-48dc-e352-43441ff4f5be"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Unnamed: 0  pointno          U    SigmaU     Alpha      MannL  MannGamma  \\\n",
              "0           0        1   4.000000  0.100000 -0.650000   7.500000   1.000000   \n",
              "1           1        2  10.150758  1.208656 -0.139692  48.470634   1.363636   \n",
              "\n",
              "   VeerDeltaPhi    TT_Mx_avg   TT_My_avg     TB_Mx_avg    TB_My_avg  \\\n",
              "0    -22.250000   747.561872  200.666288   6708.717789  8861.885588   \n",
              "1     -4.771217  3556.031457  676.339081  16692.647572  6329.099515   \n",
              "\n",
              "     TT_Mz_avg    MS_Mz_avg     BR_Mx_avg     BR_My_avg  \n",
              "0   819.209904    63.457528   4253.317748  15006.726860  \n",
              "1  3746.460605  1354.995442  10409.290476  16289.414152  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-48029be3-f1af-4391-a732-1d897e5be447\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>pointno</th>\n",
              "      <th>U</th>\n",
              "      <th>SigmaU</th>\n",
              "      <th>Alpha</th>\n",
              "      <th>MannL</th>\n",
              "      <th>MannGamma</th>\n",
              "      <th>VeerDeltaPhi</th>\n",
              "      <th>TT_Mx_avg</th>\n",
              "      <th>TT_My_avg</th>\n",
              "      <th>TB_Mx_avg</th>\n",
              "      <th>TB_My_avg</th>\n",
              "      <th>TT_Mz_avg</th>\n",
              "      <th>MS_Mz_avg</th>\n",
              "      <th>BR_Mx_avg</th>\n",
              "      <th>BR_My_avg</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>-0.650000</td>\n",
              "      <td>7.500000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-22.250000</td>\n",
              "      <td>747.561872</td>\n",
              "      <td>200.666288</td>\n",
              "      <td>6708.717789</td>\n",
              "      <td>8861.885588</td>\n",
              "      <td>819.209904</td>\n",
              "      <td>63.457528</td>\n",
              "      <td>4253.317748</td>\n",
              "      <td>15006.726860</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>10.150758</td>\n",
              "      <td>1.208656</td>\n",
              "      <td>-0.139692</td>\n",
              "      <td>48.470634</td>\n",
              "      <td>1.363636</td>\n",
              "      <td>-4.771217</td>\n",
              "      <td>3556.031457</td>\n",
              "      <td>676.339081</td>\n",
              "      <td>16692.647572</td>\n",
              "      <td>6329.099515</td>\n",
              "      <td>3746.460605</td>\n",
              "      <td>1354.995442</td>\n",
              "      <td>10409.290476</td>\n",
              "      <td>16289.414152</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-48029be3-f1af-4391-a732-1d897e5be447')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-48029be3-f1af-4391-a732-1d897e5be447 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-48029be3-f1af-4391-a732-1d897e5be447');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "df = pd.read_excel('LoadsDataBase_6D_Set123_FiltMinMaxCrit.xlsx') # Average the values from Set1,Set2 and Set3.\n",
        "df.head(2)\n",
        "# 0 : TT_Mx_avg\n",
        "# 1 : TT_My_avg\n",
        "# 2 : TB_Mx_avg\n",
        "# 3 : TB_My_avg\n",
        "# 4 : MS_Mz_avg\n",
        "# 5 : BR_Mx_avg\n",
        "# 6 : BR_My_avg\n",
        "# 7 : TT-Mz_avg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVi4tQ09zRnV"
      },
      "source": [
        "# ============================================================\n",
        "# ==========================Section 1 ==========================\n",
        "# ============================================================"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "gTxt7cZJzRnV"
      },
      "outputs": [],
      "source": [
        "X = df.iloc[:,2:8]\n",
        "y = df.iloc[:,8:] # .drop(['TT_Mx_avg','TT_Mz_avg'], axis = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eX_LPh9wzRnW",
        "outputId": "9f4ef0ca-48d9-4bf2-cd02-668864d975a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The filtered data set consits on: 7664 entries.\n",
            "A total of 6131 will be used for training and validation.\n",
            "A total of 1533 will be used for testing the final model.\n"
          ]
        }
      ],
      "source": [
        "# Test split:\n",
        "X, X_test, y, y_test = train_test_split(X,y, test_size = 0.2, shuffle = True,  random_state = 101)\n",
        "\n",
        "print(f'The filtered data set consits on: {len(df)} entries.')\n",
        "print(f'A total of {len(X)} will be used for training and validation.')\n",
        "print(f'A total of {len(X_test)} will be used for testing the final model.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6sjTHp4zRnW"
      },
      "source": [
        "### From now on, \"X\" and \"y\" will be used for train-validate the model. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "4nBJ9T1TzRnW"
      },
      "outputs": [],
      "source": [
        "feature_range = (0, 1)\n",
        "scaler_x = preprocessing.MinMaxScaler(feature_range=feature_range).fit(X)\n",
        "X_scaled = scaler_x.transform(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fp5EhbtozRnW"
      },
      "source": [
        "### Separte between train and validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "_kR4yaL0zRnX"
      },
      "outputs": [],
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(X_scaled,y.values, test_size = 0.2, shuffle = True,  random_state = 101)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ezp2exU1NR36",
        "outputId": "217e7130-e59a-4cc0-dd38-15c0e644789a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A total of 4904 for training, 64.0 % of total data\n",
            "A total of 1227 for validation, 16.0 % of total data\n",
            "A total of 1533 for testing, 20.0 % of total data\n"
          ]
        }
      ],
      "source": [
        "# printing number of samples for train-validation-test\n",
        "print(f'A total of {y_train.shape[0]} for training, {round(100*y_train.shape[0]/len(df),1)} % of total data')\n",
        "print(f'A total of {y_val.shape[0]} for validation, {round(100*y_val.shape[0]/len(df),1)} % of total data')\n",
        "print(f'A total of {y_test.shape[0]} for testing, {round(100*y_test.shape[0]/len(df),1)} % of total data')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Y6KSzGizRnX"
      },
      "source": [
        "### The PyTorch worfklow can be summarized as follows:\n",
        "- 1) Design model (input, output size, forward pass)\n",
        "- 2) Construct loss and optimizer\n",
        "- 3) Training loop\n",
        "   - forward pass: compute prediction based on the current weights and biases of the net\n",
        "   - backward pass: compute the gradients of the loss function wrt. to model parameters\n",
        "   - update weigths in"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "hVfzqEXbzRnX"
      },
      "outputs": [],
      "source": [
        "input_size = np.shape(X_train)[1]             # np.shape(X_train)[1]\n",
        "output_channels = np.shape(y_train)[1]        # np.shape(y_train)[1]\n",
        "#hidden_size = 50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "QS60F1Q3vBYi"
      },
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "    \"\"\" A feedforward network designed for tuning number of layers and hidden units.\n",
        "    By @GonMazzini\"\"\"\n",
        "    def __init__(self, input_dim = 6, output_dim = 8, n_hidLayers = 2, hidden_size = 50, act_fn = F.relu):\n",
        "        super(MLP, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.hidden_size = hidden_size\n",
        "        self.n_hidLayers = n_hidLayers\n",
        "        self.act_fn = act_fn\n",
        "\n",
        "        \n",
        "        current_dim = input_dim\n",
        "        self.layers = nn.ModuleList()\n",
        "        \n",
        "        for hdim in [self.hidden_size]*self.n_hidLayers:\n",
        "            self.layers.append(nn.Linear(current_dim, hdim))\n",
        "            current_dim = hdim\n",
        "        self.layers.append(nn.Linear(current_dim, output_dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.layers[:-1]:\n",
        "            x = self.act_fn(layer(x))\n",
        "        out = self.layers[-1](x)  # should apply act-fn ?\n",
        "        return out "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hr0F9U6hxA1O",
        "outputId": "2d5e7f43-1ce4-4242-b8ce-b9ca32add494"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLP(\n",
              "  (layers): ModuleList(\n",
              "    (0): Linear(in_features=6, out_features=50, bias=True)\n",
              "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
              "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
              "    (3): Linear(in_features=50, out_features=8, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "# test dummy forward\n",
        "model = MLP(input_size,output_channels,3,50) \n",
        "model(torch.tensor(X_train[0]).float())\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "kQJG0CJ8zRnY"
      },
      "outputs": [],
      "source": [
        "# instantiate the class\n",
        "#model = Net(hidden_size)  # will be instantiated inside HP loop\n",
        "\n",
        "# Define the loss function (mean square error)\n",
        "loss = nn.MSELoss()\n",
        "\n",
        "# Instantiate optimizer passing the net parameters as argument, and learning rate\n",
        "#optimizer = optim.Adam(model.parameters(), lr = 0.01)  # will be instantiated inside HypParam loop \n",
        "\n",
        "# list to store results\n",
        "train_losses , val_losses= [],[]\n",
        "\n",
        "# Try a dummy forward\n",
        "#model(torch.tensor(X_train[0]).float())  # beware that need to convert from double to float"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xO8OBaiRzRnY"
      },
      "source": [
        "# 2.1- Use the PyTorch DataLoader and Dataset utils.\n",
        "- DataLoader class combines a dataset and a sampler, and provides an iterable over the given dataset for training the model\n",
        "- Dataset: just an abstract class representing a :class:`Dataset`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "guVFdZgMzRnY"
      },
      "outputs": [],
      "source": [
        "class FatigueLoads_TrainSet(Dataset):\n",
        "\n",
        "    def __init__(self):\n",
        "        self.n_samples = X_train.shape[0]\n",
        "        self.x_data = torch.from_numpy(X_train) # size [n_samples, n_features]\n",
        "        self.y_data = torch.from_numpy(y_train) # size [n_samples, 1]\n",
        "\n",
        "    # support indexing such that dataset[i] can be used to get i-th sample\n",
        "    def __getitem__(self, index):\n",
        "        return self.x_data[index], self.y_data[index]\n",
        "\n",
        "    # we can call len(dataset) to return the size\n",
        "    def __len__(self):\n",
        "        return self.n_samples\n",
        "    \n",
        "class FatigueLoads_ValidationSet(Dataset):\n",
        "\n",
        "    def __init__(self):\n",
        "        self.n_samples = X_val.shape[0]\n",
        "        self.x_data = torch.from_numpy(X_val) # size [n_samples, n_features]\n",
        "        self.y_data = torch.from_numpy(y_val) # size [n_samples, 1]\n",
        "\n",
        "    # support indexing such that dataset[i] can be used to get i-th sample\n",
        "    def __getitem__(self, index):\n",
        "        return self.x_data[index], self.y_data[index]\n",
        "\n",
        "    # we can call len(dataset) to return the size\n",
        "    def __len__(self):\n",
        "        return self.n_samples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sp46xHZVzRnZ"
      },
      "source": [
        "### Get first sample and unpack. Note that the enviromental inputs are normalized using MinMaxScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zALPQBILzRnZ",
        "outputId": "42d4eea5-e367-43c7-86e5-aeb70dfb5cee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.9213, 0.5395, 0.2971, 0.1574, 0.7369, 0.3075], dtype=torch.float64) tensor([ 7116.1376,   808.7441, 21966.9989, 16383.7768,  7347.3186,   807.4632,\n",
            "        18567.6736, 16935.8620], dtype=torch.float64)\n"
          ]
        }
      ],
      "source": [
        "train_dataset = FatigueLoads_TrainSet()\n",
        "valid_dataset = FatigueLoads_ValidationSet()\n",
        "\n",
        "first_data = train_dataset[0]\n",
        "features, loads = first_data\n",
        "print(features, loads)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "323d_UnszRnZ",
        "outputId": "aaf6dc00-7983-440c-df96-1a410fefe178"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batches train: 38\n",
            "batches test:  9\n"
          ]
        }
      ],
      "source": [
        "batch_size = 128\n",
        "num_epochs = 600\n",
        "\n",
        "num_batches_train = X_train.shape[0] // batch_size\n",
        "num_batches_test = X_val.shape[0] // batch_size\n",
        "print(f'batches train: {num_batches_train}')\n",
        "print(f'batches test:  {num_batches_test}')\n",
        "\n",
        "train_loader = DataLoader(dataset=train_dataset,\n",
        "                          batch_size=batch_size,\n",
        "                          shuffle=True,\n",
        "                          num_workers=0)\n",
        "\n",
        "valid_loader = DataLoader(dataset=valid_dataset,\n",
        "                          batch_size=batch_size,\n",
        "                          shuffle=True,\n",
        "                          num_workers=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "5KXldvsU08MV"
      },
      "outputs": [],
      "source": [
        "from itertools import repeat\n",
        "#algorithm = sherpa.algorithms.GridSearch(num_grid_points=3)\n",
        "algorithm = sherpa.algorithms.RandomSearch(max_num_trials = 32)\n",
        "# algorithm = sherpa.algorithms.RandomSearch(max_num_trials=15)\n",
        "\n",
        "# parameters = [sherpa.Discrete('hidden_size', [16,512]),\n",
        "#               sherpa.Continuous('lr',[0.001,0.1], scale='log')]\n",
        "\n",
        "parameters = [sherpa.Discrete('n_hidLayers', [1, 4]),\n",
        "              sherpa.Discrete('hidden_size', [16, 512]),\n",
        "              sherpa.Continuous('lr',[0.1,0.001],'log'),\n",
        "              sherpa.Choice('activation', [F.relu, F.tanh])]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXK9eWS81rHZ",
        "outputId": "c9559635-12c6-41ad-d751-0845fc2c7cf7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:sherpa.core:\n",
            "-------------------------------------------------------\n",
            "SHERPA Dashboard running. Access via\n",
            "http://172.28.0.2:8880 if on a cluster or\n",
            "http://localhost:8880 if running locally.\n",
            "-------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Serving Flask app \"sherpa.app.app\" (lazy loading)\n",
            " * Environment: production\n",
            "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
            "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
            " * Debug mode: on\n"
          ]
        }
      ],
      "source": [
        "study = sherpa.Study(parameters=parameters,\n",
        "                     algorithm=algorithm,\n",
        "                     lower_is_better=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GnOKI0762M9u",
        "outputId": "a0966533-201c-413e-f6ae-bae6765aab33"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 1:\t{'n_hidLayers': 3, 'hidden_size': 487, 'lr': 0.003129431672526212, 'activation': <function tanh at 0x7f7bd37c4290>}\n",
            "Epoch: 1/600 | Train loss: 187971       | Val loss 204964\n",
            "Epoch: 21/600 | Train loss: 166042       | Val loss 181337\n",
            "Epoch: 41/600 | Train loss: 147729       | Val loss 161705\n",
            "Epoch: 61/600 | Train loss: 130939       | Val loss 142012\n",
            "Epoch: 81/600 | Train loss: 116401       | Val loss 126090\n",
            "Epoch: 101/600 | Train loss: 102775       | Val loss 113367\n",
            "Epoch: 121/600 | Train loss: 91619       | Val loss 99959\n",
            "Epoch: 141/600 | Train loss: 81446       | Val loss 88473\n",
            "Epoch: 161/600 | Train loss: 72137       | Val loss 78600\n",
            "Epoch: 181/600 | Train loss: 64433       | Val loss 70966\n",
            "Epoch: 201/600 | Train loss: 57635       | Val loss 62507\n",
            "Epoch: 221/600 | Train loss: 51604       | Val loss 56248\n",
            "Epoch: 241/600 | Train loss: 47361       | Val loss 52308\n",
            "Epoch: 261/600 | Train loss: 43223       | Val loss 48000\n",
            "Epoch: 281/600 | Train loss: 40529       | Val loss 44330\n",
            "Epoch: 301/600 | Train loss: 37875       | Val loss 41476\n",
            "Epoch: 321/600 | Train loss: 36262       | Val loss 39574\n",
            "Epoch: 341/600 | Train loss: 34927       | Val loss 38597\n",
            "Epoch: 361/600 | Train loss: 34092       | Val loss 37542\n",
            "Epoch: 381/600 | Train loss: 33600       | Val loss 36452\n",
            "Epoch: 401/600 | Train loss: 33206       | Val loss 36479\n",
            "Epoch: 421/600 | Train loss: 32914       | Val loss 36012\n",
            "Epoch: 441/600 | Train loss: 32871       | Val loss 35870\n",
            "Epoch: 461/600 | Train loss: 32741       | Val loss 35795\n",
            "Epoch: 481/600 | Train loss: 32847       | Val loss 35692\n",
            "Epoch: 501/600 | Train loss: 32728       | Val loss 35887\n",
            "Epoch: 521/600 | Train loss: 32964       | Val loss 35790\n",
            "Epoch: 541/600 | Train loss: 32819       | Val loss 35539\n",
            "Epoch: 561/600 | Train loss: 32767       | Val loss 35681\n",
            "Epoch: 581/600 | Train loss: 32995       | Val loss 35672\n",
            "Trial 2:\t{'n_hidLayers': 2, 'hidden_size': 153, 'lr': 0.004567742796003811, 'activation': <function relu at 0x7f7bd37c17a0>}\n",
            "Epoch: 1/600 | Train loss: 183094       | Val loss 199239\n",
            "Epoch: 21/600 | Train loss: 14742       | Val loss 16037\n",
            "Epoch: 41/600 | Train loss: 14266       | Val loss 15168\n",
            "Epoch: 61/600 | Train loss: 13109       | Val loss 13975\n",
            "Epoch: 81/600 | Train loss: 7665       | Val loss 8126\n",
            "Epoch: 101/600 | Train loss: 5093       | Val loss 5302\n",
            "Epoch: 121/600 | Train loss: 4372       | Val loss 4577\n",
            "Epoch: 141/600 | Train loss: 3964       | Val loss 4165\n",
            "Epoch: 161/600 | Train loss: 3547       | Val loss 3729\n",
            "Epoch: 181/600 | Train loss: 2856       | Val loss 3051\n",
            "Epoch: 201/600 | Train loss: 2355       | Val loss 2523\n",
            "Epoch: 221/600 | Train loss: 2132       | Val loss 2248\n",
            "Epoch: 241/600 | Train loss: 1933       | Val loss 2030\n",
            "Epoch: 261/600 | Train loss: 1786       | Val loss 1928\n",
            "Epoch: 281/600 | Train loss: 1710       | Val loss 1809\n",
            "Epoch: 301/600 | Train loss: 1606       | Val loss 1690\n",
            "Epoch: 321/600 | Train loss: 1528       | Val loss 1658\n",
            "Epoch: 341/600 | Train loss: 1456       | Val loss 1566\n",
            "Epoch: 361/600 | Train loss: 1390       | Val loss 1521\n",
            "Epoch: 381/600 | Train loss: 1322       | Val loss 1464\n",
            "Epoch: 401/600 | Train loss: 1272       | Val loss 1406\n",
            "Epoch: 421/600 | Train loss: 1191       | Val loss 1377\n",
            "Epoch: 441/600 | Train loss: 1134       | Val loss 1277\n",
            "Epoch: 461/600 | Train loss: 1075       | Val loss 1251\n",
            "Epoch: 481/600 | Train loss: 1026       | Val loss 1195\n",
            "Epoch: 501/600 | Train loss: 936       | Val loss 1117\n",
            "Epoch: 521/600 | Train loss: 879       | Val loss 1046\n",
            "Epoch: 541/600 | Train loss: 832       | Val loss 991\n",
            "Epoch: 561/600 | Train loss: 786       | Val loss 932\n",
            "Epoch: 581/600 | Train loss: 721       | Val loss 869\n",
            "Trial 3:\t{'n_hidLayers': 2, 'hidden_size': 81, 'lr': 0.015588241834819229, 'activation': <function relu at 0x7f7bd37c17a0>}\n",
            "Epoch: 1/600 | Train loss: 146568       | Val loss 159295\n",
            "Epoch: 21/600 | Train loss: 14214       | Val loss 15195\n",
            "Epoch: 41/600 | Train loss: 7966       | Val loss 8604\n",
            "Epoch: 61/600 | Train loss: 4964       | Val loss 5208\n",
            "Epoch: 81/600 | Train loss: 4443       | Val loss 4596\n",
            "Epoch: 101/600 | Train loss: 3984       | Val loss 4234\n",
            "Epoch: 121/600 | Train loss: 3182       | Val loss 3408\n",
            "Epoch: 141/600 | Train loss: 2414       | Val loss 2553\n",
            "Epoch: 161/600 | Train loss: 2096       | Val loss 2265\n",
            "Epoch: 181/600 | Train loss: 2070       | Val loss 2200\n",
            "Epoch: 201/600 | Train loss: 1939       | Val loss 2069\n",
            "Epoch: 221/600 | Train loss: 1779       | Val loss 1879\n",
            "Epoch: 241/600 | Train loss: 1404       | Val loss 1520\n",
            "Epoch: 261/600 | Train loss: 1109       | Val loss 1242\n",
            "Epoch: 281/600 | Train loss: 886       | Val loss 1026\n",
            "Epoch: 301/600 | Train loss: 799       | Val loss 959\n",
            "Epoch: 321/600 | Train loss: 731       | Val loss 854\n",
            "Epoch: 341/600 | Train loss: 720       | Val loss 879\n",
            "Epoch: 361/600 | Train loss: 645       | Val loss 778\n",
            "Epoch: 381/600 | Train loss: 628       | Val loss 752\n",
            "Epoch: 401/600 | Train loss: 626       | Val loss 757\n",
            "Epoch: 421/600 | Train loss: 596       | Val loss 717\n",
            "Epoch: 441/600 | Train loss: 578       | Val loss 713\n",
            "Epoch: 461/600 | Train loss: 568       | Val loss 695\n",
            "Epoch: 481/600 | Train loss: 568       | Val loss 701\n",
            "Epoch: 501/600 | Train loss: 549       | Val loss 668\n",
            "Epoch: 521/600 | Train loss: 539       | Val loss 673\n",
            "Epoch: 541/600 | Train loss: 523       | Val loss 656\n",
            "Epoch: 561/600 | Train loss: 528       | Val loss 646\n",
            "Epoch: 581/600 | Train loss: 511       | Val loss 632\n",
            "Trial 4:\t{'n_hidLayers': 3, 'hidden_size': 19, 'lr': 0.0015044803812006254, 'activation': <function relu at 0x7f7bd37c17a0>}\n",
            "Epoch: 1/600 | Train loss: 188884       | Val loss 204572\n",
            "Epoch: 21/600 | Train loss: 29308       | Val loss 31655\n",
            "Epoch: 41/600 | Train loss: 23033       | Val loss 25196\n",
            "Epoch: 61/600 | Train loss: 16833       | Val loss 18374\n",
            "Epoch: 81/600 | Train loss: 14672       | Val loss 15877\n",
            "Epoch: 101/600 | Train loss: 14481       | Val loss 15642\n",
            "Epoch: 121/600 | Train loss: 14296       | Val loss 15458\n",
            "Epoch: 141/600 | Train loss: 14341       | Val loss 15364\n",
            "Epoch: 161/600 | Train loss: 14324       | Val loss 15418\n",
            "Epoch: 181/600 | Train loss: 14273       | Val loss 15290\n",
            "Epoch: 201/600 | Train loss: 14357       | Val loss 15385\n",
            "Epoch: 221/600 | Train loss: 14194       | Val loss 15311\n",
            "Epoch: 241/600 | Train loss: 14059       | Val loss 15253\n",
            "Epoch: 261/600 | Train loss: 13872       | Val loss 14982\n",
            "Epoch: 281/600 | Train loss: 13586       | Val loss 14584\n",
            "Epoch: 301/600 | Train loss: 13556       | Val loss 14516\n",
            "Epoch: 321/600 | Train loss: 13457       | Val loss 14362\n",
            "Epoch: 341/600 | Train loss: 13405       | Val loss 14212\n",
            "Epoch: 361/600 | Train loss: 13330       | Val loss 14336\n",
            "Epoch: 381/600 | Train loss: 13334       | Val loss 14300\n",
            "Epoch: 401/600 | Train loss: 13377       | Val loss 14471\n",
            "Epoch: 421/600 | Train loss: 13338       | Val loss 14259\n",
            "Epoch: 441/600 | Train loss: 13315       | Val loss 14340\n",
            "Epoch: 461/600 | Train loss: 13261       | Val loss 14276\n",
            "Epoch: 481/600 | Train loss: 13252       | Val loss 14254\n",
            "Epoch: 501/600 | Train loss: 13340       | Val loss 14158\n",
            "Epoch: 521/600 | Train loss: 13299       | Val loss 14192\n",
            "Epoch: 541/600 | Train loss: 13201       | Val loss 14240\n",
            "Epoch: 561/600 | Train loss: 13178       | Val loss 14354\n",
            "Epoch: 581/600 | Train loss: 13146       | Val loss 14157\n",
            "Trial 5:\t{'n_hidLayers': 1, 'hidden_size': 335, 'lr': 0.03591117492378989, 'activation': <function tanh at 0x7f7bd37c4290>}\n",
            "Epoch: 1/600 | Train loss: 179804       | Val loss 195963\n",
            "Epoch: 21/600 | Train loss: 73240       | Val loss 80879\n",
            "Epoch: 41/600 | Train loss: 40897       | Val loss 45198\n",
            "Epoch: 61/600 | Train loss: 24735       | Val loss 27230\n",
            "Epoch: 81/600 | Train loss: 17198       | Val loss 18945\n",
            "Epoch: 101/600 | Train loss: 13618       | Val loss 14807\n",
            "Epoch: 121/600 | Train loss: 11598       | Val loss 12400\n",
            "Epoch: 141/600 | Train loss: 10193       | Val loss 10929\n",
            "Epoch: 161/600 | Train loss: 8778       | Val loss 9599\n",
            "Epoch: 181/600 | Train loss: 7166       | Val loss 7830\n",
            "Epoch: 201/600 | Train loss: 5832       | Val loss 6351\n",
            "Epoch: 221/600 | Train loss: 4765       | Val loss 5241\n",
            "Epoch: 241/600 | Train loss: 3971       | Val loss 4405\n",
            "Epoch: 261/600 | Train loss: 3334       | Val loss 3813\n",
            "Epoch: 281/600 | Train loss: 2869       | Val loss 3233\n",
            "Epoch: 301/600 | Train loss: 2483       | Val loss 2888\n",
            "Epoch: 321/600 | Train loss: 2163       | Val loss 2538\n",
            "Epoch: 341/600 | Train loss: 1902       | Val loss 2281\n",
            "Epoch: 361/600 | Train loss: 1701       | Val loss 2045\n",
            "Epoch: 381/600 | Train loss: 1523       | Val loss 1857\n",
            "Epoch: 401/600 | Train loss: 1365       | Val loss 1708\n",
            "Epoch: 421/600 | Train loss: 1239       | Val loss 1585\n",
            "Epoch: 441/600 | Train loss: 1141       | Val loss 1466\n",
            "Epoch: 461/600 | Train loss: 1063       | Val loss 1399\n",
            "Epoch: 481/600 | Train loss: 1029       | Val loss 1317\n",
            "Epoch: 501/600 | Train loss: 917       | Val loss 1287\n",
            "Epoch: 521/600 | Train loss: 867       | Val loss 1189\n",
            "Epoch: 541/600 | Train loss: 814       | Val loss 1129\n",
            "Epoch: 561/600 | Train loss: 760       | Val loss 1070\n",
            "Epoch: 581/600 | Train loss: 719       | Val loss 1017\n",
            "Trial 6:\t{'n_hidLayers': 3, 'hidden_size': 343, 'lr': 0.027659506169512665, 'activation': <function relu at 0x7f7bd37c17a0>}\n",
            "Epoch: 1/600 | Train loss: 15768       | Val loss 17330\n",
            "Epoch: 21/600 | Train loss: 1584       | Val loss 1758\n",
            "Epoch: 41/600 | Train loss: 590       | Val loss 722\n",
            "Epoch: 61/600 | Train loss: 772       | Val loss 935\n",
            "Epoch: 81/600 | Train loss: 370       | Val loss 484\n",
            "Epoch: 101/600 | Train loss: 532       | Val loss 683\n",
            "Epoch: 121/600 | Train loss: 674       | Val loss 815\n",
            "Epoch: 141/600 | Train loss: 324       | Val loss 438\n",
            "Epoch: 161/600 | Train loss: 371       | Val loss 487\n",
            "Epoch: 181/600 | Train loss: 400       | Val loss 545\n",
            "Epoch: 201/600 | Train loss: 250       | Val loss 394\n",
            "Epoch: 221/600 | Train loss: 205       | Val loss 345\n",
            "Epoch: 241/600 | Train loss: 223       | Val loss 357\n",
            "Epoch: 261/600 | Train loss: 189       | Val loss 333\n",
            "Epoch: 281/600 | Train loss: 223       | Val loss 422\n",
            "Epoch: 301/600 | Train loss: 212       | Val loss 388\n",
            "Epoch: 321/600 | Train loss: 201       | Val loss 391\n",
            "Epoch: 341/600 | Train loss: 194       | Val loss 409\n",
            "Epoch: 361/600 | Train loss: 146       | Val loss 339\n",
            "Epoch: 381/600 | Train loss: 153       | Val loss 391\n",
            "Epoch: 401/600 | Train loss: 175       | Val loss 394\n",
            "Epoch: 421/600 | Train loss: 130       | Val loss 362\n",
            "Epoch: 441/600 | Train loss: 120       | Val loss 372\n",
            "Epoch: 461/600 | Train loss: 109       | Val loss 369\n",
            "Epoch: 481/600 | Train loss: 106       | Val loss 368\n",
            "Epoch: 501/600 | Train loss: 87       | Val loss 366\n",
            "Epoch: 521/600 | Train loss: 111       | Val loss 391\n",
            "Epoch: 541/600 | Train loss: 89       | Val loss 375\n",
            "Epoch: 561/600 | Train loss: 73       | Val loss 375\n",
            "Epoch: 581/600 | Train loss: 70       | Val loss 378\n",
            "Trial 7:\t{'n_hidLayers': 2, 'hidden_size': 424, 'lr': 0.01364147922411774, 'activation': <function tanh at 0x7f7bd37c4290>}\n",
            "Epoch: 1/600 | Train loss: 184622       | Val loss 201378\n",
            "Epoch: 21/600 | Train loss: 117545       | Val loss 129727\n",
            "Epoch: 41/600 | Train loss: 77930       | Val loss 84755\n",
            "Epoch: 61/600 | Train loss: 54215       | Val loss 59360\n",
            "Epoch: 81/600 | Train loss: 41654       | Val loss 45537\n",
            "Epoch: 101/600 | Train loss: 35941       | Val loss 39907\n",
            "Epoch: 121/600 | Train loss: 33615       | Val loss 37059\n",
            "Epoch: 141/600 | Train loss: 32981       | Val loss 36077\n",
            "Epoch: 161/600 | Train loss: 33123       | Val loss 36278\n",
            "Epoch: 181/600 | Train loss: 32771       | Val loss 35982\n",
            "Epoch: 201/600 | Train loss: 29183       | Val loss 32118\n",
            "Epoch: 221/600 | Train loss: 27187       | Val loss 29581\n",
            "Epoch: 241/600 | Train loss: 26036       | Val loss 28669\n",
            "Epoch: 261/600 | Train loss: 24932       | Val loss 26858\n",
            "Epoch: 281/600 | Train loss: 23552       | Val loss 25505\n",
            "Epoch: 301/600 | Train loss: 13324       | Val loss 14252\n",
            "Epoch: 321/600 | Train loss: 9556       | Val loss 10408\n",
            "Epoch: 341/600 | Train loss: 6982       | Val loss 7561\n",
            "Epoch: 361/600 | Train loss: 5181       | Val loss 5812\n",
            "Epoch: 381/600 | Train loss: 3914       | Val loss 4506\n",
            "Epoch: 401/600 | Train loss: 2975       | Val loss 3472\n",
            "Epoch: 421/600 | Train loss: 2245       | Val loss 2657\n",
            "Epoch: 441/600 | Train loss: 1790       | Val loss 2170\n",
            "Epoch: 461/600 | Train loss: 1419       | Val loss 1755\n",
            "Epoch: 481/600 | Train loss: 1094       | Val loss 1431\n",
            "Epoch: 501/600 | Train loss: 918       | Val loss 1257\n",
            "Epoch: 521/600 | Train loss: 737       | Val loss 1086\n",
            "Epoch: 541/600 | Train loss: 628       | Val loss 940\n",
            "Epoch: 561/600 | Train loss: 554       | Val loss 854\n",
            "Epoch: 581/600 | Train loss: 493       | Val loss 747\n",
            "Trial 8:\t{'n_hidLayers': 1, 'hidden_size': 156, 'lr': 0.0010204275095212303, 'activation': <function tanh at 0x7f7bd37c4290>}\n",
            "Epoch: 1/600 | Train loss: 189530       | Val loss 205621\n",
            "Epoch: 21/600 | Train loss: 186237       | Val loss 202484\n",
            "Epoch: 41/600 | Train loss: 183504       | Val loss 198886\n",
            "Epoch: 61/600 | Train loss: 180710       | Val loss 196868\n",
            "Epoch: 81/600 | Train loss: 178758       | Val loss 195561\n",
            "Epoch: 101/600 | Train loss: 176520       | Val loss 191933\n",
            "Epoch: 121/600 | Train loss: 174206       | Val loss 189600\n",
            "Epoch: 141/600 | Train loss: 172095       | Val loss 187853\n",
            "Epoch: 161/600 | Train loss: 169511       | Val loss 184162\n",
            "Epoch: 181/600 | Train loss: 167292       | Val loss 182124\n",
            "Epoch: 201/600 | Train loss: 165180       | Val loss 179474\n",
            "Epoch: 221/600 | Train loss: 163322       | Val loss 177454\n",
            "Epoch: 241/600 | Train loss: 161061       | Val loss 175042\n",
            "Epoch: 261/600 | Train loss: 159119       | Val loss 173702\n",
            "Epoch: 281/600 | Train loss: 157224       | Val loss 170348\n",
            "Epoch: 301/600 | Train loss: 154417       | Val loss 167476\n",
            "Epoch: 321/600 | Train loss: 153002       | Val loss 166060\n",
            "Epoch: 341/600 | Train loss: 150777       | Val loss 165373\n",
            "Epoch: 361/600 | Train loss: 148970       | Val loss 161872\n",
            "Epoch: 381/600 | Train loss: 146776       | Val loss 159115\n",
            "Epoch: 401/600 | Train loss: 145443       | Val loss 157989\n",
            "Epoch: 421/600 | Train loss: 143272       | Val loss 155809\n",
            "Epoch: 441/600 | Train loss: 140815       | Val loss 154918\n",
            "Epoch: 461/600 | Train loss: 139249       | Val loss 150754\n",
            "Epoch: 481/600 | Train loss: 137518       | Val loss 149445\n",
            "Epoch: 501/600 | Train loss: 135477       | Val loss 147383\n",
            "Epoch: 521/600 | Train loss: 134370       | Val loss 145665\n",
            "Epoch: 541/600 | Train loss: 132175       | Val loss 143945\n",
            "Epoch: 561/600 | Train loss: 130322       | Val loss 141586\n",
            "Epoch: 581/600 | Train loss: 128131       | Val loss 138619\n",
            "Trial 9:\t{'n_hidLayers': 3, 'hidden_size': 104, 'lr': 0.08279084396063215, 'activation': <function tanh at 0x7f7bd37c4290>}\n",
            "Epoch: 1/600 | Train loss: 182954       | Val loss 198825\n",
            "Epoch: 21/600 | Train loss: 94574       | Val loss 104193\n",
            "Epoch: 41/600 | Train loss: 54737       | Val loss 60045\n",
            "Epoch: 61/600 | Train loss: 38848       | Val loss 42975\n",
            "Epoch: 81/600 | Train loss: 34193       | Val loss 37373\n",
            "Epoch: 101/600 | Train loss: 32884       | Val loss 36209\n",
            "Epoch: 121/600 | Train loss: 32880       | Val loss 35933\n",
            "Epoch: 141/600 | Train loss: 32835       | Val loss 36132\n",
            "Epoch: 161/600 | Train loss: 32873       | Val loss 35755\n",
            "Epoch: 181/600 | Train loss: 32922       | Val loss 35863\n",
            "Epoch: 201/600 | Train loss: 32818       | Val loss 36248\n",
            "Epoch: 221/600 | Train loss: 32792       | Val loss 35764\n",
            "Epoch: 241/600 | Train loss: 32907       | Val loss 36123\n",
            "Epoch: 261/600 | Train loss: 32817       | Val loss 35526\n",
            "Epoch: 281/600 | Train loss: 32879       | Val loss 35582\n",
            "Epoch: 301/600 | Train loss: 32964       | Val loss 36157\n",
            "Epoch: 321/600 | Train loss: 32621       | Val loss 35761\n",
            "Epoch: 341/600 | Train loss: 32848       | Val loss 35928\n",
            "Epoch: 361/600 | Train loss: 32851       | Val loss 35637\n",
            "Epoch: 381/600 | Train loss: 32889       | Val loss 35868\n",
            "Epoch: 401/600 | Train loss: 32807       | Val loss 36223\n",
            "Epoch: 421/600 | Train loss: 32820       | Val loss 36252\n",
            "Epoch: 441/600 | Train loss: 32863       | Val loss 35793\n",
            "Epoch: 461/600 | Train loss: 32855       | Val loss 35665\n",
            "Epoch: 481/600 | Train loss: 33025       | Val loss 35779\n",
            "Epoch: 501/600 | Train loss: 32933       | Val loss 36151\n",
            "Epoch: 521/600 | Train loss: 32845       | Val loss 35433\n",
            "Epoch: 541/600 | Train loss: 32938       | Val loss 36368\n",
            "Epoch: 561/600 | Train loss: 32827       | Val loss 35809\n",
            "Epoch: 581/600 | Train loss: 32800       | Val loss 35563\n",
            "Trial 10:\t{'n_hidLayers': 1, 'hidden_size': 208, 'lr': 0.006527724826692302, 'activation': <function tanh at 0x7f7bd37c4290>}\n",
            "Epoch: 1/600 | Train loss: 188655       | Val loss 204984\n",
            "Epoch: 21/600 | Train loss: 167355       | Val loss 182078\n",
            "Epoch: 41/600 | Train loss: 150125       | Val loss 162584\n",
            "Epoch: 61/600 | Train loss: 135007       | Val loss 146500\n",
            "Epoch: 81/600 | Train loss: 120523       | Val loss 130687\n",
            "Epoch: 101/600 | Train loss: 109077       | Val loss 117819\n",
            "Epoch: 121/600 | Train loss: 97651       | Val loss 106088\n",
            "Epoch: 141/600 | Train loss: 87549       | Val loss 95993\n",
            "Epoch: 161/600 | Train loss: 78627       | Val loss 86014\n",
            "Epoch: 181/600 | Train loss: 70972       | Val loss 78295\n",
            "Epoch: 201/600 | Train loss: 63980       | Val loss 69420\n",
            "Epoch: 221/600 | Train loss: 57554       | Val loss 63131\n",
            "Epoch: 241/600 | Train loss: 52498       | Val loss 57228\n",
            "Epoch: 261/600 | Train loss: 47251       | Val loss 52222\n",
            "Epoch: 281/600 | Train loss: 42354       | Val loss 46223\n",
            "Epoch: 301/600 | Train loss: 38080       | Val loss 41801\n",
            "Epoch: 321/600 | Train loss: 34623       | Val loss 38107\n",
            "Epoch: 341/600 | Train loss: 31410       | Val loss 34557\n",
            "Epoch: 361/600 | Train loss: 28745       | Val loss 31866\n",
            "Epoch: 381/600 | Train loss: 26454       | Val loss 29176\n",
            "Epoch: 401/600 | Train loss: 24344       | Val loss 26375\n",
            "Epoch: 421/600 | Train loss: 22414       | Val loss 24514\n",
            "Epoch: 441/600 | Train loss: 20836       | Val loss 22763\n",
            "Epoch: 461/600 | Train loss: 19482       | Val loss 21225\n",
            "Epoch: 481/600 | Train loss: 18110       | Val loss 19857\n",
            "Epoch: 501/600 | Train loss: 17102       | Val loss 18737\n",
            "Epoch: 521/600 | Train loss: 15963       | Val loss 17641\n",
            "Epoch: 541/600 | Train loss: 15256       | Val loss 16355\n",
            "Epoch: 561/600 | Train loss: 14451       | Val loss 15758\n",
            "Epoch: 581/600 | Train loss: 13813       | Val loss 15103\n",
            "Trial 11:\t{'n_hidLayers': 3, 'hidden_size': 33, 'lr': 0.012144642564015337, 'activation': <function relu at 0x7f7bd37c17a0>}\n",
            "Epoch: 1/600 | Train loss: 137953       | Val loss 150611\n",
            "Epoch: 21/600 | Train loss: 14103       | Val loss 15102\n",
            "Epoch: 41/600 | Train loss: 13289       | Val loss 14287\n",
            "Epoch: 61/600 | Train loss: 13308       | Val loss 14141\n",
            "Epoch: 81/600 | Train loss: 7050       | Val loss 7520\n",
            "Epoch: 101/600 | Train loss: 6418       | Val loss 6923\n",
            "Epoch: 121/600 | Train loss: 3918       | Val loss 4211\n",
            "Epoch: 141/600 | Train loss: 3581       | Val loss 3766\n",
            "Epoch: 161/600 | Train loss: 3293       | Val loss 3548\n",
            "Epoch: 181/600 | Train loss: 2950       | Val loss 3129\n",
            "Epoch: 201/600 | Train loss: 2122       | Val loss 2327\n",
            "Epoch: 221/600 | Train loss: 1483       | Val loss 1600\n",
            "Epoch: 241/600 | Train loss: 1319       | Val loss 1436\n",
            "Epoch: 261/600 | Train loss: 1275       | Val loss 1364\n",
            "Epoch: 281/600 | Train loss: 1268       | Val loss 1350\n",
            "Epoch: 301/600 | Train loss: 1192       | Val loss 1308\n",
            "Epoch: 321/600 | Train loss: 1200       | Val loss 1307\n",
            "Epoch: 341/600 | Train loss: 1271       | Val loss 1331\n",
            "Epoch: 361/600 | Train loss: 1102       | Val loss 1210\n",
            "Epoch: 381/600 | Train loss: 1073       | Val loss 1201\n",
            "Epoch: 401/600 | Train loss: 1038       | Val loss 1158\n",
            "Epoch: 421/600 | Train loss: 1008       | Val loss 1133\n",
            "Epoch: 441/600 | Train loss: 962       | Val loss 1056\n",
            "Epoch: 461/600 | Train loss: 942       | Val loss 1045\n",
            "Epoch: 481/600 | Train loss: 878       | Val loss 998\n",
            "Epoch: 501/600 | Train loss: 912       | Val loss 1027\n",
            "Epoch: 521/600 | Train loss: 809       | Val loss 936\n",
            "Epoch: 541/600 | Train loss: 764       | Val loss 866\n",
            "Epoch: 561/600 | Train loss: 742       | Val loss 874\n",
            "Epoch: 581/600 | Train loss: 707       | Val loss 831\n",
            "Trial 12:\t{'n_hidLayers': 1, 'hidden_size': 69, 'lr': 0.09003290545145128, 'activation': <function tanh at 0x7f7bd37c4290>}\n",
            "Epoch: 1/600 | Train loss: 184759       | Val loss 200977\n",
            "Epoch: 21/600 | Train loss: 112997       | Val loss 123411\n",
            "Epoch: 41/600 | Train loss: 72606       | Val loss 79113\n",
            "Epoch: 61/600 | Train loss: 50309       | Val loss 55245\n",
            "Epoch: 81/600 | Train loss: 35245       | Val loss 38456\n",
            "Epoch: 101/600 | Train loss: 26084       | Val loss 28520\n",
            "Epoch: 121/600 | Train loss: 20262       | Val loss 22235\n",
            "Epoch: 141/600 | Train loss: 16599       | Val loss 18088\n",
            "Epoch: 161/600 | Train loss: 14174       | Val loss 15397\n",
            "Epoch: 181/600 | Train loss: 12403       | Val loss 13399\n",
            "Epoch: 201/600 | Train loss: 11053       | Val loss 12064\n",
            "Epoch: 221/600 | Train loss: 9965       | Val loss 11031\n",
            "Epoch: 241/600 | Train loss: 9066       | Val loss 10059\n",
            "Epoch: 261/600 | Train loss: 7932       | Val loss 8618\n",
            "Epoch: 281/600 | Train loss: 6988       | Val loss 7886\n",
            "Epoch: 301/600 | Train loss: 5996       | Val loss 6716\n",
            "Epoch: 321/600 | Train loss: 5373       | Val loss 6119\n",
            "Epoch: 341/600 | Train loss: 4763       | Val loss 5355\n",
            "Epoch: 361/600 | Train loss: 4417       | Val loss 4955\n",
            "Epoch: 381/600 | Train loss: 4008       | Val loss 4464\n",
            "Epoch: 401/600 | Train loss: 3687       | Val loss 4103\n",
            "Epoch: 421/600 | Train loss: 3417       | Val loss 3881\n",
            "Epoch: 441/600 | Train loss: 3162       | Val loss 3609\n",
            "Epoch: 461/600 | Train loss: 3026       | Val loss 3376\n",
            "Epoch: 481/600 | Train loss: 2791       | Val loss 3182\n",
            "Epoch: 501/600 | Train loss: 2612       | Val loss 3083\n",
            "Epoch: 521/600 | Train loss: 2455       | Val loss 2942\n",
            "Epoch: 541/600 | Train loss: 2375       | Val loss 2828\n",
            "Epoch: 561/600 | Train loss: 2237       | Val loss 2689\n",
            "Epoch: 581/600 | Train loss: 2164       | Val loss 2621\n",
            "Trial 13:\t{'n_hidLayers': 3, 'hidden_size': 98, 'lr': 0.014307440853717534, 'activation': <function tanh at 0x7f7bd37c4290>}\n",
            "Epoch: 1/600 | Train loss: 188347       | Val loss 203464\n",
            "Epoch: 21/600 | Train loss: 167411       | Val loss 183027\n",
            "Epoch: 41/600 | Train loss: 150679       | Val loss 162783\n",
            "Epoch: 61/600 | Train loss: 134563       | Val loss 146078\n",
            "Epoch: 81/600 | Train loss: 120178       | Val loss 132054\n",
            "Epoch: 101/600 | Train loss: 107565       | Val loss 117883\n",
            "Epoch: 121/600 | Train loss: 96006       | Val loss 105147\n",
            "Epoch: 141/600 | Train loss: 85839       | Val loss 93884\n",
            "Epoch: 161/600 | Train loss: 77197       | Val loss 84514\n",
            "Epoch: 181/600 | Train loss: 69048       | Val loss 75613\n",
            "Epoch: 201/600 | Train loss: 62075       | Val loss 67825\n",
            "Epoch: 221/600 | Train loss: 55868       | Val loss 60973\n",
            "Epoch: 241/600 | Train loss: 50851       | Val loss 55140\n",
            "Epoch: 261/600 | Train loss: 46649       | Val loss 51146\n",
            "Epoch: 281/600 | Train loss: 43167       | Val loss 47474\n",
            "Epoch: 301/600 | Train loss: 40511       | Val loss 44155\n",
            "Epoch: 321/600 | Train loss: 38102       | Val loss 42115\n",
            "Epoch: 341/600 | Train loss: 36750       | Val loss 40046\n",
            "Epoch: 361/600 | Train loss: 35125       | Val loss 38368\n",
            "Epoch: 381/600 | Train loss: 34463       | Val loss 38030\n",
            "Epoch: 401/600 | Train loss: 33710       | Val loss 36719\n",
            "Epoch: 421/600 | Train loss: 33383       | Val loss 36471\n",
            "Epoch: 441/600 | Train loss: 32922       | Val loss 35938\n",
            "Epoch: 461/600 | Train loss: 32891       | Val loss 36204\n",
            "Epoch: 481/600 | Train loss: 32833       | Val loss 36282\n",
            "Epoch: 501/600 | Train loss: 32882       | Val loss 35961\n",
            "Epoch: 521/600 | Train loss: 32873       | Val loss 35814\n",
            "Epoch: 541/600 | Train loss: 32827       | Val loss 35535\n",
            "Epoch: 561/600 | Train loss: 32824       | Val loss 35593\n",
            "Epoch: 581/600 | Train loss: 32880       | Val loss 35983\n",
            "Trial 14:\t{'n_hidLayers': 3, 'hidden_size': 187, 'lr': 0.004928311038438611, 'activation': <function tanh at 0x7f7bd37c4290>}\n",
            "Epoch: 1/600 | Train loss: 188675       | Val loss 205617\n",
            "Epoch: 21/600 | Train loss: 174335       | Val loss 190614\n",
            "Epoch: 41/600 | Train loss: 162630       | Val loss 177105\n",
            "Epoch: 61/600 | Train loss: 151408       | Val loss 164090\n",
            "Epoch: 81/600 | Train loss: 140507       | Val loss 152640\n",
            "Epoch: 101/600 | Train loss: 129746       | Val loss 141496\n",
            "Epoch: 121/600 | Train loss: 120562       | Val loss 131098\n",
            "Epoch: 141/600 | Train loss: 111986       | Val loss 122578\n",
            "Epoch: 161/600 | Train loss: 104375       | Val loss 112905\n",
            "Epoch: 181/600 | Train loss: 96546       | Val loss 104970\n",
            "Epoch: 201/600 | Train loss: 89911       | Val loss 97331\n",
            "Epoch: 221/600 | Train loss: 83349       | Val loss 90749\n",
            "Epoch: 241/600 | Train loss: 76910       | Val loss 84147\n",
            "Epoch: 261/600 | Train loss: 71664       | Val loss 78355\n",
            "Epoch: 281/600 | Train loss: 66129       | Val loss 72132\n",
            "Epoch: 301/600 | Train loss: 61790       | Val loss 67546\n",
            "Epoch: 321/600 | Train loss: 57375       | Val loss 63587\n",
            "Epoch: 341/600 | Train loss: 53862       | Val loss 59033\n",
            "Epoch: 361/600 | Train loss: 50270       | Val loss 54894\n",
            "Epoch: 381/600 | Train loss: 47311       | Val loss 52201\n",
            "Epoch: 401/600 | Train loss: 44648       | Val loss 49390\n",
            "Epoch: 421/600 | Train loss: 42614       | Val loss 46493\n",
            "Epoch: 441/600 | Train loss: 40790       | Val loss 44662\n",
            "Epoch: 461/600 | Train loss: 39019       | Val loss 42676\n",
            "Epoch: 481/600 | Train loss: 37606       | Val loss 41264\n",
            "Epoch: 501/600 | Train loss: 36452       | Val loss 39827\n",
            "Epoch: 521/600 | Train loss: 35686       | Val loss 39225\n",
            "Epoch: 541/600 | Train loss: 35022       | Val loss 38438\n",
            "Epoch: 561/600 | Train loss: 34126       | Val loss 37651\n",
            "Epoch: 581/600 | Train loss: 33955       | Val loss 37444\n",
            "Trial 15:\t{'n_hidLayers': 1, 'hidden_size': 248, 'lr': 0.022140609563512482, 'activation': <function tanh at 0x7f7bd37c4290>}\n",
            "Epoch: 1/600 | Train loss: 185166       | Val loss 200287\n",
            "Epoch: 21/600 | Train loss: 118967       | Val loss 129496\n",
            "Epoch: 41/600 | Train loss: 80163       | Val loss 87376\n",
            "Epoch: 61/600 | Train loss: 56257       | Val loss 61623\n",
            "Epoch: 81/600 | Train loss: 43313       | Val loss 46783\n",
            "Epoch: 101/600 | Train loss: 31571       | Val loss 34688\n",
            "Epoch: 121/600 | Train loss: 24199       | Val loss 26523\n",
            "Epoch: 141/600 | Train loss: 19285       | Val loss 20909\n",
            "Epoch: 161/600 | Train loss: 16134       | Val loss 17670\n",
            "Epoch: 181/600 | Train loss: 14008       | Val loss 15397\n",
            "Epoch: 201/600 | Train loss: 12392       | Val loss 13690\n",
            "Epoch: 221/600 | Train loss: 11153       | Val loss 12133\n",
            "Epoch: 241/600 | Train loss: 10112       | Val loss 11054\n",
            "Epoch: 261/600 | Train loss: 9273       | Val loss 10157\n",
            "Epoch: 281/600 | Train loss: 8449       | Val loss 9091\n",
            "Epoch: 301/600 | Train loss: 7273       | Val loss 8012\n",
            "Epoch: 321/600 | Train loss: 6469       | Val loss 7024\n",
            "Epoch: 341/600 | Train loss: 5683       | Val loss 6180\n",
            "Epoch: 361/600 | Train loss: 4997       | Val loss 5579\n",
            "Epoch: 381/600 | Train loss: 4428       | Val loss 4827\n",
            "Epoch: 401/600 | Train loss: 4003       | Val loss 4530\n",
            "Epoch: 421/600 | Train loss: 3611       | Val loss 4093\n",
            "Epoch: 441/600 | Train loss: 3255       | Val loss 3718\n",
            "Epoch: 461/600 | Train loss: 3001       | Val loss 3494\n",
            "Epoch: 481/600 | Train loss: 2758       | Val loss 3187\n",
            "Epoch: 501/600 | Train loss: 2514       | Val loss 2935\n",
            "Epoch: 521/600 | Train loss: 2357       | Val loss 2708\n",
            "Epoch: 541/600 | Train loss: 2150       | Val loss 2526\n",
            "Epoch: 561/600 | Train loss: 1986       | Val loss 2396\n",
            "Epoch: 581/600 | Train loss: 1833       | Val loss 2206\n",
            "Trial 16:\t{'n_hidLayers': 3, 'hidden_size': 328, 'lr': 0.0038327482373284394, 'activation': <function relu at 0x7f7bd37c17a0>}\n",
            "Epoch: 1/600 | Train loss: 47383       | Val loss 51567\n",
            "Epoch: 21/600 | Train loss: 7016       | Val loss 7516\n",
            "Epoch: 41/600 | Train loss: 2943       | Val loss 3129\n",
            "Epoch: 61/600 | Train loss: 2474       | Val loss 2694\n",
            "Epoch: 81/600 | Train loss: 1628       | Val loss 1757\n",
            "Epoch: 101/600 | Train loss: 1125       | Val loss 1259\n",
            "Epoch: 121/600 | Train loss: 758       | Val loss 895\n",
            "Epoch: 141/600 | Train loss: 569       | Val loss 697\n",
            "Epoch: 161/600 | Train loss: 500       | Val loss 624\n",
            "Epoch: 181/600 | Train loss: 455       | Val loss 594\n",
            "Epoch: 201/600 | Train loss: 398       | Val loss 497\n",
            "Epoch: 221/600 | Train loss: 411       | Val loss 530\n",
            "Epoch: 241/600 | Train loss: 345       | Val loss 459\n",
            "Epoch: 261/600 | Train loss: 363       | Val loss 476\n",
            "Epoch: 281/600 | Train loss: 321       | Val loss 437\n",
            "Epoch: 301/600 | Train loss: 298       | Val loss 418\n",
            "Epoch: 321/600 | Train loss: 287       | Val loss 412\n",
            "Epoch: 341/600 | Train loss: 273       | Val loss 399\n",
            "Epoch: 361/600 | Train loss: 307       | Val loss 435\n",
            "Epoch: 381/600 | Train loss: 309       | Val loss 440\n",
            "Epoch: 401/600 | Train loss: 246       | Val loss 367\n",
            "Epoch: 421/600 | Train loss: 307       | Val loss 425\n",
            "Epoch: 441/600 | Train loss: 272       | Val loss 383\n",
            "Epoch: 461/600 | Train loss: 276       | Val loss 399\n",
            "Epoch: 481/600 | Train loss: 225       | Val loss 344\n",
            "Epoch: 501/600 | Train loss: 237       | Val loss 357\n",
            "Epoch: 521/600 | Train loss: 265       | Val loss 395\n",
            "Epoch: 541/600 | Train loss: 229       | Val loss 348\n",
            "Epoch: 561/600 | Train loss: 231       | Val loss 360\n",
            "Epoch: 581/600 | Train loss: 216       | Val loss 344\n",
            "Trial 17:\t{'n_hidLayers': 1, 'hidden_size': 433, 'lr': 0.0027227958730754094, 'activation': <function relu at 0x7f7bd37c17a0>}\n",
            "Epoch: 1/600 | Train loss: 189618       | Val loss 205758\n",
            "Epoch: 21/600 | Train loss: 50023       | Val loss 54271\n",
            "Epoch: 41/600 | Train loss: 30072       | Val loss 32925\n",
            "Epoch: 61/600 | Train loss: 27218       | Val loss 29513\n",
            "Epoch: 81/600 | Train loss: 23973       | Val loss 25994\n",
            "Epoch: 101/600 | Train loss: 21025       | Val loss 22571\n",
            "Epoch: 121/600 | Train loss: 17682       | Val loss 19447\n",
            "Epoch: 141/600 | Train loss: 16002       | Val loss 17398\n",
            "Epoch: 161/600 | Train loss: 14324       | Val loss 15485\n",
            "Epoch: 181/600 | Train loss: 12803       | Val loss 13859\n",
            "Epoch: 201/600 | Train loss: 11344       | Val loss 12378\n",
            "Epoch: 221/600 | Train loss: 10153       | Val loss 10882\n",
            "Epoch: 241/600 | Train loss: 9377       | Val loss 9950\n",
            "Epoch: 261/600 | Train loss: 8625       | Val loss 9081\n",
            "Epoch: 281/600 | Train loss: 8136       | Val loss 8581\n",
            "Epoch: 301/600 | Train loss: 7755       | Val loss 8126\n",
            "Epoch: 321/600 | Train loss: 7397       | Val loss 7817\n",
            "Epoch: 341/600 | Train loss: 6927       | Val loss 7377\n",
            "Epoch: 361/600 | Train loss: 6442       | Val loss 6717\n",
            "Epoch: 381/600 | Train loss: 6018       | Val loss 6266\n",
            "Epoch: 401/600 | Train loss: 5642       | Val loss 5941\n",
            "Epoch: 421/600 | Train loss: 5309       | Val loss 5558\n",
            "Epoch: 441/600 | Train loss: 5023       | Val loss 5339\n",
            "Epoch: 461/600 | Train loss: 4784       | Val loss 5115\n",
            "Epoch: 481/600 | Train loss: 4634       | Val loss 4859\n",
            "Epoch: 501/600 | Train loss: 4510       | Val loss 4802\n",
            "Epoch: 521/600 | Train loss: 4341       | Val loss 4586\n",
            "Epoch: 541/600 | Train loss: 4256       | Val loss 4473\n",
            "Epoch: 561/600 | Train loss: 4126       | Val loss 4342\n",
            "Epoch: 581/600 | Train loss: 3973       | Val loss 4251\n",
            "Trial 18:\t{'n_hidLayers': 1, 'hidden_size': 260, 'lr': 0.06882296396257112, 'activation': <function relu at 0x7f7bd37c17a0>}\n",
            "Epoch: 1/600 | Train loss: 103632       | Val loss 112233\n",
            "Epoch: 21/600 | Train loss: 10026       | Val loss 10666\n",
            "Epoch: 41/600 | Train loss: 6160       | Val loss 6549\n",
            "Epoch: 61/600 | Train loss: 4286       | Val loss 4507\n",
            "Epoch: 81/600 | Train loss: 3466       | Val loss 3698\n",
            "Epoch: 101/600 | Train loss: 2986       | Val loss 3178\n",
            "Epoch: 121/600 | Train loss: 2655       | Val loss 2835\n",
            "Epoch: 141/600 | Train loss: 2358       | Val loss 2497\n",
            "Epoch: 161/600 | Train loss: 2157       | Val loss 2231\n",
            "Epoch: 181/600 | Train loss: 2014       | Val loss 2153\n",
            "Epoch: 201/600 | Train loss: 1914       | Val loss 2152\n",
            "Epoch: 221/600 | Train loss: 1873       | Val loss 2025\n",
            "Epoch: 241/600 | Train loss: 1804       | Val loss 2006\n",
            "Epoch: 261/600 | Train loss: 1698       | Val loss 1826\n",
            "Epoch: 281/600 | Train loss: 1551       | Val loss 1679\n",
            "Epoch: 301/600 | Train loss: 1375       | Val loss 1532\n",
            "Epoch: 321/600 | Train loss: 1208       | Val loss 1356\n",
            "Epoch: 341/600 | Train loss: 1070       | Val loss 1219\n",
            "Epoch: 361/600 | Train loss: 958       | Val loss 1109\n",
            "Epoch: 381/600 | Train loss: 862       | Val loss 1032\n",
            "Epoch: 401/600 | Train loss: 794       | Val loss 893\n",
            "Epoch: 421/600 | Train loss: 703       | Val loss 808\n",
            "Epoch: 441/600 | Train loss: 672       | Val loss 784\n",
            "Epoch: 461/600 | Train loss: 625       | Val loss 737\n",
            "Epoch: 481/600 | Train loss: 597       | Val loss 713\n",
            "Epoch: 501/600 | Train loss: 591       | Val loss 729\n",
            "Epoch: 521/600 | Train loss: 561       | Val loss 694\n",
            "Epoch: 541/600 | Train loss: 548       | Val loss 665\n",
            "Epoch: 561/600 | Train loss: 537       | Val loss 657\n",
            "Epoch: 581/600 | Train loss: 542       | Val loss 657\n",
            "Trial 19:\t{'n_hidLayers': 2, 'hidden_size': 365, 'lr': 0.005156018406754538, 'activation': <function relu at 0x7f7bd37c17a0>}\n",
            "Epoch: 1/600 | Train loss: 147618       | Val loss 159581\n",
            "Epoch: 21/600 | Train loss: 11065       | Val loss 11855\n",
            "Epoch: 41/600 | Train loss: 6854       | Val loss 7216\n",
            "Epoch: 61/600 | Train loss: 4465       | Val loss 4725\n",
            "Epoch: 81/600 | Train loss: 3882       | Val loss 4061\n",
            "Epoch: 101/600 | Train loss: 2958       | Val loss 3155\n",
            "Epoch: 121/600 | Train loss: 2297       | Val loss 2546\n",
            "Epoch: 141/600 | Train loss: 2002       | Val loss 2147\n",
            "Epoch: 161/600 | Train loss: 1745       | Val loss 1888\n",
            "Epoch: 181/600 | Train loss: 1500       | Val loss 1626\n",
            "Epoch: 201/600 | Train loss: 1284       | Val loss 1451\n",
            "Epoch: 221/600 | Train loss: 1156       | Val loss 1285\n",
            "Epoch: 241/600 | Train loss: 900       | Val loss 1087\n",
            "Epoch: 261/600 | Train loss: 807       | Val loss 973\n",
            "Epoch: 281/600 | Train loss: 644       | Val loss 788\n",
            "Epoch: 301/600 | Train loss: 658       | Val loss 786\n",
            "Epoch: 321/600 | Train loss: 500       | Val loss 622\n",
            "Epoch: 341/600 | Train loss: 463       | Val loss 615\n",
            "Epoch: 361/600 | Train loss: 443       | Val loss 590\n",
            "Epoch: 381/600 | Train loss: 417       | Val loss 537\n",
            "Epoch: 401/600 | Train loss: 399       | Val loss 517\n",
            "Epoch: 421/600 | Train loss: 377       | Val loss 511\n",
            "Epoch: 441/600 | Train loss: 398       | Val loss 535\n",
            "Epoch: 461/600 | Train loss: 406       | Val loss 537\n",
            "Epoch: 481/600 | Train loss: 395       | Val loss 527\n",
            "Epoch: 501/600 | Train loss: 348       | Val loss 485\n",
            "Epoch: 521/600 | Train loss: 324       | Val loss 444\n",
            "Epoch: 541/600 | Train loss: 335       | Val loss 478\n",
            "Epoch: 561/600 | Train loss: 310       | Val loss 438\n",
            "Epoch: 581/600 | Train loss: 338       | Val loss 464\n",
            "Trial 20:\t{'n_hidLayers': 1, 'hidden_size': 33, 'lr': 0.015019419063289335, 'activation': <function tanh at 0x7f7bd37c4290>}\n",
            "Epoch: 1/600 | Train loss: 188756       | Val loss 204710\n",
            "Epoch: 21/600 | Train loss: 181105       | Val loss 198156\n",
            "Epoch: 41/600 | Train loss: 173631       | Val loss 189288\n",
            "Epoch: 61/600 | Train loss: 166653       | Val loss 181256\n",
            "Epoch: 81/600 | Train loss: 160160       | Val loss 174184\n",
            "Epoch: 101/600 | Train loss: 153273       | Val loss 166201\n",
            "Epoch: 121/600 | Train loss: 147368       | Val loss 161479\n",
            "Epoch: 141/600 | Train loss: 140639       | Val loss 153490\n",
            "Epoch: 161/600 | Train loss: 135231       | Val loss 146575\n",
            "Epoch: 181/600 | Train loss: 130802       | Val loss 141031\n",
            "Epoch: 201/600 | Train loss: 124840       | Val loss 135703\n",
            "Epoch: 221/600 | Train loss: 119282       | Val loss 130920\n",
            "Epoch: 241/600 | Train loss: 114223       | Val loss 124622\n",
            "Epoch: 261/600 | Train loss: 110006       | Val loss 119573\n",
            "Epoch: 281/600 | Train loss: 105108       | Val loss 115182\n",
            "Epoch: 301/600 | Train loss: 101351       | Val loss 110370\n",
            "Epoch: 321/600 | Train loss: 96632       | Val loss 106431\n",
            "Epoch: 341/600 | Train loss: 92700       | Val loss 101327\n",
            "Epoch: 361/600 | Train loss: 88639       | Val loss 97470\n",
            "Epoch: 381/600 | Train loss: 85096       | Val loss 92616\n",
            "Epoch: 401/600 | Train loss: 81339       | Val loss 88824\n",
            "Epoch: 421/600 | Train loss: 77920       | Val loss 85614\n",
            "Epoch: 441/600 | Train loss: 74736       | Val loss 81420\n",
            "Epoch: 461/600 | Train loss: 72012       | Val loss 78780\n",
            "Epoch: 481/600 | Train loss: 68760       | Val loss 75359\n",
            "Epoch: 501/600 | Train loss: 65896       | Val loss 71876\n",
            "Epoch: 521/600 | Train loss: 62982       | Val loss 69255\n",
            "Epoch: 541/600 | Train loss: 60343       | Val loss 65443\n",
            "Epoch: 561/600 | Train loss: 57775       | Val loss 63549\n",
            "Epoch: 581/600 | Train loss: 55440       | Val loss 60313\n",
            "Trial 21:\t{'n_hidLayers': 1, 'hidden_size': 213, 'lr': 0.004051458985834618, 'activation': <function tanh at 0x7f7bd37c4290>}\n",
            "Epoch: 1/600 | Train loss: 189082       | Val loss 204789\n",
            "Epoch: 21/600 | Train loss: 174211       | Val loss 191116\n",
            "Epoch: 41/600 | Train loss: 162326       | Val loss 177256\n",
            "Epoch: 61/600 | Train loss: 152054       | Val loss 165423\n",
            "Epoch: 81/600 | Train loss: 141876       | Val loss 155426\n",
            "Epoch: 101/600 | Train loss: 132158       | Val loss 143430\n",
            "Epoch: 121/600 | Train loss: 123071       | Val loss 135375\n",
            "Epoch: 141/600 | Train loss: 115121       | Val loss 125515\n",
            "Epoch: 161/600 | Train loss: 107361       | Val loss 117257\n",
            "Epoch: 181/600 | Train loss: 100075       | Val loss 109130\n",
            "Epoch: 201/600 | Train loss: 93299       | Val loss 101988\n",
            "Epoch: 221/600 | Train loss: 86724       | Val loss 93969\n",
            "Epoch: 241/600 | Train loss: 80834       | Val loss 87540\n",
            "Epoch: 261/600 | Train loss: 75285       | Val loss 81553\n",
            "Epoch: 281/600 | Train loss: 70194       | Val loss 77574\n",
            "Epoch: 301/600 | Train loss: 65561       | Val loss 71410\n",
            "Epoch: 321/600 | Train loss: 61168       | Val loss 66378\n",
            "Epoch: 341/600 | Train loss: 57204       | Val loss 62616\n",
            "Epoch: 361/600 | Train loss: 53772       | Val loss 59013\n",
            "Epoch: 381/600 | Train loss: 50017       | Val loss 54168\n",
            "Epoch: 401/600 | Train loss: 46616       | Val loss 50633\n",
            "Epoch: 421/600 | Train loss: 43095       | Val loss 47522\n",
            "Epoch: 441/600 | Train loss: 40350       | Val loss 44369\n",
            "Epoch: 461/600 | Train loss: 37623       | Val loss 41315\n",
            "Epoch: 481/600 | Train loss: 35820       | Val loss 38707\n",
            "Epoch: 501/600 | Train loss: 33313       | Val loss 36393\n",
            "Epoch: 521/600 | Train loss: 31436       | Val loss 34250\n",
            "Epoch: 541/600 | Train loss: 29397       | Val loss 32487\n",
            "Epoch: 561/600 | Train loss: 28071       | Val loss 30653\n",
            "Epoch: 581/600 | Train loss: 26319       | Val loss 28984\n",
            "Trial 22:\t{'n_hidLayers': 3, 'hidden_size': 149, 'lr': 0.009741920052079604, 'activation': <function tanh at 0x7f7bd37c4290>}\n",
            "Epoch: 1/600 | Train loss: 188228       | Val loss 204768\n",
            "Epoch: 21/600 | Train loss: 166989       | Val loss 182001\n",
            "Epoch: 41/600 | Train loss: 149125       | Val loss 161775\n",
            "Epoch: 61/600 | Train loss: 132845       | Val loss 144827\n",
            "Epoch: 81/600 | Train loss: 118515       | Val loss 129251\n",
            "Epoch: 101/600 | Train loss: 105999       | Val loss 114756\n",
            "Epoch: 121/600 | Train loss: 94001       | Val loss 103487\n",
            "Epoch: 141/600 | Train loss: 84297       | Val loss 92423\n",
            "Epoch: 161/600 | Train loss: 75115       | Val loss 82076\n",
            "Epoch: 181/600 | Train loss: 67234       | Val loss 73195\n",
            "Epoch: 201/600 | Train loss: 60286       | Val loss 65691\n",
            "Epoch: 221/600 | Train loss: 54829       | Val loss 58593\n",
            "Epoch: 241/600 | Train loss: 49142       | Val loss 53894\n",
            "Epoch: 261/600 | Train loss: 45192       | Val loss 49617\n",
            "Epoch: 281/600 | Train loss: 41803       | Val loss 45464\n",
            "Epoch: 301/600 | Train loss: 39192       | Val loss 42737\n",
            "Epoch: 321/600 | Train loss: 37538       | Val loss 40978\n",
            "Epoch: 341/600 | Train loss: 35652       | Val loss 39185\n",
            "Epoch: 361/600 | Train loss: 34901       | Val loss 37952\n",
            "Epoch: 381/600 | Train loss: 34071       | Val loss 37323\n",
            "Epoch: 401/600 | Train loss: 33487       | Val loss 36719\n",
            "Epoch: 421/600 | Train loss: 33157       | Val loss 36099\n",
            "Epoch: 441/600 | Train loss: 32984       | Val loss 36099\n",
            "Epoch: 461/600 | Train loss: 32686       | Val loss 35700\n",
            "Epoch: 481/600 | Train loss: 32810       | Val loss 35781\n",
            "Epoch: 501/600 | Train loss: 32916       | Val loss 35916\n",
            "Epoch: 521/600 | Train loss: 33058       | Val loss 36202\n",
            "Epoch: 541/600 | Train loss: 32784       | Val loss 36124\n",
            "Epoch: 561/600 | Train loss: 32806       | Val loss 35865\n",
            "Epoch: 581/600 | Train loss: 32773       | Val loss 35657\n",
            "Trial 23:\t{'n_hidLayers': 2, 'hidden_size': 121, 'lr': 0.09278332803409887, 'activation': <function relu at 0x7f7bd37c17a0>}\n",
            "Epoch: 1/600 | Train loss: 23874       | Val loss 25976\n",
            "Epoch: 21/600 | Train loss: 3912       | Val loss 4137\n",
            "Epoch: 41/600 | Train loss: 2168       | Val loss 2338\n",
            "Epoch: 61/600 | Train loss: 1766       | Val loss 1929\n",
            "Epoch: 81/600 | Train loss: 1171       | Val loss 1383\n",
            "Epoch: 101/600 | Train loss: 747       | Val loss 914\n",
            "Epoch: 121/600 | Train loss: 634       | Val loss 788\n",
            "Epoch: 141/600 | Train loss: 617       | Val loss 790\n",
            "Epoch: 161/600 | Train loss: 493       | Val loss 626\n",
            "Epoch: 181/600 | Train loss: 523       | Val loss 654\n",
            "Epoch: 201/600 | Train loss: 480       | Val loss 615\n",
            "Epoch: 221/600 | Train loss: 399       | Val loss 548\n",
            "Epoch: 241/600 | Train loss: 439       | Val loss 570\n",
            "Epoch: 261/600 | Train loss: 403       | Val loss 516\n",
            "Epoch: 281/600 | Train loss: 331       | Val loss 453\n",
            "Epoch: 301/600 | Train loss: 344       | Val loss 466\n",
            "Epoch: 321/600 | Train loss: 397       | Val loss 512\n",
            "Epoch: 341/600 | Train loss: 473       | Val loss 653\n",
            "Epoch: 361/600 | Train loss: 317       | Val loss 446\n",
            "Epoch: 381/600 | Train loss: 361       | Val loss 524\n",
            "Epoch: 401/600 | Train loss: 301       | Val loss 464\n",
            "Epoch: 421/600 | Train loss: 298       | Val loss 463\n",
            "Epoch: 441/600 | Train loss: 322       | Val loss 484\n",
            "Epoch: 461/600 | Train loss: 278       | Val loss 453\n",
            "Epoch: 481/600 | Train loss: 268       | Val loss 458\n",
            "Epoch: 501/600 | Train loss: 233       | Val loss 409\n",
            "Epoch: 521/600 | Train loss: 244       | Val loss 425\n",
            "Epoch: 541/600 | Train loss: 323       | Val loss 497\n",
            "Epoch: 561/600 | Train loss: 228       | Val loss 428\n",
            "Epoch: 581/600 | Train loss: 220       | Val loss 418\n",
            "Trial 24:\t{'n_hidLayers': 1, 'hidden_size': 36, 'lr': 0.03930677772594339, 'activation': <function relu at 0x7f7bd37c17a0>}\n",
            "Epoch: 1/600 | Train loss: 183875       | Val loss 201359\n",
            "Epoch: 21/600 | Train loss: 23977       | Val loss 25807\n",
            "Epoch: 41/600 | Train loss: 15319       | Val loss 16823\n",
            "Epoch: 61/600 | Train loss: 12408       | Val loss 13363\n",
            "Epoch: 81/600 | Train loss: 10356       | Val loss 11451\n",
            "Epoch: 101/600 | Train loss: 8623       | Val loss 9121\n",
            "Epoch: 121/600 | Train loss: 7310       | Val loss 7707\n",
            "Epoch: 141/600 | Train loss: 6413       | Val loss 6600\n",
            "Epoch: 161/600 | Train loss: 5554       | Val loss 5807\n",
            "Epoch: 181/600 | Train loss: 4835       | Val loss 5149\n",
            "Epoch: 201/600 | Train loss: 4315       | Val loss 4542\n",
            "Epoch: 221/600 | Train loss: 3769       | Val loss 4076\n",
            "Epoch: 241/600 | Train loss: 3443       | Val loss 3763\n",
            "Epoch: 261/600 | Train loss: 3146       | Val loss 3428\n",
            "Epoch: 281/600 | Train loss: 2952       | Val loss 3145\n",
            "Epoch: 301/600 | Train loss: 2761       | Val loss 2991\n",
            "Epoch: 321/600 | Train loss: 2591       | Val loss 2826\n",
            "Epoch: 341/600 | Train loss: 2503       | Val loss 2725\n",
            "Epoch: 361/600 | Train loss: 2408       | Val loss 2567\n",
            "Epoch: 381/600 | Train loss: 2421       | Val loss 2545\n",
            "Epoch: 401/600 | Train loss: 2351       | Val loss 2529\n",
            "Epoch: 421/600 | Train loss: 2319       | Val loss 2465\n",
            "Epoch: 441/600 | Train loss: 2270       | Val loss 2462\n",
            "Epoch: 461/600 | Train loss: 2246       | Val loss 2451\n",
            "Epoch: 481/600 | Train loss: 2239       | Val loss 2453\n",
            "Epoch: 501/600 | Train loss: 2228       | Val loss 2384\n",
            "Epoch: 521/600 | Train loss: 2259       | Val loss 2408\n",
            "Epoch: 541/600 | Train loss: 2199       | Val loss 2366\n",
            "Epoch: 561/600 | Train loss: 2185       | Val loss 2334\n",
            "Epoch: 581/600 | Train loss: 2175       | Val loss 2343\n",
            "Trial 25:\t{'n_hidLayers': 3, 'hidden_size': 221, 'lr': 0.006607258739570834, 'activation': <function tanh at 0x7f7bd37c4290>}\n",
            "Epoch: 1/600 | Train loss: 188087       | Val loss 204763\n",
            "Epoch: 21/600 | Train loss: 166908       | Val loss 182169\n",
            "Epoch: 41/600 | Train loss: 148845       | Val loss 162424\n",
            "Epoch: 61/600 | Train loss: 132510       | Val loss 144686\n",
            "Epoch: 81/600 | Train loss: 118074       | Val loss 128816\n",
            "Epoch: 101/600 | Train loss: 105397       | Val loss 115072\n",
            "Epoch: 121/600 | Train loss: 94090       | Val loss 102978\n",
            "Epoch: 141/600 | Train loss: 83581       | Val loss 92406\n",
            "Epoch: 161/600 | Train loss: 75038       | Val loss 82031\n",
            "Epoch: 181/600 | Train loss: 66549       | Val loss 72987\n",
            "Epoch: 201/600 | Train loss: 59740       | Val loss 65093\n",
            "Epoch: 221/600 | Train loss: 53818       | Val loss 59146\n",
            "Epoch: 241/600 | Train loss: 48937       | Val loss 54584\n",
            "Epoch: 261/600 | Train loss: 45050       | Val loss 49829\n",
            "Epoch: 281/600 | Train loss: 41663       | Val loss 46041\n",
            "Epoch: 301/600 | Train loss: 39323       | Val loss 42538\n",
            "Epoch: 321/600 | Train loss: 37181       | Val loss 41130\n",
            "Epoch: 341/600 | Train loss: 35588       | Val loss 38827\n",
            "Epoch: 361/600 | Train loss: 34726       | Val loss 38016\n",
            "Epoch: 381/600 | Train loss: 33832       | Val loss 37359\n",
            "Epoch: 401/600 | Train loss: 33399       | Val loss 36514\n",
            "Epoch: 421/600 | Train loss: 33088       | Val loss 36085\n",
            "Epoch: 441/600 | Train loss: 32922       | Val loss 35747\n",
            "Epoch: 461/600 | Train loss: 32794       | Val loss 35636\n",
            "Epoch: 481/600 | Train loss: 33053       | Val loss 35875\n",
            "Epoch: 501/600 | Train loss: 32858       | Val loss 35870\n",
            "Epoch: 521/600 | Train loss: 32722       | Val loss 36038\n",
            "Epoch: 541/600 | Train loss: 32848       | Val loss 35905\n",
            "Epoch: 561/600 | Train loss: 32728       | Val loss 35803\n",
            "Epoch: 581/600 | Train loss: 32737       | Val loss 36222\n",
            "Trial 26:\t{'n_hidLayers': 3, 'hidden_size': 259, 'lr': 0.0031795615349914427, 'activation': <function relu at 0x7f7bd37c17a0>}\n",
            "Epoch: 1/600 | Train loss: 108946       | Val loss 118718\n",
            "Epoch: 21/600 | Train loss: 8014       | Val loss 8435\n",
            "Epoch: 41/600 | Train loss: 3719       | Val loss 3972\n",
            "Epoch: 61/600 | Train loss: 2799       | Val loss 3094\n",
            "Epoch: 81/600 | Train loss: 2456       | Val loss 2654\n",
            "Epoch: 101/600 | Train loss: 1507       | Val loss 1689\n",
            "Epoch: 121/600 | Train loss: 1051       | Val loss 1170\n",
            "Epoch: 141/600 | Train loss: 765       | Val loss 879\n",
            "Epoch: 161/600 | Train loss: 664       | Val loss 770\n",
            "Epoch: 181/600 | Train loss: 583       | Val loss 717\n",
            "Epoch: 201/600 | Train loss: 565       | Val loss 691\n",
            "Epoch: 221/600 | Train loss: 542       | Val loss 654\n",
            "Epoch: 241/600 | Train loss: 481       | Val loss 609\n",
            "Epoch: 261/600 | Train loss: 449       | Val loss 580\n",
            "Epoch: 281/600 | Train loss: 456       | Val loss 593\n",
            "Epoch: 301/600 | Train loss: 572       | Val loss 718\n",
            "Epoch: 321/600 | Train loss: 397       | Val loss 515\n",
            "Epoch: 341/600 | Train loss: 394       | Val loss 512\n",
            "Epoch: 361/600 | Train loss: 359       | Val loss 488\n",
            "Epoch: 381/600 | Train loss: 341       | Val loss 461\n",
            "Epoch: 401/600 | Train loss: 363       | Val loss 473\n",
            "Epoch: 421/600 | Train loss: 327       | Val loss 443\n",
            "Epoch: 441/600 | Train loss: 319       | Val loss 440\n",
            "Epoch: 461/600 | Train loss: 321       | Val loss 428\n",
            "Epoch: 481/600 | Train loss: 306       | Val loss 433\n",
            "Epoch: 501/600 | Train loss: 303       | Val loss 427\n",
            "Epoch: 521/600 | Train loss: 333       | Val loss 454\n",
            "Epoch: 541/600 | Train loss: 288       | Val loss 405\n",
            "Epoch: 561/600 | Train loss: 289       | Val loss 416\n",
            "Epoch: 581/600 | Train loss: 293       | Val loss 412\n",
            "Trial 27:\t{'n_hidLayers': 3, 'hidden_size': 305, 'lr': 0.01037524829747413, 'activation': <function relu at 0x7f7bd37c17a0>}\n",
            "Epoch: 1/600 | Train loss: 31553       | Val loss 34197\n",
            "Epoch: 21/600 | Train loss: 3062       | Val loss 3302\n",
            "Epoch: 41/600 | Train loss: 1202       | Val loss 1311\n",
            "Epoch: 61/600 | Train loss: 771       | Val loss 876\n",
            "Epoch: 81/600 | Train loss: 566       | Val loss 695\n",
            "Epoch: 101/600 | Train loss: 473       | Val loss 606\n",
            "Epoch: 121/600 | Train loss: 430       | Val loss 534\n",
            "Epoch: 141/600 | Train loss: 374       | Val loss 484\n",
            "Epoch: 161/600 | Train loss: 368       | Val loss 500\n",
            "Epoch: 181/600 | Train loss: 380       | Val loss 509\n",
            "Epoch: 201/600 | Train loss: 383       | Val loss 501\n",
            "Epoch: 221/600 | Train loss: 339       | Val loss 456\n",
            "Epoch: 241/600 | Train loss: 260       | Val loss 394\n",
            "Epoch: 261/600 | Train loss: 304       | Val loss 464\n",
            "Epoch: 281/600 | Train loss: 430       | Val loss 616\n",
            "Epoch: 301/600 | Train loss: 268       | Val loss 405\n",
            "Epoch: 321/600 | Train loss: 252       | Val loss 406\n",
            "Epoch: 341/600 | Train loss: 219       | Val loss 360\n",
            "Epoch: 361/600 | Train loss: 221       | Val loss 372\n",
            "Epoch: 381/600 | Train loss: 222       | Val loss 386\n",
            "Epoch: 401/600 | Train loss: 211       | Val loss 370\n",
            "Epoch: 421/600 | Train loss: 249       | Val loss 431\n",
            "Epoch: 441/600 | Train loss: 186       | Val loss 367\n",
            "Epoch: 461/600 | Train loss: 190       | Val loss 381\n",
            "Epoch: 481/600 | Train loss: 181       | Val loss 367\n",
            "Epoch: 501/600 | Train loss: 200       | Val loss 382\n",
            "Epoch: 521/600 | Train loss: 191       | Val loss 385\n",
            "Epoch: 541/600 | Train loss: 178       | Val loss 371\n",
            "Epoch: 561/600 | Train loss: 154       | Val loss 354\n",
            "Epoch: 581/600 | Train loss: 211       | Val loss 433\n",
            "Trial 28:\t{'n_hidLayers': 1, 'hidden_size': 241, 'lr': 0.01121102478179167, 'activation': <function tanh at 0x7f7bd37c4290>}\n",
            "Epoch: 1/600 | Train loss: 187866       | Val loss 203242\n",
            "Epoch: 21/600 | Train loss: 149292       | Val loss 161704\n",
            "Epoch: 41/600 | Train loss: 120995       | Val loss 131688\n",
            "Epoch: 61/600 | Train loss: 98574       | Val loss 107961\n",
            "Epoch: 81/600 | Train loss: 80268       | Val loss 87465\n",
            "Epoch: 101/600 | Train loss: 66860       | Val loss 72047\n",
            "Epoch: 121/600 | Train loss: 55130       | Val loss 61111\n",
            "Epoch: 141/600 | Train loss: 47369       | Val loss 51075\n",
            "Epoch: 161/600 | Train loss: 39696       | Val loss 43744\n",
            "Epoch: 181/600 | Train loss: 32588       | Val loss 35673\n",
            "Epoch: 201/600 | Train loss: 27745       | Val loss 30355\n",
            "Epoch: 221/600 | Train loss: 23965       | Val loss 26224\n",
            "Epoch: 241/600 | Train loss: 20941       | Val loss 22784\n",
            "Epoch: 261/600 | Train loss: 18653       | Val loss 20015\n",
            "Epoch: 281/600 | Train loss: 16482       | Val loss 18173\n",
            "Epoch: 301/600 | Train loss: 14972       | Val loss 16397\n",
            "Epoch: 321/600 | Train loss: 13788       | Val loss 14828\n",
            "Epoch: 341/600 | Train loss: 12706       | Val loss 13910\n",
            "Epoch: 361/600 | Train loss: 11889       | Val loss 12737\n",
            "Epoch: 381/600 | Train loss: 11164       | Val loss 12063\n",
            "Epoch: 401/600 | Train loss: 10453       | Val loss 11309\n",
            "Epoch: 421/600 | Train loss: 9819       | Val loss 10693\n",
            "Epoch: 441/600 | Train loss: 9335       | Val loss 10139\n",
            "Epoch: 461/600 | Train loss: 8789       | Val loss 9619\n",
            "Epoch: 481/600 | Train loss: 8176       | Val loss 8882\n",
            "Epoch: 501/600 | Train loss: 7576       | Val loss 8457\n",
            "Epoch: 521/600 | Train loss: 7039       | Val loss 7696\n",
            "Epoch: 541/600 | Train loss: 6572       | Val loss 7236\n",
            "Epoch: 561/600 | Train loss: 6046       | Val loss 6897\n",
            "Epoch: 581/600 | Train loss: 5644       | Val loss 6379\n",
            "Trial 29:\t{'n_hidLayers': 3, 'hidden_size': 170, 'lr': 0.002505228856360884, 'activation': <function relu at 0x7f7bd37c17a0>}\n",
            "Epoch: 1/600 | Train loss: 177679       | Val loss 193716\n",
            "Epoch: 21/600 | Train loss: 13863       | Val loss 14725\n",
            "Epoch: 41/600 | Train loss: 8764       | Val loss 9439\n",
            "Epoch: 61/600 | Train loss: 6272       | Val loss 6802\n",
            "Epoch: 81/600 | Train loss: 3768       | Val loss 4032\n",
            "Epoch: 101/600 | Train loss: 2990       | Val loss 3191\n",
            "Epoch: 121/600 | Train loss: 2465       | Val loss 2623\n",
            "Epoch: 141/600 | Train loss: 1952       | Val loss 2097\n",
            "Epoch: 161/600 | Train loss: 1671       | Val loss 1828\n",
            "Epoch: 181/600 | Train loss: 1324       | Val loss 1463\n",
            "Epoch: 201/600 | Train loss: 1050       | Val loss 1214\n",
            "Epoch: 221/600 | Train loss: 941       | Val loss 1050\n",
            "Epoch: 241/600 | Train loss: 836       | Val loss 975\n",
            "Epoch: 261/600 | Train loss: 722       | Val loss 863\n",
            "Epoch: 281/600 | Train loss: 647       | Val loss 802\n",
            "Epoch: 301/600 | Train loss: 600       | Val loss 729\n",
            "Epoch: 321/600 | Train loss: 565       | Val loss 708\n",
            "Epoch: 341/600 | Train loss: 510       | Val loss 648\n",
            "Epoch: 361/600 | Train loss: 522       | Val loss 647\n",
            "Epoch: 381/600 | Train loss: 467       | Val loss 600\n",
            "Epoch: 401/600 | Train loss: 438       | Val loss 577\n",
            "Epoch: 421/600 | Train loss: 469       | Val loss 589\n",
            "Epoch: 441/600 | Train loss: 416       | Val loss 542\n",
            "Epoch: 461/600 | Train loss: 398       | Val loss 518\n",
            "Epoch: 481/600 | Train loss: 396       | Val loss 512\n",
            "Epoch: 501/600 | Train loss: 386       | Val loss 498\n",
            "Epoch: 521/600 | Train loss: 401       | Val loss 509\n",
            "Epoch: 541/600 | Train loss: 444       | Val loss 566\n",
            "Epoch: 561/600 | Train loss: 362       | Val loss 482\n",
            "Epoch: 581/600 | Train loss: 362       | Val loss 465\n",
            "Trial 30:\t{'n_hidLayers': 1, 'hidden_size': 328, 'lr': 0.006314937234888006, 'activation': <function tanh at 0x7f7bd37c4290>}\n",
            "Epoch: 1/600 | Train loss: 188477       | Val loss 204233\n",
            "Epoch: 21/600 | Train loss: 157316       | Val loss 171560\n",
            "Epoch: 41/600 | Train loss: 133582       | Val loss 145960\n",
            "Epoch: 61/600 | Train loss: 113981       | Val loss 123617\n",
            "Epoch: 81/600 | Train loss: 97347       | Val loss 105612\n",
            "Epoch: 101/600 | Train loss: 83014       | Val loss 90784\n",
            "Epoch: 121/600 | Train loss: 71751       | Val loss 76993\n",
            "Epoch: 141/600 | Train loss: 60909       | Val loss 67009\n",
            "Epoch: 161/600 | Train loss: 53004       | Val loss 58000\n",
            "Epoch: 181/600 | Train loss: 46878       | Val loss 50691\n",
            "Epoch: 201/600 | Train loss: 41263       | Val loss 45211\n",
            "Epoch: 221/600 | Train loss: 34556       | Val loss 37906\n",
            "Epoch: 241/600 | Train loss: 30171       | Val loss 32938\n",
            "Epoch: 261/600 | Train loss: 26626       | Val loss 29558\n",
            "Epoch: 281/600 | Train loss: 23482       | Val loss 26002\n",
            "Epoch: 301/600 | Train loss: 21142       | Val loss 23113\n",
            "Epoch: 321/600 | Train loss: 19060       | Val loss 20872\n",
            "Epoch: 341/600 | Train loss: 17317       | Val loss 18958\n",
            "Epoch: 361/600 | Train loss: 15953       | Val loss 17298\n",
            "Epoch: 381/600 | Train loss: 14807       | Val loss 16228\n",
            "Epoch: 401/600 | Train loss: 13966       | Val loss 15357\n",
            "Epoch: 421/600 | Train loss: 12978       | Val loss 13891\n",
            "Epoch: 441/600 | Train loss: 12199       | Val loss 13473\n",
            "Epoch: 461/600 | Train loss: 11617       | Val loss 12418\n",
            "Epoch: 481/600 | Train loss: 11000       | Val loss 11818\n",
            "Epoch: 501/600 | Train loss: 10458       | Val loss 11230\n",
            "Epoch: 521/600 | Train loss: 9958       | Val loss 10763\n",
            "Epoch: 541/600 | Train loss: 9579       | Val loss 10388\n",
            "Epoch: 561/600 | Train loss: 9274       | Val loss 10009\n",
            "Epoch: 581/600 | Train loss: 8745       | Val loss 9560\n",
            "Trial 31:\t{'n_hidLayers': 1, 'hidden_size': 81, 'lr': 0.018837353872380096, 'activation': <function tanh at 0x7f7bd37c4290>}\n",
            "Epoch: 1/600 | Train loss: 188013       | Val loss 203706\n",
            "Epoch: 21/600 | Train loss: 165097       | Val loss 181141\n",
            "Epoch: 41/600 | Train loss: 146349       | Val loss 158635\n",
            "Epoch: 61/600 | Train loss: 129417       | Val loss 140798\n",
            "Epoch: 81/600 | Train loss: 114825       | Val loss 125792\n",
            "Epoch: 101/600 | Train loss: 102008       | Val loss 111271\n",
            "Epoch: 121/600 | Train loss: 90329       | Val loss 98584\n",
            "Epoch: 141/600 | Train loss: 80190       | Val loss 87488\n",
            "Epoch: 161/600 | Train loss: 70965       | Val loss 77946\n",
            "Epoch: 181/600 | Train loss: 63520       | Val loss 69223\n",
            "Epoch: 201/600 | Train loss: 56916       | Val loss 63035\n",
            "Epoch: 221/600 | Train loss: 50581       | Val loss 54626\n",
            "Epoch: 241/600 | Train loss: 44844       | Val loss 48444\n",
            "Epoch: 261/600 | Train loss: 39657       | Val loss 43574\n",
            "Epoch: 281/600 | Train loss: 35504       | Val loss 39207\n",
            "Epoch: 301/600 | Train loss: 32470       | Val loss 35670\n",
            "Epoch: 321/600 | Train loss: 28953       | Val loss 31747\n",
            "Epoch: 341/600 | Train loss: 26517       | Val loss 29459\n",
            "Epoch: 361/600 | Train loss: 23943       | Val loss 26515\n",
            "Epoch: 381/600 | Train loss: 21953       | Val loss 24456\n",
            "Epoch: 401/600 | Train loss: 20338       | Val loss 22210\n",
            "Epoch: 421/600 | Train loss: 18812       | Val loss 20592\n",
            "Epoch: 441/600 | Train loss: 17519       | Val loss 19055\n",
            "Epoch: 461/600 | Train loss: 16288       | Val loss 18345\n",
            "Epoch: 481/600 | Train loss: 15318       | Val loss 17039\n",
            "Epoch: 501/600 | Train loss: 14446       | Val loss 15815\n",
            "Epoch: 521/600 | Train loss: 13709       | Val loss 14733\n",
            "Epoch: 541/600 | Train loss: 12991       | Val loss 14035\n",
            "Epoch: 561/600 | Train loss: 12318       | Val loss 13420\n",
            "Epoch: 581/600 | Train loss: 11789       | Val loss 12872\n",
            "Trial 32:\t{'n_hidLayers': 2, 'hidden_size': 394, 'lr': 0.04275882839002674, 'activation': <function relu at 0x7f7bd37c17a0>}\n",
            "Epoch: 1/600 | Train loss: 23680       | Val loss 25894\n",
            "Epoch: 21/600 | Train loss: 3313       | Val loss 3588\n",
            "Epoch: 41/600 | Train loss: 2421       | Val loss 2607\n",
            "Epoch: 61/600 | Train loss: 1505       | Val loss 1675\n",
            "Epoch: 81/600 | Train loss: 851       | Val loss 959\n",
            "Epoch: 101/600 | Train loss: 576       | Val loss 718\n",
            "Epoch: 121/600 | Train loss: 481       | Val loss 614\n",
            "Epoch: 141/600 | Train loss: 419       | Val loss 560\n",
            "Epoch: 161/600 | Train loss: 405       | Val loss 566\n",
            "Epoch: 181/600 | Train loss: 575       | Val loss 707\n",
            "Epoch: 201/600 | Train loss: 321       | Val loss 454\n",
            "Epoch: 221/600 | Train loss: 298       | Val loss 455\n",
            "Epoch: 241/600 | Train loss: 356       | Val loss 481\n",
            "Epoch: 261/600 | Train loss: 286       | Val loss 435\n",
            "Epoch: 281/600 | Train loss: 337       | Val loss 464\n",
            "Epoch: 301/600 | Train loss: 314       | Val loss 451\n",
            "Epoch: 321/600 | Train loss: 251       | Val loss 391\n",
            "Epoch: 341/600 | Train loss: 275       | Val loss 411\n",
            "Epoch: 361/600 | Train loss: 276       | Val loss 452\n",
            "Epoch: 381/600 | Train loss: 220       | Val loss 374\n",
            "Epoch: 401/600 | Train loss: 227       | Val loss 403\n",
            "Epoch: 421/600 | Train loss: 282       | Val loss 459\n",
            "Epoch: 441/600 | Train loss: 250       | Val loss 430\n",
            "Epoch: 461/600 | Train loss: 186       | Val loss 378\n",
            "Epoch: 481/600 | Train loss: 237       | Val loss 447\n",
            "Epoch: 501/600 | Train loss: 181       | Val loss 379\n",
            "Epoch: 521/600 | Train loss: 206       | Val loss 435\n",
            "Epoch: 541/600 | Train loss: 169       | Val loss 384\n",
            "Epoch: 561/600 | Train loss: 178       | Val loss 406\n",
            "Epoch: 581/600 | Train loss: 198       | Val loss 420\n"
          ]
        }
      ],
      "source": [
        "for trial in study:\n",
        "  print(\"Trial {}:\\t{}\".format(trial.id, trial.parameters))\n",
        "  # model = Net(trial.parameters['hidden_size'])\n",
        "  model=MLP(input_dim=np.shape(X_train)[1] , \n",
        "            output_dim=np.shape(y_train)[1] ,\n",
        "            n_hidLayers=trial.parameters['n_hidLayers'],\n",
        "            hidden_size=trial.parameters['hidden_size'],\n",
        "            act_fn = trial.parameters['activation']\n",
        "            )    \n",
        "  #model.train()\n",
        "  model.to(device)\n",
        "  optimizer = optim.Adam(model.parameters(), lr = trial.parameters['lr']) # lr = trial.parameters['lr']\n",
        "  \n",
        "  for epoch in range(num_epochs):\n",
        "\n",
        "    epoch_loss = 0\n",
        "    model.train()\n",
        "    \n",
        "    for i, (inputs, loads) in enumerate(train_loader):\n",
        "        #print(features, loads)\n",
        "        \n",
        "        optimizer.zero_grad()                      # zeroize accumulated gradients in parameters             \n",
        "        \n",
        "        output = model(inputs.float().to(device))             # forwards pass       \n",
        "        batch_loss = loss(output, loads.float().to(device))   # compute loss for current batch\n",
        "        \n",
        "        batch_loss.backward()                      # compute the gradient of the loss wrt. model parameters\n",
        "        optimizer.step()                           # update weights according to the comptued gradients\n",
        "        \n",
        "    \n",
        "    epoch_loss_train = 0\n",
        "    epoch_loss_test = 0\n",
        "    model.eval()\n",
        "    \n",
        "    ##### Evaluate training\n",
        "    for i, (inputs, loads) in enumerate(train_loader):\n",
        "        \n",
        "        output = model(inputs.float().to(device))\n",
        "        \n",
        "        batch_loss_train = loss(output, loads.float().to(device))  # compute loss for the current batch\n",
        "        epoch_loss_train += batch_loss_train            # accumulate loss for the current epoch\n",
        "        \n",
        "        #print(f'Epoch: {epoch+1}/{num_epochs}  | Step {i+1}/{n_iterations}')\n",
        "    \n",
        "    ##### Evaluate validation    \n",
        "    for i, (inputs, loads) in enumerate(valid_loader):\n",
        "        \n",
        "        output = model(inputs.float().to(device))\n",
        "        \n",
        "        batch_loss_test = loss(output, loads.float().to(device))  # compute loss for the current batch\n",
        "        epoch_loss_test += batch_loss_test     # accumulate loss for the current epoch\n",
        "        \n",
        "        #print(f'Epoch: {epoch+1}/{num_epochs}  | Step {i+1}/{n_iterations}')\n",
        "    \n",
        "    if epoch % 20 == 0: \n",
        "        print(f'Epoch: {epoch+1}/{num_epochs} | Train loss: {0.001*epoch_loss_train/num_batches_train:.0f}       | Val loss {0.001*epoch_loss_test/num_batches_test:.0f}')\n",
        "\n",
        "        study.add_observation(trial=trial,\n",
        "                              iteration=epoch,\n",
        "                              objective=0.001*epoch_loss_test.cpu().detach().numpy()/num_batches_test)\n",
        "        \n",
        "    if study.should_trial_stop(trial):\n",
        "        break \n",
        "    # store in list for plotting the loss per epoch    \n",
        "    val_losses.append(0.001*epoch_loss_test.cpu().detach().numpy()/num_batches_test)  \n",
        "    train_losses.append(0.001*epoch_loss_train.cpu().detach().numpy()/num_batches_train)  \n",
        "\n",
        "  study.finalize(trial) \n",
        "\n",
        "  #study.get_best_result()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "rpdUjrJQ0lcJ"
      },
      "outputs": [],
      "source": [
        "df = study.results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GfYXHRs66M5x"
      },
      "outputs": [],
      "source": [
        "Error On Purpose"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "MyFIkShRyPpH",
        "outputId": "ff13dbff-354c-4036-b846-12a5fc4e67d3"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_a7eeba0d-31e4-4b32-94b8-4d6d1c469d22\", \"Random4params.csv\", 100972)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from google.colab import files\n",
        "df.to_csv('Random4params.csv') \n",
        "files.download('Random4params.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mzhkLlRv29u0",
        "outputId": "4f182b52-859a-4610-9a2c-517b629ec1f5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'Iteration': 440,\n",
              " 'Objective': 566.0530526315789,\n",
              " 'Trial-ID': 15,\n",
              " 'hidden_size': 53,\n",
              " 'lr': 0.04078152864919068,\n",
              " 'n_hidLayers': 5}"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "study.get_best_result()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CSx8TyoulAS0"
      },
      "outputs": [],
      "source": [
        "study.get_best_result()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3TJI1oV0jZnd"
      },
      "outputs": [],
      "source": [
        "study.results.Objective.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z_uF07KUG5uX"
      },
      "outputs": [],
      "source": [
        "study.get_best_result()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yrrA661lhyJW"
      },
      "outputs": [],
      "source": [
        "study.results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "diZ_MjPfzRna"
      },
      "source": [
        "# 3.1- Start training loop "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f6-pIKVRzRna"
      },
      "outputs": [],
      "source": [
        "for epoch in range(num_epochs):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    model.train()\n",
        "    \n",
        "    for i, (inputs, loads) in enumerate(train_loader):\n",
        "        #print(features, loads)\n",
        "        \n",
        "        optimizer.zero_grad()                      # zeroize accumulated gradients in parameters             \n",
        "        \n",
        "        output = model(inputs.float())             # forwards pass       \n",
        "        batch_loss = loss(output, loads.float())   # compute loss for current batch\n",
        "        \n",
        "        batch_loss.backward()                      # compute the gradient of the loss wrt. model parameters\n",
        "        optimizer.step()                           # update weights according to the comptued gradients\n",
        "        \n",
        "    \n",
        "    epoch_loss_train = 0\n",
        "    epoch_loss_test = 0\n",
        "    model.eval()\n",
        "    \n",
        "    ##### Evaluate training\n",
        "    for i, (inputs, loads) in enumerate(train_loader):\n",
        "        \n",
        "        output = model(inputs.float())\n",
        "        \n",
        "        batch_loss_train = loss(output, loads.float())  # compute loss for the current batch\n",
        "        epoch_loss_train += batch_loss_train            # accumulate loss for the current epoch\n",
        "        \n",
        "        #print(f'Epoch: {epoch+1}/{num_epochs}  | Step {i+1}/{n_iterations}')\n",
        "    \n",
        "    ##### Evaluate validation    \n",
        "    for i, (inputs, loads) in enumerate(valid_loader):\n",
        "        \n",
        "        output = model(inputs.float())\n",
        "        \n",
        "        batch_loss_test = loss(output, loads.float())  # compute loss for the current batch\n",
        "        epoch_loss_test += batch_loss_test     # accumulate loss for the current epoch\n",
        "        \n",
        "        #print(f'Epoch: {epoch+1}/{num_epochs}  | Step {i+1}/{n_iterations}')\n",
        "    \n",
        "    if epoch % 100 == 0: \n",
        "        print(f'Epoch: {epoch+1}/{num_epochs} | Train loss: {epoch_loss_train/num_batches_train}       | Val loss {epoch_loss_test/num_batches_test}')\n",
        "        \n",
        "    # store in list for plotting the loss per epoch    \n",
        "    val_losses.append(epoch_loss_test/num_batches_test)  \n",
        "    train_losses.append(epoch_loss_train/num_batches_train)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vgCFr4fIzRna"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize = (8,6))\n",
        "plt.plot(train_losses[50:])\n",
        "plt.plot(val_losses[50:], '-k')\n",
        "plt.title('Training-Validation loss', fontsize = 16)\n",
        "plt.grid()\n",
        "plt.legend(['Training loss','Validation loss'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSkpKZ2HzRna"
      },
      "source": [
        "# Evalute on the test data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6lcj4lLzRnb"
      },
      "source": [
        "### Scale the test data before evaluating the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e-TPXw0rzRnb"
      },
      "outputs": [],
      "source": [
        "Test_scaler_x = preprocessing.MinMaxScaler(feature_range=feature_range).fit(X_test) # maybe try another for X?\n",
        "X_Test_scaled = Test_scaler_x.transform(X_test)\n",
        "\n",
        "predictions = model(torch.tensor(X_Test_scaled).float()).detach().numpy()\n",
        "test_targets = y_test.values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ySZ9om3tzRnb"
      },
      "outputs": [],
      "source": [
        "for idx, ch in enumerate(df.columns.tolist()[8:]):\n",
        "  # issues with 2 and 7\n",
        "    plt.figure(idx, figsize = (8,6))\n",
        "    sns.scatterplot(predictions[:,idx],test_targets[:,idx])\n",
        "    plt.title(f'DELs: {ch}', fontsize = 20)\n",
        "    plt.xlabel('Predictions [kNm]', fontsize = 12)\n",
        "    plt.ylabel('Targets [kNm]')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Py30VR4zRnb"
      },
      "outputs": [],
      "source": [
        "mse_list = []\n",
        "r2_score_list = []\n",
        "for i in range(len(df.columns.tolist()[8:])):\n",
        "    #print(f'MSE {AllTargetData.columns[i]} Channel : \\n {mean_squared_error(AllTargetData.values[:,i], Yout[:,i])}')\n",
        "    mse_list.append(mean_squared_error(test_targets[:,i], predictions[:,i]))\n",
        "    r2_score_list.append(r2_score(test_targets[:,i], predictions[:,i]))\n",
        " \n",
        "\n",
        " # Compute the normalized mean square error:\n",
        "Norm_RMSE = np.sqrt(np.array(mse_list)) / y_test.describe().loc['mean'].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0MOqWtwxzRnb"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize= (14,4)) \n",
        "sns.barplot(x= df.columns.tolist()[8:], y= r2_score_list) \n",
        "plt.ylim([min(r2_score_list)*0.92,max(r2_score_list)*1.02])\n",
        "plt.ylabel('R2 score', fontsize = 20)\n",
        "plt.grid()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZR2NNMLYzRnc"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize= (10,4)) \n",
        "sns.barplot(x= df.columns.tolist()[8:], y= Norm_RMSE) \n",
        "plt.ylim([0,max(Norm_RMSE)*1.02])\n",
        "plt.ylabel('Normalized Root Mean Square Error', fontsize = 20)\n",
        "plt.grid()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7bJWGF9zRnc"
      },
      "source": [
        "# ============================================================\n",
        "# ==========================Section 2 ==========================\n",
        "# ============================================================\n",
        "### Compare versus the wind2loads neural net"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HdQbK_iozRnc"
      },
      "outputs": [],
      "source": [
        "error on purpose"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kvVzwFAqzRnc"
      },
      "outputs": [],
      "source": [
        "from w2l import neuralnets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qz34TBv6zRnc"
      },
      "outputs": [],
      "source": [
        "# define the model using the same net architecture, and train for the same number of epochs using the previously batch size as well.\n",
        "w2l_net = neuralnets.ann(layersizes = [input_size, num_hid_1, num_hid_2, output_channels],\n",
        "                       params = {'minibatchsize':64, 'nepochs':500}, \n",
        "                       output_style = 'None',\n",
        "                        testratios = [0.7, 0.3, 0.])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8f_LQU-gzRnd"
      },
      "outputs": [],
      "source": [
        "# train using the data from the first split\n",
        "Outdata = w2l_net.train(X.values,y.values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zKsFN84lzRnd"
      },
      "outputs": [],
      "source": [
        "w2l_net.params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8oSktNjXzRnd"
      },
      "outputs": [],
      "source": [
        "Outdata.keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4JWEI6izRnd"
      },
      "source": [
        "$\\color{red}{\\text{Why 500 epochs tho}}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nsafzKKMzRnd"
      },
      "outputs": [],
      "source": [
        "plt.plot(Outdata['Jhist'][50:])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k75YNlcxzRnd"
      },
      "outputs": [],
      "source": [
        "Yout = w2l_net.predict(X_test.values) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R5m1sXI1zRne"
      },
      "outputs": [],
      "source": [
        "for idx, ch in enumerate(df.columns.tolist()[8:]):\n",
        "  # issues with 2 and 7\n",
        "    plt.figure(idx, figsize = (8,6))\n",
        "    sns.scatterplot(Yout[:,idx],test_targets[:,idx])\n",
        "    sns.scatterplot(predictions[:,idx],test_targets[:,idx])\n",
        "    plt.title(f'DELs: {ch}', fontsize = 20)\n",
        "    plt.legend(['W2L','PyTorch'])\n",
        "    plt.xlabel('Predictions [kNm]', fontsize = 12)\n",
        "    plt.ylabel('Targets [kNm]')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uGzKSU3dzRne"
      },
      "outputs": [],
      "source": [
        "W2L_mse_list = []\n",
        "W2L_r2_score_list = []\n",
        "for i in range(len(df.columns.tolist()[8:])):\n",
        "    #print(f'MSE {AllTargetData.columns[i]} Channel : \\n {mean_squared_error(AllTargetData.values[:,i], Yout[:,i])}')\n",
        "    W2L_mse_list.append(mean_squared_error(test_targets[:,i], Yout[:,i]))\n",
        "    W2L_r2_score_list.append(r2_score(test_targets[:,i], Yout[:,i]))\n",
        "    \n",
        "W2L_Norm_RMSE = np.sqrt(np.array(W2L_mse_list)) / y_test.describe().loc['mean'].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OWxxa9MSzRne"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize= (12,6)) \n",
        "sns.barplot(x= df.columns.tolist()[8:], y= W2L_r2_score_list) \n",
        "plt.ylim([min(W2L_r2_score_list)*0.92,max(W2L_r2_score_list)*1.02])\n",
        "plt.ylabel('R2 score', fontsize = 20)\n",
        "plt.grid()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mczhs3xmzRne"
      },
      "source": [
        "# Compare Wind2Loads net versus PyTorch implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5k0G9KK1zRne"
      },
      "outputs": [],
      "source": [
        "barplot_lst = []\n",
        "for i,ch in enumerate(df.columns.tolist()[8:]):\n",
        "    barplot_lst.append(['pytorch',ch,r2_score_list[i]])\n",
        "for i,ch in enumerate(df.columns.tolist()[8:]):\n",
        "    barplot_lst.append(['W2L',ch,W2L_r2_score_list[i]])  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zCZCFJffzRne"
      },
      "outputs": [],
      "source": [
        "df_comparison = pd.DataFrame(barplot_lst,\n",
        "                  columns=['Model','channel','r2'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IIlsLG7FzRne"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12,6))\n",
        "df_comparison.pivot(\"channel\", \"Model\", \"r2\").plot(kind='bar')\n",
        "plt.ylim([min(r2_score_list)*0.98,max(r2_score_list)*1.02])\n",
        "plt.title('R2', fontsize = 18)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VA-jwSfqzRnf"
      },
      "outputs": [],
      "source": [
        "barplot_lst_NMSE = []\n",
        "for i,ch in enumerate(df.columns.tolist()[8:]):\n",
        "    barplot_lst_NMSE.append(['pytorch',ch , Norm_RMSE[i]])\n",
        "for i,ch in enumerate(df.columns.tolist()[8:]):\n",
        "    barplot_lst_NMSE.append(['W2L', ch, W2L_Norm_RMSE[i]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qsiebtcqzRnf"
      },
      "outputs": [],
      "source": [
        "df_NRMSE_comparison = pd.DataFrame(barplot_lst_NMSE,\n",
        "                  columns=['Model','channel','NRMSE'])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kZT_Jd4uzRnf"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(14,6))\n",
        "df_NRMSE_comparison.pivot(\"channel\", \"Model\", \"NRMSE\").plot(kind='bar')\n",
        "plt.ylim([0,max(W2L_Norm_RMSE)*1.02])\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "g6lcj4lLzRnb"
      ],
      "name": "PyTorch_Model_v4_Sherpa.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}