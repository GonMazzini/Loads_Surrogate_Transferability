{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GonMazzini/Loads_Surrogate_Transferability/blob/main/TuningFramework.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9P0l_d61zRnQ"
      },
      "source": [
        "Thits notebook is the MASTER notebook for running hyper-parameter tuning.\n",
        "\n",
        "\n",
        "> Section 1\n",
        "\n",
        "\n",
        "1.   Read data\n",
        "2.   Train-Val-Test split\n",
        "3.   Scale data\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "l9b4V5nszS9-",
        "outputId": "330fd85f-2ca5-410b-8102-77001e57998f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting parameter-sherpa\n",
            "  Downloading parameter-sherpa-1.0.6.tar.gz (513 kB)\n",
            "\u001b[K     |████████████████████████████████| 513 kB 4.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas>=0.20.3 in /usr/local/lib/python3.7/dist-packages (from parameter-sherpa) (1.3.5)\n",
            "Requirement already satisfied: pymongo>=3.5.1 in /usr/local/lib/python3.7/dist-packages (from parameter-sherpa) (4.0.2)\n",
            "Requirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python3.7/dist-packages (from parameter-sherpa) (1.21.5)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from parameter-sherpa) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from parameter-sherpa) (1.0.2)\n",
            "Requirement already satisfied: flask>=0.12.2 in /usr/local/lib/python3.7/dist-packages (from parameter-sherpa) (1.1.4)\n",
            "Collecting GPyOpt>=1.2.5\n",
            "  Downloading GPyOpt-1.2.6.tar.gz (56 kB)\n",
            "\u001b[K     |████████████████████████████████| 56 kB 4.9 MB/s \n",
            "\u001b[?25hCollecting enum34\n",
            "  Downloading enum34-1.1.10-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from parameter-sherpa) (3.2.2)\n",
            "Requirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from flask>=0.12.2->parameter-sherpa) (2.11.3)\n",
            "Requirement already satisfied: click<8.0,>=5.1 in /usr/local/lib/python3.7/dist-packages (from flask>=0.12.2->parameter-sherpa) (7.1.2)\n",
            "Requirement already satisfied: Werkzeug<2.0,>=0.15 in /usr/local/lib/python3.7/dist-packages (from flask>=0.12.2->parameter-sherpa) (1.0.1)\n",
            "Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from flask>=0.12.2->parameter-sherpa) (1.1.0)\n",
            "Collecting GPy>=1.8\n",
            "  Downloading GPy-1.10.0.tar.gz (959 kB)\n",
            "\u001b[K     |████████████████████████████████| 959 kB 43.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from GPy>=1.8->GPyOpt>=1.2.5->parameter-sherpa) (1.15.0)\n",
            "Collecting paramz>=0.9.0\n",
            "  Downloading paramz-0.9.5.tar.gz (71 kB)\n",
            "\u001b[K     |████████████████████████████████| 71 kB 7.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cython>=0.29 in /usr/local/lib/python3.7/dist-packages (from GPy>=1.8->GPyOpt>=1.2.5->parameter-sherpa) (0.29.28)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2<3.0,>=2.10.1->flask>=0.12.2->parameter-sherpa) (2.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.20.3->parameter-sherpa) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.20.3->parameter-sherpa) (2018.9)\n",
            "Requirement already satisfied: decorator>=4.0.10 in /usr/local/lib/python3.7/dist-packages (from paramz>=0.9.0->GPy>=1.8->GPyOpt>=1.2.5->parameter-sherpa) (4.4.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.19.1->parameter-sherpa) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.19.1->parameter-sherpa) (3.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->parameter-sherpa) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->parameter-sherpa) (3.0.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->parameter-sherpa) (1.3.2)\n",
            "Building wheels for collected packages: parameter-sherpa, GPyOpt, GPy, paramz\n",
            "  Building wheel for parameter-sherpa (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for parameter-sherpa: filename=parameter_sherpa-1.0.6-py2.py3-none-any.whl size=542134 sha256=60d00c78087d0f12861da8782a4babf64c47b164eee5b443ed10b6deafcaead2\n",
            "  Stored in directory: /root/.cache/pip/wheels/96/d9/cb/99569566e5e9b3ef0265ba4cbce3ff16f7692988833aa942f5\n",
            "  Building wheel for GPyOpt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for GPyOpt: filename=GPyOpt-1.2.6-py3-none-any.whl size=83609 sha256=70311806697ee8736c7c36769277159d3b0764461b25a7c0cb3910b124a783ba\n",
            "  Stored in directory: /root/.cache/pip/wheels/e6/fa/d1/f9652b5af79f769a0ab74dbead7c7aea9a93c6bc74543fd3ec\n",
            "  Building wheel for GPy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for GPy: filename=GPy-1.10.0-cp37-cp37m-linux_x86_64.whl size=2565092 sha256=52c6501dc1698f4f4e586641a334e212fa1526be6afe18c226c4eae5bed6a98d\n",
            "  Stored in directory: /root/.cache/pip/wheels/f7/18/28/dd1ce0192a81b71a3b086fd952511d088b21e8359ea496860a\n",
            "  Building wheel for paramz (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for paramz: filename=paramz-0.9.5-py3-none-any.whl size=102566 sha256=5bf3edf0896219ce4292885034307ee7c3f59d8f3c81955179140536ce979cfe\n",
            "  Stored in directory: /root/.cache/pip/wheels/c8/95/f5/ce28482da28162e6028c4b3a32c41d147395825b3cd62bc810\n",
            "Successfully built parameter-sherpa GPyOpt GPy paramz\n",
            "Installing collected packages: paramz, GPy, GPyOpt, enum34, parameter-sherpa\n",
            "Successfully installed GPy-1.10.0 GPyOpt-1.2.6 enum34-1.1.10 parameter-sherpa-1.0.6 paramz-0.9.5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "enum"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install parameter-sherpa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Q2oZpZQIzoQT",
        "outputId": "20d0399d-4260-47eb-b70f-c342f4fdebc7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU\n"
          ]
        }
      ],
      "source": [
        "# Import all required libraries\n",
        "\n",
        "from __future__ import print_function\n",
        "import sherpa\n",
        "from sherpa.algorithms import Genetic\n",
        "import time\n",
        "import pandas as pd \n",
        "import numpy as np\n",
        "import math\n",
        "from random import shuffle\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda:0\")\n",
        "    print(\"GPU\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"CPU\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nF2rjMCtzRnU"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Section 1.1: Read data\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "id": "W1kffmHZzRnV",
        "outputId": "e9da2db4-0a4a-4cd3-a8ce-e312171323cc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Unnamed: 0  pointno          U    SigmaU     Alpha      MannL  MannGamma  \\\n",
              "0           0        1   4.000000  0.100000 -0.650000   7.500000   1.000000   \n",
              "1           1        2  10.150758  1.208656 -0.139692  48.470634   1.363636   \n",
              "\n",
              "   VeerDeltaPhi    TT_Mx_avg   TT_My_avg     TB_Mx_avg    TB_My_avg  \\\n",
              "0    -22.250000   747.561872  200.666288   6708.717789  8861.885588   \n",
              "1     -4.771217  3556.031457  676.339081  16692.647572  6329.099515   \n",
              "\n",
              "     TT_Mz_avg    MS_Mz_avg     BR_Mx_avg     BR_My_avg  \n",
              "0   819.209904    63.457528   4253.317748  15006.726860  \n",
              "1  3746.460605  1354.995442  10409.290476  16289.414152  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e9fefca2-44b2-4cc7-a6cd-7895cbeb16ec\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>pointno</th>\n",
              "      <th>U</th>\n",
              "      <th>SigmaU</th>\n",
              "      <th>Alpha</th>\n",
              "      <th>MannL</th>\n",
              "      <th>MannGamma</th>\n",
              "      <th>VeerDeltaPhi</th>\n",
              "      <th>TT_Mx_avg</th>\n",
              "      <th>TT_My_avg</th>\n",
              "      <th>TB_Mx_avg</th>\n",
              "      <th>TB_My_avg</th>\n",
              "      <th>TT_Mz_avg</th>\n",
              "      <th>MS_Mz_avg</th>\n",
              "      <th>BR_Mx_avg</th>\n",
              "      <th>BR_My_avg</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>-0.650000</td>\n",
              "      <td>7.500000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-22.250000</td>\n",
              "      <td>747.561872</td>\n",
              "      <td>200.666288</td>\n",
              "      <td>6708.717789</td>\n",
              "      <td>8861.885588</td>\n",
              "      <td>819.209904</td>\n",
              "      <td>63.457528</td>\n",
              "      <td>4253.317748</td>\n",
              "      <td>15006.726860</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>10.150758</td>\n",
              "      <td>1.208656</td>\n",
              "      <td>-0.139692</td>\n",
              "      <td>48.470634</td>\n",
              "      <td>1.363636</td>\n",
              "      <td>-4.771217</td>\n",
              "      <td>3556.031457</td>\n",
              "      <td>676.339081</td>\n",
              "      <td>16692.647572</td>\n",
              "      <td>6329.099515</td>\n",
              "      <td>3746.460605</td>\n",
              "      <td>1354.995442</td>\n",
              "      <td>10409.290476</td>\n",
              "      <td>16289.414152</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e9fefca2-44b2-4cc7-a6cd-7895cbeb16ec')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-e9fefca2-44b2-4cc7-a6cd-7895cbeb16ec button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-e9fefca2-44b2-4cc7-a6cd-7895cbeb16ec');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "df = pd.read_excel('LoadsDataBase_6D_Set123_FiltMinMaxCrit.xlsx') # Average the values from Set1,Set2 and Set3.\n",
        "df.head(2)\n",
        "# 0 : TT_Mx_avg# 1 : TT_My_avg# 2 : TB_Mx_avg# 3 : TB_My_avg# 4 : MS_Mz_avg# 5 : BR_Mx_avg# 6 : BR_My_avg# 7 : TT-Mz_avg"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "Section 1.2: Train-Val-Test split."
      ],
      "metadata": {
        "id": "whDMqYOyKsWS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "gTxt7cZJzRnV"
      },
      "outputs": [],
      "source": [
        "X = df.iloc[:,2:8]\n",
        "y = df.iloc[:,8:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eX_LPh9wzRnW",
        "outputId": "78abeb32-d62f-4d82-daaa-2c431f2f96bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The filtered data set consits on: 7664 entries.\n",
            "A total of 6131 will be used for training and validation.\n",
            "A total of 1533 will be used for testing the final model.\n"
          ]
        }
      ],
      "source": [
        "# Test split:\n",
        "X, X_test, y, y_test = train_test_split(X,y, test_size = 0.2, shuffle = True,  random_state = 101)\n",
        "\n",
        "print(f'The filtered data set consits on: {len(df)} entries.')\n",
        "print(f'A total of {len(X)} will be used for training and validation.')\n",
        "print(f'A total of {len(X_test)} will be used for testing the final model.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6sjTHp4zRnW"
      },
      "source": [
        "---\n",
        "Section 1.3: Feature scaling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "4nBJ9T1TzRnW"
      },
      "outputs": [],
      "source": [
        "feature_range = (0, 1)\n",
        "scaler_x = preprocessing.MinMaxScaler(feature_range=feature_range).fit(X)\n",
        "X_scaled = scaler_x.transform(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fp5EhbtozRnW"
      },
      "source": [
        "### Separte between train and validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "_kR4yaL0zRnX"
      },
      "outputs": [],
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(X_scaled,y.values, test_size = 0.2, shuffle = True,  random_state = 101)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ezp2exU1NR36",
        "outputId": "f7b145c7-4b92-4d41-cf59-cfc3e917b7cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A total of 4904 for training, 64.0 % of total data\n",
            "A total of 1227 for validation, 16.0 % of total data\n",
            "A total of 1533 for testing, 20.0 % of total data\n"
          ]
        }
      ],
      "source": [
        "# printing number of samples for train-validation-test\n",
        "print(f'A total of {y_train.shape[0]} for training, {round(100*y_train.shape[0]/len(df),1)} % of total data')\n",
        "print(f'A total of {y_val.shape[0]} for validation, {round(100*y_val.shape[0]/len(df),1)} % of total data')\n",
        "print(f'A total of {y_test.shape[0]} for testing, {round(100*y_test.shape[0]/len(df),1)} % of total data')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Y6KSzGizRnX"
      },
      "source": [
        "### The PyTorch worfklow can be summarized as follows:\n",
        "- 1) Design model (input, output size, forward pass)\n",
        "- 2) Construct loss and optimizer\n",
        "- 3) Training loop\n",
        "   - forward pass: compute prediction based on the current weights and biases of the net\n",
        "   - backward pass: compute the gradients of the loss function wrt. to model parameters\n",
        "   - update weigths in"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Select the model according to the hyper-parameter to be tuned. The following classes are available:\n",
        "\n",
        "> *BaseModel* (**just for number of hidden units**)\n",
        "\n",
        "\n",
        "*   2 hidden layers with same number of hidden units.\n",
        "*   Weights initialized with Normal Kaimin (=He)\n",
        "*   ReLu act_fn\n",
        "\n",
        "> *VariableLayers* (**just for number of hidden units**)\n",
        "\n",
        "\n",
        "*   Variable number of Hidden Layers\n",
        "*   Weights initialized with Normal Kaimin (=He)\n",
        "*   ReLu act_fn\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Bb7kqq74LuI_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "hVfzqEXbzRnX"
      },
      "outputs": [],
      "source": [
        "input_size = 6             # np.shape(X_train)[1]\n",
        "output_channels = 8        # np.shape(y_train)[1]\n",
        "hidden_size = 50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "hEj_xF9pzRnX"
      },
      "outputs": [],
      "source": [
        "class BaseModel(nn.Module):\n",
        "    \n",
        "    def __init__(self, hidden_size):   \n",
        "        super(BaseModel,self).__init__()  # inherit from the superclass Module\n",
        "        self.hidden_size = hidden_size\n",
        "        \n",
        "        self.fc1 = nn.Linear(in_features= input_size,\n",
        "                             out_features= self.hidden_size,                             \n",
        "                            bias = True)  \n",
        "        nn.init.kaiming_normal_(self.fc1.weight)\n",
        "\n",
        "        self.fc2 = nn.Linear(in_features = self.hidden_size, \n",
        "                             out_features = self.hidden_size,\n",
        "                            bias = True)\n",
        "        nn.init.kaiming_normal_(self.fc1.weight)\n",
        "\n",
        "        self.fc3 = nn.Linear(in_features = self.hidden_size, \n",
        "                             out_features = output_channels,\n",
        "                            bias = True)\n",
        "        nn.init.kaiming_normal_(self.fc1.weight)\n",
        "        \n",
        "    def forward(self,x):\n",
        "        \n",
        "        out = self.fc1(x)  \n",
        "        out = F.relu(out)\n",
        "        out = self.fc2(out)\n",
        "        out = F.relu(out)\n",
        "        out = self.fc3(out)                       #  torch.tanh(self.fc3(out))\n",
        "        \n",
        "        return out  "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO :: Add HE intializing (how to acces each module?)\n",
        "\n",
        "class VariableLayers(nn.Module):\n",
        "\n",
        "    \"\"\" A feedforward network designed for tuning number of layers and hidden units.\n",
        "    By @GonMazzini\"\"\"\n",
        "\n",
        "    def __init__(self, input_dim, output_dim, n_hidLayers, hidden_size):\n",
        "        super(VariableLayers, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.hidden_size = hidden_size\n",
        "        self.n_hidLayers = n_hidLayers\n",
        "        current_dim = input_dim\n",
        "        self.layers = nn.ModuleList()\n",
        "        \n",
        "        for hdim in [self.hidden_size]*self.n_hidLayers:\n",
        "            self.layers.append(nn.Linear(current_dim, hdim))\n",
        "            current_dim = hdim\n",
        "        self.layers.append(nn.Linear(current_dim, output_dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.layers[:-1]:\n",
        "            x = F.relu(layer(x))\n",
        "        out = F.relu(self.layers[-1](x))\n",
        "        return out "
      ],
      "metadata": {
        "id": "ozA9yvk2MWuP"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "DhY9tsfCb2tO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kQJG0CJ8zRnY"
      },
      "outputs": [],
      "source": [
        "# instantiate the class\n",
        "#model = Net(hidden_size)  # will be instantiated inside HP loop\n",
        "\n",
        "# Define the loss function (mean square error)\n",
        "loss = nn.MSELoss()\n",
        "\n",
        "# Instantiate optimizer passing the net parameters as argument, and learning rate\n",
        "#optimizer = optim.Adam(model.parameters(), lr = 0.01)  # will be instantiated inside HP loop \n",
        "\n",
        "# list to store results\n",
        "train_losses , val_losses= [],[]\n",
        "\n",
        "# Try a dummy forward\n",
        "#model(torch.tensor(X_train[0]).float())  # beware that need to convert from double to float"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xO8OBaiRzRnY"
      },
      "source": [
        "# 2.1- Use the PyTorch DataLoader and Dataset utils.\n",
        "- DataLoader class combines a dataset and a sampler, and provides an iterable over the given dataset for training the model\n",
        "- Dataset: just an abstract class representing a :class:`Dataset`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "guVFdZgMzRnY"
      },
      "outputs": [],
      "source": [
        "class FatigueLoads_TrainSet(Dataset):\n",
        "\n",
        "    def __init__(self):\n",
        "        self.n_samples = X_train.shape[0]\n",
        "        self.x_data = torch.from_numpy(X_train) # size [n_samples, n_features]\n",
        "        self.y_data = torch.from_numpy(y_train) # size [n_samples, 1]\n",
        "\n",
        "    # support indexing such that dataset[i] can be used to get i-th sample\n",
        "    def __getitem__(self, index):\n",
        "        return self.x_data[index], self.y_data[index]\n",
        "\n",
        "    # we can call len(dataset) to return the size\n",
        "    def __len__(self):\n",
        "        return self.n_samples\n",
        "    \n",
        "class FatigueLoads_ValidationSet(Dataset):\n",
        "\n",
        "    def __init__(self):\n",
        "        self.n_samples = X_val.shape[0]\n",
        "        self.x_data = torch.from_numpy(X_val) # size [n_samples, n_features]\n",
        "        self.y_data = torch.from_numpy(y_val) # size [n_samples, 1]\n",
        "\n",
        "    # support indexing such that dataset[i] can be used to get i-th sample\n",
        "    def __getitem__(self, index):\n",
        "        return self.x_data[index], self.y_data[index]\n",
        "\n",
        "    # we can call len(dataset) to return the size\n",
        "    def __len__(self):\n",
        "        return self.n_samples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sp46xHZVzRnZ"
      },
      "source": [
        "### Get first sample and unpack. Note that the enviromental inputs are normalized using MinMaxScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zALPQBILzRnZ"
      },
      "outputs": [],
      "source": [
        "train_dataset = FatigueLoads_TrainSet()\n",
        "valid_dataset = FatigueLoads_ValidationSet()\n",
        "\n",
        "first_data = train_dataset[0]\n",
        "features, loads = first_data\n",
        "print(features, loads)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "323d_UnszRnZ"
      },
      "outputs": [],
      "source": [
        "batch_size = 128\n",
        "num_epochs = 1200\n",
        "\n",
        "num_batches_train = X_train.shape[0] // batch_size\n",
        "num_batches_test = X_val.shape[0] // batch_size\n",
        "\n",
        "train_loader = DataLoader(dataset=train_dataset,\n",
        "                          batch_size=batch_size,\n",
        "                          shuffle=True,\n",
        "                          num_workers=0)\n",
        "\n",
        "valid_loader = DataLoader(dataset=valid_dataset,\n",
        "                          batch_size=batch_size,\n",
        "                          shuffle=True,\n",
        "                          num_workers=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5KXldvsU08MV"
      },
      "outputs": [],
      "source": [
        "algorithm = sherpa.algorithms.GridSearch(num_grid_points=2)\n",
        "parameters = [sherpa.Discrete('hidden_size', [50, 512]),\n",
        "              sherpa.Continuous('lr',[0.01,0.001])]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fXK9eWS81rHZ"
      },
      "outputs": [],
      "source": [
        "study = sherpa.Study(parameters=parameters,\n",
        "                     algorithm=algorithm,\n",
        "                     lower_is_better=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GnOKI0762M9u"
      },
      "outputs": [],
      "source": [
        "for trial in study:\n",
        "  print(\"Trial {}:\\t{}\".format(trial.id, trial.parameters))\n",
        "  model = Net(trial.parameters['hidden_size'])\n",
        "  #model.train()\n",
        "  optimizer = optim.Adam(model.parameters(), lr = trial.parameters['lr'])\n",
        "  \n",
        "  for epoch in range(num_epochs):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    model.train()\n",
        "    \n",
        "    for i, (inputs, loads) in enumerate(train_loader):\n",
        "        #print(features, loads)\n",
        "        \n",
        "        optimizer.zero_grad()                      # zeroize accumulated gradients in parameters             \n",
        "        \n",
        "        output = model(inputs.float())             # forwards pass       \n",
        "        batch_loss = loss(output, loads.float())   # compute loss for current batch\n",
        "        \n",
        "        batch_loss.backward()                      # compute the gradient of the loss wrt. model parameters\n",
        "        optimizer.step()                           # update weights according to the comptued gradients\n",
        "        \n",
        "    \n",
        "    epoch_loss_train = 0\n",
        "    epoch_loss_test = 0\n",
        "    model.eval()\n",
        "    \n",
        "    ##### Evaluate training\n",
        "    for i, (inputs, loads) in enumerate(train_loader):\n",
        "        \n",
        "        output = model(inputs.float())\n",
        "        \n",
        "        batch_loss_train = loss(output, loads.float())  # compute loss for the current batch\n",
        "        epoch_loss_train += batch_loss_train            # accumulate loss for the current epoch\n",
        "        \n",
        "        #print(f'Epoch: {epoch+1}/{num_epochs}  | Step {i+1}/{n_iterations}')\n",
        "    \n",
        "    ##### Evaluate validation    \n",
        "    for i, (inputs, loads) in enumerate(valid_loader):\n",
        "        \n",
        "        output = model(inputs.float())\n",
        "        \n",
        "        batch_loss_test = loss(output, loads.float())  # compute loss for the current batch\n",
        "        epoch_loss_test += batch_loss_test     # accumulate loss for the current epoch\n",
        "        \n",
        "        #print(f'Epoch: {epoch+1}/{num_epochs}  | Step {i+1}/{n_iterations}')\n",
        "    \n",
        "    if epoch % 50 == 0: \n",
        "        print(f'Epoch: {epoch+1}/{num_epochs} | Train loss: {epoch_loss_train/num_batches_train}       | Val loss {epoch_loss_test/num_batches_test}')\n",
        "\n",
        "        study.add_observation(trial=trial,\n",
        "                              iteration=epoch,\n",
        "                              objective=epoch_loss_test.detach().numpy())\n",
        "    \n",
        "    if study.should_trial_stop(trial):\n",
        "        break \n",
        "    # store in list for plotting the loss per epoch    \n",
        "    val_losses.append(epoch_loss_test/num_batches_test)  \n",
        "    train_losses.append(epoch_loss_train/num_batches_train)  \n",
        "  #study.finalize(trial)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z_uF07KUG5uX"
      },
      "outputs": [],
      "source": [
        "study.results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "diZ_MjPfzRna"
      },
      "source": [
        "# 3.1- Start training loop "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f6-pIKVRzRna"
      },
      "outputs": [],
      "source": [
        "for epoch in range(num_epochs):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    model.train()\n",
        "    \n",
        "    for i, (inputs, loads) in enumerate(train_loader):\n",
        "        #print(features, loads)\n",
        "        \n",
        "        optimizer.zero_grad()                      # zeroize accumulated gradients in parameters             \n",
        "        \n",
        "        output = model(inputs.float())             # forwards pass       \n",
        "        batch_loss = loss(output, loads.float())   # compute loss for current batch\n",
        "        \n",
        "        batch_loss.backward()                      # compute the gradient of the loss wrt. model parameters\n",
        "        optimizer.step()                           # update weights according to the comptued gradients\n",
        "        \n",
        "    \n",
        "    epoch_loss_train = 0\n",
        "    epoch_loss_test = 0\n",
        "    model.eval()\n",
        "    \n",
        "    ##### Evaluate training\n",
        "    for i, (inputs, loads) in enumerate(train_loader):\n",
        "        \n",
        "        output = model(inputs.float())\n",
        "        \n",
        "        batch_loss_train = loss(output, loads.float())  # compute loss for the current batch\n",
        "        epoch_loss_train += batch_loss_train            # accumulate loss for the current epoch\n",
        "        \n",
        "        #print(f'Epoch: {epoch+1}/{num_epochs}  | Step {i+1}/{n_iterations}')\n",
        "    \n",
        "    ##### Evaluate validation    \n",
        "    for i, (inputs, loads) in enumerate(valid_loader):\n",
        "        \n",
        "        output = model(inputs.float())\n",
        "        \n",
        "        batch_loss_test = loss(output, loads.float())  # compute loss for the current batch\n",
        "        epoch_loss_test += batch_loss_test     # accumulate loss for the current epoch\n",
        "        \n",
        "        #print(f'Epoch: {epoch+1}/{num_epochs}  | Step {i+1}/{n_iterations}')\n",
        "    \n",
        "    if epoch % 100 == 0: \n",
        "        print(f'Epoch: {epoch+1}/{num_epochs} | Train loss: {epoch_loss_train/num_batches_train}       | Val loss {epoch_loss_test/num_batches_test}')\n",
        "        \n",
        "    # store in list for plotting the loss per epoch    \n",
        "    val_losses.append(epoch_loss_test/num_batches_test)  \n",
        "    train_losses.append(epoch_loss_train/num_batches_train)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vgCFr4fIzRna"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize = (8,6))\n",
        "plt.plot(train_losses[50:])\n",
        "plt.plot(val_losses[50:], '-k')\n",
        "plt.title('Training-Validation loss', fontsize = 16)\n",
        "plt.grid()\n",
        "plt.legend(['Training loss','Validation loss'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSkpKZ2HzRna"
      },
      "source": [
        "# Evalute on the test data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6lcj4lLzRnb"
      },
      "source": [
        "### Scale the test data before evaluating the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e-TPXw0rzRnb"
      },
      "outputs": [],
      "source": [
        "Test_scaler_x = preprocessing.MinMaxScaler(feature_range=feature_range).fit(X_test) # maybe try another for X?\n",
        "X_Test_scaled = Test_scaler_x.transform(X_test)\n",
        "\n",
        "predictions = model(torch.tensor(X_Test_scaled).float()).detach().numpy()\n",
        "test_targets = y_test.values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ySZ9om3tzRnb"
      },
      "outputs": [],
      "source": [
        "for idx, ch in enumerate(df.columns.tolist()[8:]):\n",
        "  # issues with 2 and 7\n",
        "    plt.figure(idx, figsize = (8,6))\n",
        "    sns.scatterplot(predictions[:,idx],test_targets[:,idx])\n",
        "    plt.title(f'DELs: {ch}', fontsize = 20)\n",
        "    plt.xlabel('Predictions [kNm]', fontsize = 12)\n",
        "    plt.ylabel('Targets [kNm]')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Py30VR4zRnb"
      },
      "outputs": [],
      "source": [
        "mse_list = []\n",
        "r2_score_list = []\n",
        "for i in range(len(df.columns.tolist()[8:])):\n",
        "    #print(f'MSE {AllTargetData.columns[i]} Channel : \\n {mean_squared_error(AllTargetData.values[:,i], Yout[:,i])}')\n",
        "    mse_list.append(mean_squared_error(test_targets[:,i], predictions[:,i]))\n",
        "    r2_score_list.append(r2_score(test_targets[:,i], predictions[:,i]))\n",
        " \n",
        "\n",
        " # Compute the normalized mean square error:\n",
        "Norm_RMSE = np.sqrt(np.array(mse_list)) / y_test.describe().loc['mean'].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0MOqWtwxzRnb"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize= (14,4)) \n",
        "sns.barplot(x= df.columns.tolist()[8:], y= r2_score_list) \n",
        "plt.ylim([min(r2_score_list)*0.92,max(r2_score_list)*1.02])\n",
        "plt.ylabel('R2 score', fontsize = 20)\n",
        "plt.grid()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZR2NNMLYzRnc"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize= (10,4)) \n",
        "sns.barplot(x= df.columns.tolist()[8:], y= Norm_RMSE) \n",
        "plt.ylim([0,max(Norm_RMSE)*1.02])\n",
        "plt.ylabel('Normalized Root Mean Square Error', fontsize = 20)\n",
        "plt.grid()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7bJWGF9zRnc"
      },
      "source": [
        "# ============================================================\n",
        "# ==========================Section 2 ==========================\n",
        "# ============================================================\n",
        "### Compare versus the wind2loads neural net"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HdQbK_iozRnc"
      },
      "outputs": [],
      "source": [
        "error on purpose"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kvVzwFAqzRnc"
      },
      "outputs": [],
      "source": [
        "from w2l import neuralnets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qz34TBv6zRnc"
      },
      "outputs": [],
      "source": [
        "# define the model using the same net architecture, and train for the same number of epochs using the previously batch size as well.\n",
        "w2l_net = neuralnets.ann(layersizes = [input_size, num_hid_1, num_hid_2, output_channels],\n",
        "                       params = {'minibatchsize':64, 'nepochs':500}, \n",
        "                       output_style = 'None',\n",
        "                        testratios = [0.7, 0.3, 0.])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8f_LQU-gzRnd"
      },
      "outputs": [],
      "source": [
        "# train using the data from the first split\n",
        "Outdata = w2l_net.train(X.values,y.values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zKsFN84lzRnd"
      },
      "outputs": [],
      "source": [
        "w2l_net.params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8oSktNjXzRnd"
      },
      "outputs": [],
      "source": [
        "Outdata.keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4JWEI6izRnd"
      },
      "source": [
        "$\\color{red}{\\text{Why 500 epochs tho}}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nsafzKKMzRnd"
      },
      "outputs": [],
      "source": [
        "plt.plot(Outdata['Jhist'][50:])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k75YNlcxzRnd"
      },
      "outputs": [],
      "source": [
        "Yout = w2l_net.predict(X_test.values) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R5m1sXI1zRne"
      },
      "outputs": [],
      "source": [
        "for idx, ch in enumerate(df.columns.tolist()[8:]):\n",
        "  # issues with 2 and 7\n",
        "    plt.figure(idx, figsize = (8,6))\n",
        "    sns.scatterplot(Yout[:,idx],test_targets[:,idx])\n",
        "    sns.scatterplot(predictions[:,idx],test_targets[:,idx])\n",
        "    plt.title(f'DELs: {ch}', fontsize = 20)\n",
        "    plt.legend(['W2L','PyTorch'])\n",
        "    plt.xlabel('Predictions [kNm]', fontsize = 12)\n",
        "    plt.ylabel('Targets [kNm]')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uGzKSU3dzRne"
      },
      "outputs": [],
      "source": [
        "W2L_mse_list = []\n",
        "W2L_r2_score_list = []\n",
        "for i in range(len(df.columns.tolist()[8:])):\n",
        "    #print(f'MSE {AllTargetData.columns[i]} Channel : \\n {mean_squared_error(AllTargetData.values[:,i], Yout[:,i])}')\n",
        "    W2L_mse_list.append(mean_squared_error(test_targets[:,i], Yout[:,i]))\n",
        "    W2L_r2_score_list.append(r2_score(test_targets[:,i], Yout[:,i]))\n",
        "    \n",
        "W2L_Norm_RMSE = np.sqrt(np.array(W2L_mse_list)) / y_test.describe().loc['mean'].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OWxxa9MSzRne"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize= (12,6)) \n",
        "sns.barplot(x= df.columns.tolist()[8:], y= W2L_r2_score_list) \n",
        "plt.ylim([min(W2L_r2_score_list)*0.92,max(W2L_r2_score_list)*1.02])\n",
        "plt.ylabel('R2 score', fontsize = 20)\n",
        "plt.grid()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mczhs3xmzRne"
      },
      "source": [
        "# Compare Wind2Loads net versus PyTorch implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5k0G9KK1zRne"
      },
      "outputs": [],
      "source": [
        "barplot_lst = []\n",
        "for i,ch in enumerate(df.columns.tolist()[8:]):\n",
        "    barplot_lst.append(['pytorch',ch,r2_score_list[i]])\n",
        "for i,ch in enumerate(df.columns.tolist()[8:]):\n",
        "    barplot_lst.append(['W2L',ch,W2L_r2_score_list[i]])  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zCZCFJffzRne"
      },
      "outputs": [],
      "source": [
        "df_comparison = pd.DataFrame(barplot_lst,\n",
        "                  columns=['Model','channel','r2'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IIlsLG7FzRne"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12,6))\n",
        "df_comparison.pivot(\"channel\", \"Model\", \"r2\").plot(kind='bar')\n",
        "plt.ylim([min(r2_score_list)*0.98,max(r2_score_list)*1.02])\n",
        "plt.title('R2', fontsize = 18)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VA-jwSfqzRnf"
      },
      "outputs": [],
      "source": [
        "barplot_lst_NMSE = []\n",
        "for i,ch in enumerate(df.columns.tolist()[8:]):\n",
        "    barplot_lst_NMSE.append(['pytorch',ch , Norm_RMSE[i]])\n",
        "for i,ch in enumerate(df.columns.tolist()[8:]):\n",
        "    barplot_lst_NMSE.append(['W2L', ch, W2L_Norm_RMSE[i]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qsiebtcqzRnf"
      },
      "outputs": [],
      "source": [
        "df_NRMSE_comparison = pd.DataFrame(barplot_lst_NMSE,\n",
        "                  columns=['Model','channel','NRMSE'])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kZT_Jd4uzRnf"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(14,6))\n",
        "df_NRMSE_comparison.pivot(\"channel\", \"Model\", \"NRMSE\").plot(kind='bar')\n",
        "plt.ylim([0,max(W2L_Norm_RMSE)*1.02])\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "g6lcj4lLzRnb"
      ],
      "name": "PyTorch_Model_v4_Sherpa.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}