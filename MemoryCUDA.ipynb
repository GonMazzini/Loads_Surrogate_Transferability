{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MemoryCUDA.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPe6GaFK57zvsMEGjl0JboB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GonMazzini/Loads_Surrogate_Transferability/blob/main/MemoryCUDA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Memory in PyTorch\n",
        "# A basic notebook to understand the GPU allocated memory.\n",
        "\n",
        "#by GonMazzini     \n",
        "## https://github.com/GonMazzini"
      ],
      "metadata": {
        "id": "OWgcHmRU-no7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "FArhnT0y-Zl6"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Simple example with a torch tensor."
      ],
      "metadata": {
        "id": "NuhZcy-W-r9N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn(5000,5000, dtype = torch.float64).to('cuda')"
      ],
      "metadata": {
        "id": "oCb7SZiX-bb8"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "5000*5000*64/(1024*1024*8) # MB"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "obCYTfz3-1mJ",
        "outputId": "dd0c7513-bad3-45aa-a90f-144e51b36033"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "190.73486328125"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.get_device_name(device=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "XpjR8AQG_OzS",
        "outputId": "4572dfaa-238d-4ad2-beae-64dd4608364b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Tesla K80'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.memory_allocated()/(1024*1024)  # MB"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Bv-HeWb_TQW",
        "outputId": "c0cdcd23-7d7f-49cf-838d-1c673ed0e6ea"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "190.73486328125"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.memory_reserved()/(1024*1024) # MB"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LKWpIcpH_57M",
        "outputId": "efaaa6fa-40a1-4d35-d34c-c2f5e4460ba6"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "192.0"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example with a model\n",
        "\n",
        "https://medium.com/deep-learning-for-protein-design/a-comprehensive-guide-to-memory-usage-in-pytorch-b9b7c78031d3"
      ],
      "metadata": {
        "id": "PLG2Zf76BaDS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "    \"\"\" A feedforward network designed for tuning number of layers and hidden units.\n",
        "    By @GonMazzini\"\"\"\n",
        "    def __init__(self, input_dim, output_dim, n_hidLayers, hidden_size):\n",
        "        super(MLP, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.hidden_size = hidden_size\n",
        "        self.n_hidLayers = n_hidLayers\n",
        "        current_dim = input_dim\n",
        "        self.layers = nn.ModuleList()\n",
        "        \n",
        "        for hdim in [self.hidden_size]*self.n_hidLayers:\n",
        "            self.layers.append(nn.Linear(current_dim, hdim))\n",
        "            current_dim = hdim\n",
        "        self.layers.append(nn.Linear(current_dim, output_dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.layers[:-1]:\n",
        "            x = F.relu(layer(x))\n",
        "        out = F.relu(self.layers[-1](x))\n",
        "        return out "
      ],
      "metadata": {
        "id": "4dPEcroSBc2D"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MLP(6,8,2,50)\n",
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m4TmHj3EBr-n",
        "outputId": "96b85c33-b4f4-4259-fe8a-28e3ff1df871"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLP(\n",
              "  (layers): ModuleList(\n",
              "    (0): Linear(in_features=6, out_features=50, bias=True)\n",
              "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
              "    (2): Linear(in_features=50, out_features=8, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "pytorch_total_params"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GnV_7u-YB1pj",
        "outputId": "1ecd4df4-6b40-4bf0-c661-992878852600"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3308"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn(1,pytorch_total_params, dtype = torch.float64).to('cuda')"
      ],
      "metadata": {
        "id": "PTZ9jxOeB8fd"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "1*3308*64/(1024*1024*8)  # MB"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WlpxSvxECKa-",
        "outputId": "89fea49c-e097-4f5c-a378-5746de89f7d0"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.025238037109375"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.memory_allocated(device)/(1024*1024)  # MB\n",
        "# Returns the current GPU memory occupied by tensors in bytes for a given device."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48RlUsNTB-wA",
        "outputId": "702f33a5-af0c-4f4d-fbee-acd81ec5cf49"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.025390625"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "0.025390625*1024*1024 # 26624.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vtxbbWZDE0ym",
        "outputId": "64d38ea0-acd4-4c2f-c7a0-33f41cfe9ef5"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "26624.0"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "in_size=6\n",
        "out_size=8\n",
        "hidden_size=50\n",
        "\n",
        "model = nn.Sequential(nn.Linear(in_size, hidden_size),\n",
        "                    *[nn.Linear(hidden_size, hidden_size) for _ in range(1)],\n",
        "                    nn.Linear(hidden_size, out_size))\n",
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bOI0ApABDpTY",
        "outputId": "1781e8cf-4fbb-43d2-92b9-72848b8c4283"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Linear(in_features=6, out_features=50, bias=True)\n",
              "  (1): Linear(in_features=50, out_features=50, bias=True)\n",
              "  (2): Linear(in_features=50, out_features=8, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "pytorch_total_params"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xvz6C-MtEH1m",
        "outputId": "c5cb8c04-b21d-44c6-de3b-22a25ffe1add"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3308"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def estimate_memory(model, sample_input, optimizer_type=torch.optim.Adam, batch_size=1, use_amp=False, device=0):\n",
        "    \"\"\"Predict the maximum memory usage of the model. \n",
        "    Args:\n",
        "        optimizer_type (Type): the class name of the optimizer to instantiate\n",
        "        model (nn.Module): the neural network model\n",
        "        sample_input (torch.Tensor): A sample input to the network. It should be \n",
        "            a single item, not a batch, and it will be replicated batch_size times.\n",
        "        batch_size (int): the batch size\n",
        "        use_amp (bool): whether to estimate based on using mixed precision\n",
        "        device (torch.device): the device to use\n",
        "    \"\"\"\n",
        "    # Reset model and optimizer\n",
        "    model.cpu()\n",
        "    optimizer = optimizer_type(model.parameters(), lr=.001)\n",
        "    a = torch.cuda.memory_allocated(device)\n",
        "    model.to(device)\n",
        "    b = torch.cuda.memory_allocated(device)\n",
        "    model_memory = b - a\n",
        "    model_input = sample_input.unsqueeze(0).repeat(batch_size, 1)\n",
        "    output = model(model_input.to(device)).sum()\n",
        "    c = torch.cuda.memory_allocated(device)\n",
        "    if use_amp:\n",
        "        amp_multiplier = .5\n",
        "    else:\n",
        "        amp_multiplier = 1\n",
        "    forward_pass_memory = (c - b)*amp_multiplier\n",
        "    gradient_memory = model_memory\n",
        "    if isinstance(optimizer, torch.optim.Adam):\n",
        "        o = 2\n",
        "    elif isinstance(optimizer, torch.optim.RMSprop):\n",
        "        o = 1\n",
        "    elif isinstance(optimizer, torch.optim.SGD):\n",
        "        o = 0\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported optimizer. Look up how many moments are\" +\n",
        "            \"stored by your optimizer and add a case to the optimizer checker.\")\n",
        "    gradient_moment_memory = o*gradient_memory\n",
        "    total_memory = model_memory + forward_pass_memory + gradient_memory + gradient_moment_memory\n",
        "\n",
        "    return total_memory\n",
        "\n",
        "def test_memory(in_size=6, out_size=8, hidden_size=50, optimizer_type=torch.optim.Adam, batch_size=1, use_amp=False, device=0):\n",
        "    sample_input = torch.randn(batch_size, in_size, dtype=torch.float32)\n",
        "    model = nn.Sequential(nn.Linear(in_size, hidden_size),\n",
        "                        *[nn.Linear(hidden_size, hidden_size) for _ in range(1)],\n",
        "                        nn.Linear(hidden_size, out_size))\n",
        "    \n",
        "    pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print(f'number of model parameters /byme: {pytorch_total_params}')\n",
        "\n",
        "    max_mem_est = estimate_memory(model, sample_input[0], optimizer_type=optimizer_type, batch_size=batch_size, use_amp=use_amp)\n",
        "    print(\"Maximum Memory Estimate\", max_mem_est)\n",
        "    optimizer = optimizer_type(model.parameters(), lr=.001)\n",
        "    print(\"Beginning mem:\", torch.cuda.memory_allocated(device), \"Note - this may be higher than 0, which is due to PyTorch caching. Don't worry too much about this number\")\n",
        "    model.to(device)\n",
        "    print(\"After model to device:\", torch.cuda.memory_allocated(device))\n",
        "    for i in range(3):\n",
        "        print(\"Iteration\", i)\n",
        "        with torch.cuda.amp.autocast(enabled=use_amp):\n",
        "            a = torch.cuda.memory_allocated(device)\n",
        "            out = model(sample_input.to(device)).sum() # Taking the sum here just to get a scalar output\n",
        "            b = torch.cuda.memory_allocated(device)\n",
        "        print(\"1 - After forward pass\", torch.cuda.memory_allocated(device))\n",
        "        print(\"2 - Memory consumed by forward pass\", b - a)\n",
        "        out.backward()\n",
        "        print(\"3 - After backward pass\", torch.cuda.memory_allocated(device))\n",
        "        optimizer.step()\n",
        "        print(\"4 - After optimizer step\", torch.cuda.memory_allocated(device))"
      ],
      "metadata": {
        "id": "d5e-TzdoBtEf"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " torch.cuda.memory_allocated(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dfV3lxmfDWJj",
        "outputId": "fff7b0e2-e700-4909-b928-f085a5ec0764"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_memory(batch_size = 64)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OIh0rJeoIvHa",
        "outputId": "0e41a964-2213-4650-e2e0-3afc375adbf4"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of model parameters /byme: 3308\n",
            "Maximum Memory Estimate 89088\n",
            "Beginning mem: 15360 Note - this may be higher than 0, which is due to PyTorch caching. Don't worry too much about this number\n",
            "After model to device: 15360\n",
            "Iteration 0\n",
            "1 - After forward pass 43008\n",
            "2 - Memory consumed by forward pass 27648\n",
            "3 - After backward pass 31232\n",
            "4 - After optimizer step 61952\n",
            "Iteration 1\n",
            "1 - After forward pass 89088\n",
            "2 - Memory consumed by forward pass 27136\n",
            "3 - After backward pass 61952\n",
            "4 - After optimizer step 61952\n",
            "Iteration 2\n",
            "1 - After forward pass 89088\n",
            "2 - Memory consumed by forward pass 27136\n",
            "3 - After backward pass 61952\n",
            "4 - After optimizer step 61952\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_memory(batch_size = 64)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SHNC0i8KCmkP",
        "outputId": "a14bd773-551d-46d9-95e8-5bbf9735588d"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of model parameters: 5858\n",
            "Beginning mem: 0 Note - this may be higher than 0, which is due to PyTorch caching. Don't worry too much about this number\n",
            "After model to device: 26112\n",
            "Iteration 0\n",
            "1 - After forward pass 66560\n",
            "2 - Memory consumed by forward pass 40448\n",
            "3 - After backward pass 52736\n",
            "4 - After optimizer step 104960\n",
            "Iteration 1\n",
            "1 - After forward pass 144896\n",
            "2 - Memory consumed by forward pass 39936\n",
            "3 - After backward pass 104960\n",
            "4 - After optimizer step 104960\n",
            "Iteration 2\n",
            "1 - After forward pass 144896\n",
            "2 - Memory consumed by forward pass 39936\n",
            "3 - After backward pass 104960\n",
            "4 - After optimizer step 104960\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_memory(batch_size = 32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uGmD22sGFqCE",
        "outputId": "6a725dff-434f-42d1-cefd-009a52d84ed5"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of model parameters: 5858\n",
            "Beginning mem: 0 Note - this may be higher than 0, which is due to PyTorch caching. Don't worry too much about this number\n",
            "After model to device: 26112\n",
            "Iteration 0\n",
            "1 - After forward pass 47616\n",
            "2 - Memory consumed by forward pass 21504\n",
            "3 - After backward pass 52736\n",
            "4 - After optimizer step 104960\n",
            "Iteration 1\n",
            "1 - After forward pass 125952\n",
            "2 - Memory consumed by forward pass 20992\n",
            "3 - After backward pass 104960\n",
            "4 - After optimizer step 104960\n",
            "Iteration 2\n",
            "1 - After forward pass 125952\n",
            "2 - Memory consumed by forward pass 20992\n",
            "3 - After backward pass 104960\n",
            "4 - After optimizer step 104960\n"
          ]
        }
      ]
    }
  ]
}