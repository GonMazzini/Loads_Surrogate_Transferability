{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MemoryCUDA.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMhWgmEHTxRFsGWKCJYNP/C",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GonMazzini/Loads_Surrogate_Transferability/blob/main/MemoryCUDA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Memory in PyTorch\n",
        "# A basic notebook to understand the GPU allocated memory.\n",
        "\n",
        "#by GonMazzini     \n",
        "## https://github.com/GonMazzini"
      ],
      "metadata": {
        "id": "OWgcHmRU-no7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "FArhnT0y-Zl6"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Simple example with a torch tensor."
      ],
      "metadata": {
        "id": "NuhZcy-W-r9N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn(5000,5000, dtype = torch.float64).to('cuda')"
      ],
      "metadata": {
        "id": "oCb7SZiX-bb8"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "5000*5000*64/(1024*1024*8) # MB"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "obCYTfz3-1mJ",
        "outputId": "dd0c7513-bad3-45aa-a90f-144e51b36033"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "190.73486328125"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.get_device_name(device=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "XpjR8AQG_OzS",
        "outputId": "4572dfaa-238d-4ad2-beae-64dd4608364b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Tesla K80'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.memory_allocated()/(1024*1024)  # MB"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Bv-HeWb_TQW",
        "outputId": "c0cdcd23-7d7f-49cf-838d-1c673ed0e6ea"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "190.73486328125"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.memory_reserved()/(1024*1024) # MB"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LKWpIcpH_57M",
        "outputId": "efaaa6fa-40a1-4d35-d34c-c2f5e4460ba6"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "192.0"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example with a model"
      ],
      "metadata": {
        "id": "PLG2Zf76BaDS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "    \"\"\" A feedforward network designed for tuning number of layers and hidden units.\n",
        "    By @GonMazzini\"\"\"\n",
        "    def __init__(self, input_dim, output_dim, n_hidLayers, hidden_size):\n",
        "        super(MLP, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.hidden_size = hidden_size\n",
        "        self.n_hidLayers = n_hidLayers\n",
        "        current_dim = input_dim\n",
        "        self.layers = nn.ModuleList()\n",
        "        \n",
        "        for hdim in [self.hidden_size]*self.n_hidLayers:\n",
        "            self.layers.append(nn.Linear(current_dim, hdim))\n",
        "            current_dim = hdim\n",
        "        self.layers.append(nn.Linear(current_dim, output_dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.layers[:-1]:\n",
        "            x = F.relu(layer(x))\n",
        "        out = F.relu(self.layers[-1](x))\n",
        "        return out "
      ],
      "metadata": {
        "id": "4dPEcroSBc2D"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MLP(6,8,2,50)\n",
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m4TmHj3EBr-n",
        "outputId": "96b85c33-b4f4-4259-fe8a-28e3ff1df871"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLP(\n",
              "  (layers): ModuleList(\n",
              "    (0): Linear(in_features=6, out_features=50, bias=True)\n",
              "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
              "    (2): Linear(in_features=50, out_features=8, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "pytorch_total_params"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GnV_7u-YB1pj",
        "outputId": "99730b57-74b6-4610-86ba-c702511956ae"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3308"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn(1,pytorch_total_params, dtype = torch.float64).to('cuda')"
      ],
      "metadata": {
        "id": "PTZ9jxOeB8fd"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "1*3308*64/(1024*1024*8)  # MB"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WlpxSvxECKa-",
        "outputId": "89fea49c-e097-4f5c-a378-5746de89f7d0"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.025238037109375"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.memory_allocated(device)/(1024*1024)  # MB"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48RlUsNTB-wA",
        "outputId": "702f33a5-af0c-4f4d-fbee-acd81ec5cf49"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.025390625"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "in_size=6\n",
        "out_size=8\n",
        "hidden_size=50\n",
        "\n",
        "model = nn.Sequential(nn.Linear(in_size, hidden_size),\n",
        "                    *[nn.Linear(hidden_size, hidden_size) for _ in range(1)],\n",
        "                    nn.Linear(hidden_size, out_size))\n",
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bOI0ApABDpTY",
        "outputId": "1b3ef537-00a6-46bf-f004-0ff30cd62071"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Linear(in_features=6, out_features=50, bias=True)\n",
              "  (1): Linear(in_features=50, out_features=50, bias=True)\n",
              "  (2): Linear(in_features=50, out_features=8, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Xvz6C-MtEH1m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_memory(in_size=6, out_size=8, hidden_size=50, optimizer_type=torch.optim.Adam, batch_size=1, use_amp=False, device=0):\n",
        "    sample_input = torch.randn(batch_size, in_size, dtype=torch.float32)\n",
        "    model = nn.Sequential(nn.Linear(in_size, hidden_size),\n",
        "                        *[nn.Linear(hidden_size, hidden_size) for _ in range(2)],\n",
        "                        nn.Linear(hidden_size, out_size))\n",
        "    \n",
        "    pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print(f'model parameters: {pytorch_total_params}')\n",
        "\n",
        "    # max_mem_est = estimate_memory(model, sample_input[0], optimizer_type=optimizer_type, batch_size=batch_size, use_amp=use_amp)\n",
        "    # print(\"Maximum Memory Estimate\", max_mem_est)\n",
        "    optimizer = optimizer_type(model.parameters(), lr=.001)\n",
        "    print(\"Beginning mem:\", torch.cuda.memory_allocated(device), \"Note - this may be higher than 0, which is due to PyTorch caching. Don't worry too much about this number\")\n",
        "    model.to(device)\n",
        "    print(\"After model to device:\", torch.cuda.memory_allocated(device))\n",
        "    for i in range(3):\n",
        "        print(\"Iteration\", i)\n",
        "        with torch.cuda.amp.autocast(enabled=use_amp):\n",
        "            a = torch.cuda.memory_allocated(device)\n",
        "            out = model(sample_input.to(device)).sum() # Taking the sum here just to get a scalar output\n",
        "            b = torch.cuda.memory_allocated(device)\n",
        "        print(\"1 - After forward pass\", torch.cuda.memory_allocated(device))\n",
        "        print(\"2 - Memory consumed by forward pass\", b - a)\n",
        "        out.backward()\n",
        "        print(\"3 - After backward pass\", torch.cuda.memory_allocated(device))\n",
        "        optimizer.step()\n",
        "        print(\"4 - After optimizer step\", torch.cuda.memory_allocated(device))"
      ],
      "metadata": {
        "id": "d5e-TzdoBtEf"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " torch.cuda.memory_allocated(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dfV3lxmfDWJj",
        "outputId": "304827c3-4a9a-4106-d30a-24df0756b53f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_memory(batch_size = 64)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SHNC0i8KCmkP",
        "outputId": "1cbb5965-b861-42e6-bee8-fe66e47c57bc"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Beginning mem: 0 Note - this may be higher than 0, which is due to PyTorch caching. Don't worry too much about this number\n",
            "After model to device: 8237568\n",
            "Iteration 0\n",
            "1 - After forward pass 13409280\n",
            "2 - Memory consumed by forward pass 5171712\n",
            "3 - After backward pass 16475648\n",
            "4 - After optimizer step 32950784\n",
            "Iteration 1\n",
            "1 - After forward pass 38121984\n",
            "2 - Memory consumed by forward pass 5171200\n",
            "3 - After backward pass 32950784\n",
            "4 - After optimizer step 32950784\n",
            "Iteration 2\n",
            "1 - After forward pass 38121984\n",
            "2 - Memory consumed by forward pass 5171200\n",
            "3 - After backward pass 32950784\n",
            "4 - After optimizer step 32950784\n"
          ]
        }
      ]
    }
  ]
}