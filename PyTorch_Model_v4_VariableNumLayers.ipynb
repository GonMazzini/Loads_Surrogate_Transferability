{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GonMazzini/Loads_Surrogate_Transferability/blob/main/PyTorch_Model_v4_VariableNumLayers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9P0l_d61zRnQ"
      },
      "source": [
        "GridSearch:\n",
        "\n",
        "-This notebook uses a 3 point GridSearch for learning rate (0.1,0.01,0.001) and number of hidden units (16,256,512).\n",
        "-The objective was set to be the validation error (MSE).\n",
        "-The best result was obtained for 512 units and 0.01 lr."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install parameter-sherpa"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "l9b4V5nszS9-",
        "outputId": "a666dbd8-def5-4f50-caac-b99f4ab7d4f8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting parameter-sherpa\n",
            "  Downloading parameter-sherpa-1.0.6.tar.gz (513 kB)\n",
            "\u001b[K     |████████████████████████████████| 513 kB 5.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas>=0.20.3 in /usr/local/lib/python3.7/dist-packages (from parameter-sherpa) (1.3.5)\n",
            "Requirement already satisfied: pymongo>=3.5.1 in /usr/local/lib/python3.7/dist-packages (from parameter-sherpa) (4.0.1)\n",
            "Requirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python3.7/dist-packages (from parameter-sherpa) (1.21.5)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from parameter-sherpa) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from parameter-sherpa) (1.0.2)\n",
            "Requirement already satisfied: flask>=0.12.2 in /usr/local/lib/python3.7/dist-packages (from parameter-sherpa) (1.1.4)\n",
            "Collecting GPyOpt>=1.2.5\n",
            "  Downloading GPyOpt-1.2.6.tar.gz (56 kB)\n",
            "\u001b[K     |████████████████████████████████| 56 kB 4.0 MB/s \n",
            "\u001b[?25hCollecting enum34\n",
            "  Downloading enum34-1.1.10-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from parameter-sherpa) (3.2.2)\n",
            "Requirement already satisfied: click<8.0,>=5.1 in /usr/local/lib/python3.7/dist-packages (from flask>=0.12.2->parameter-sherpa) (7.1.2)\n",
            "Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from flask>=0.12.2->parameter-sherpa) (1.1.0)\n",
            "Requirement already satisfied: Werkzeug<2.0,>=0.15 in /usr/local/lib/python3.7/dist-packages (from flask>=0.12.2->parameter-sherpa) (1.0.1)\n",
            "Requirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from flask>=0.12.2->parameter-sherpa) (2.11.3)\n",
            "Collecting GPy>=1.8\n",
            "  Downloading GPy-1.10.0.tar.gz (959 kB)\n",
            "\u001b[K     |████████████████████████████████| 959 kB 42.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from GPy>=1.8->GPyOpt>=1.2.5->parameter-sherpa) (1.15.0)\n",
            "Collecting paramz>=0.9.0\n",
            "  Downloading paramz-0.9.5.tar.gz (71 kB)\n",
            "\u001b[K     |████████████████████████████████| 71 kB 7.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cython>=0.29 in /usr/local/lib/python3.7/dist-packages (from GPy>=1.8->GPyOpt>=1.2.5->parameter-sherpa) (0.29.28)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2<3.0,>=2.10.1->flask>=0.12.2->parameter-sherpa) (2.0.1)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.20.3->parameter-sherpa) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.20.3->parameter-sherpa) (2.8.2)\n",
            "Requirement already satisfied: decorator>=4.0.10 in /usr/local/lib/python3.7/dist-packages (from paramz>=0.9.0->GPy>=1.8->GPyOpt>=1.2.5->parameter-sherpa) (4.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.19.1->parameter-sherpa) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.19.1->parameter-sherpa) (1.1.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->parameter-sherpa) (3.0.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->parameter-sherpa) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->parameter-sherpa) (0.11.0)\n",
            "Building wheels for collected packages: parameter-sherpa, GPyOpt, GPy, paramz\n",
            "  Building wheel for parameter-sherpa (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for parameter-sherpa: filename=parameter_sherpa-1.0.6-py2.py3-none-any.whl size=542134 sha256=1e8f7c0f524b78ac18924f84bfbcabccf14d0de00db06ef0838159ffc752e790\n",
            "  Stored in directory: /root/.cache/pip/wheels/96/d9/cb/99569566e5e9b3ef0265ba4cbce3ff16f7692988833aa942f5\n",
            "  Building wheel for GPyOpt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for GPyOpt: filename=GPyOpt-1.2.6-py3-none-any.whl size=83609 sha256=4a59608e746d219b717e0ca8d0fd23f442d9d9e5efb7e0561250c6535fd0f590\n",
            "  Stored in directory: /root/.cache/pip/wheels/e6/fa/d1/f9652b5af79f769a0ab74dbead7c7aea9a93c6bc74543fd3ec\n",
            "  Building wheel for GPy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for GPy: filename=GPy-1.10.0-cp37-cp37m-linux_x86_64.whl size=2565056 sha256=890df7f1d2df9e4f1b51bf42494d4c51b3603f9844f92054a18ca9a65c057293\n",
            "  Stored in directory: /root/.cache/pip/wheels/f7/18/28/dd1ce0192a81b71a3b086fd952511d088b21e8359ea496860a\n",
            "  Building wheel for paramz (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for paramz: filename=paramz-0.9.5-py3-none-any.whl size=102566 sha256=8e22f9476a0f84323f6afc926b084b22a9308867fd49fee88d1df06dd2cf046a\n",
            "  Stored in directory: /root/.cache/pip/wheels/c8/95/f5/ce28482da28162e6028c4b3a32c41d147395825b3cd62bc810\n",
            "Successfully built parameter-sherpa GPyOpt GPy paramz\n",
            "Installing collected packages: paramz, GPy, GPyOpt, enum34, parameter-sherpa\n",
            "Successfully installed GPy-1.10.0 GPyOpt-1.2.6 enum34-1.1.10 parameter-sherpa-1.0.6 paramz-0.9.5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "enum"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "import sherpa\n",
        "from sherpa.algorithms import Genetic\n",
        "import time"
      ],
      "metadata": {
        "id": "Q2oZpZQIzoQT"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "nVCWJUJ2zRnS"
      },
      "outputs": [],
      "source": [
        "import pandas as pd \n",
        "import numpy as np\n",
        "import math\n",
        "from random import shuffle\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zm8YCN4AzRnT",
        "outputId": "2fdc1912-0f60-46b6-c6a4-27e717fc1abb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU\n"
          ]
        }
      ],
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda:0\")\n",
        "    print(\"GPU\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"CPU\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nF2rjMCtzRnU"
      },
      "source": [
        "# 0- Read the data as a data frame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "W1kffmHZzRnV",
        "outputId": "c67a9bbd-bd31-4ef4-f224-ef43a929e326"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-3af1bbc6-dbe8-4868-86d8-6ac50a853a59\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>pointno</th>\n",
              "      <th>U</th>\n",
              "      <th>SigmaU</th>\n",
              "      <th>Alpha</th>\n",
              "      <th>MannL</th>\n",
              "      <th>MannGamma</th>\n",
              "      <th>VeerDeltaPhi</th>\n",
              "      <th>TT_Mx_avg</th>\n",
              "      <th>TT_My_avg</th>\n",
              "      <th>TB_Mx_avg</th>\n",
              "      <th>TB_My_avg</th>\n",
              "      <th>TT_Mz_avg</th>\n",
              "      <th>MS_Mz_avg</th>\n",
              "      <th>BR_Mx_avg</th>\n",
              "      <th>BR_My_avg</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>-0.650000</td>\n",
              "      <td>7.500000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-22.250000</td>\n",
              "      <td>747.561872</td>\n",
              "      <td>200.666288</td>\n",
              "      <td>6708.717789</td>\n",
              "      <td>8861.885588</td>\n",
              "      <td>819.209904</td>\n",
              "      <td>63.457528</td>\n",
              "      <td>4253.317748</td>\n",
              "      <td>15006.726860</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>10.150758</td>\n",
              "      <td>1.208656</td>\n",
              "      <td>-0.139692</td>\n",
              "      <td>48.470634</td>\n",
              "      <td>1.363636</td>\n",
              "      <td>-4.771217</td>\n",
              "      <td>3556.031457</td>\n",
              "      <td>676.339081</td>\n",
              "      <td>16692.647572</td>\n",
              "      <td>6329.099515</td>\n",
              "      <td>3746.460605</td>\n",
              "      <td>1354.995442</td>\n",
              "      <td>10409.290476</td>\n",
              "      <td>16289.414152</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3af1bbc6-dbe8-4868-86d8-6ac50a853a59')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3af1bbc6-dbe8-4868-86d8-6ac50a853a59 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3af1bbc6-dbe8-4868-86d8-6ac50a853a59');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   Unnamed: 0  pointno          U    SigmaU     Alpha      MannL  MannGamma  \\\n",
              "0           0        1   4.000000  0.100000 -0.650000   7.500000   1.000000   \n",
              "1           1        2  10.150758  1.208656 -0.139692  48.470634   1.363636   \n",
              "\n",
              "   VeerDeltaPhi    TT_Mx_avg   TT_My_avg     TB_Mx_avg    TB_My_avg  \\\n",
              "0    -22.250000   747.561872  200.666288   6708.717789  8861.885588   \n",
              "1     -4.771217  3556.031457  676.339081  16692.647572  6329.099515   \n",
              "\n",
              "     TT_Mz_avg    MS_Mz_avg     BR_Mx_avg     BR_My_avg  \n",
              "0   819.209904    63.457528   4253.317748  15006.726860  \n",
              "1  3746.460605  1354.995442  10409.290476  16289.414152  "
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "df = pd.read_excel('LoadsDataBase_6D_Set123_FiltMinMaxCrit.xlsx') # Average the values from Set1,Set2 and Set3.\n",
        "df.head(2)\n",
        "# 0 : TT_Mx_avg\n",
        "# 1 : TT_My_avg\n",
        "# 2 : TB_Mx_avg\n",
        "# 3 : TB_My_avg\n",
        "# 4 : MS_Mz_avg\n",
        "# 5 : BR_Mx_avg\n",
        "# 6 : BR_My_avg\n",
        "# 7 : TT-Mz_avg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVi4tQ09zRnV"
      },
      "source": [
        "# ============================================================\n",
        "# ==========================Section 1 ==========================\n",
        "# ============================================================"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "gTxt7cZJzRnV"
      },
      "outputs": [],
      "source": [
        "X = df.iloc[:,2:8]\n",
        "y = df.iloc[:,8:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eX_LPh9wzRnW",
        "outputId": "b7de929e-93f3-4de9-da33-e1328109e3fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The filtered data set consits on: 7664 entries.\n",
            "A total of 6131 will be used for training and validation.\n",
            "A total of 1533 will be used for testing the final model.\n"
          ]
        }
      ],
      "source": [
        "# Test split:\n",
        "X, X_test, y, y_test = train_test_split(X,y, test_size = 0.2, shuffle = True,  random_state = 101)\n",
        "\n",
        "print(f'The filtered data set consits on: {len(df)} entries.')\n",
        "print(f'A total of {len(X)} will be used for training and validation.')\n",
        "print(f'A total of {len(X_test)} will be used for testing the final model.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6sjTHp4zRnW"
      },
      "source": [
        "### From now on, \"X\" and \"y\" will be used for train-validate the model. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "4nBJ9T1TzRnW"
      },
      "outputs": [],
      "source": [
        "feature_range = (0, 1)\n",
        "scaler_x = preprocessing.MinMaxScaler(feature_range=feature_range).fit(X)\n",
        "X_scaled = scaler_x.transform(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fp5EhbtozRnW"
      },
      "source": [
        "### Separte between train and validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "_kR4yaL0zRnX"
      },
      "outputs": [],
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(X_scaled,y.values, test_size = 0.2, shuffle = True,  random_state = 101)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# printing number of samples for train-validation-test\n",
        "print(f'A total of {y_train.shape[0]} for training, {round(100*y_train.shape[0]/len(df),1)} % of total data')\n",
        "print(f'A total of {y_val.shape[0]} for validation, {round(100*y_val.shape[0]/len(df),1)} % of total data')\n",
        "print(f'A total of {y_test.shape[0]} for testing, {round(100*y_test.shape[0]/len(df),1)} % of total data')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ezp2exU1NR36",
        "outputId": "760f59b6-8cb6-4831-b69f-d4f5453536f7"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A total of 4904 for training, 64.0 % of total data\n",
            "A total of 1227 for validation, 16.0 % of total data\n",
            "A total of 1533 for testing, 20.0 % of total data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Y6KSzGizRnX"
      },
      "source": [
        "### The PyTorch worfklow can be summarized as follows:\n",
        "- 1) Design model (input, output size, forward pass)\n",
        "- 2) Construct loss and optimizer\n",
        "- 3) Training loop\n",
        "   - forward pass: compute prediction based on the current weights and biases of the net\n",
        "   - backward pass: compute the gradients of the loss function wrt. to model parameters\n",
        "   - update weigths in"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "hVfzqEXbzRnX"
      },
      "outputs": [],
      "source": [
        "input_size = 6             # np.shape(X_train)[1]\n",
        "output_channels = 8        # np.shape(y_train)[1]\n",
        "hidden_size = 50"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim, output_dim, n_hidLayers, hidden_size):\n",
        "        super(MLP, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.hidden_size = hidden_size\n",
        "        self.n_hidLayers = n_hidLayers\n",
        "        current_dim = input_dim\n",
        "        self.layers = nn.ModuleList()\n",
        "        \n",
        "        for hdim in [self.hidden_size]*self.n_hidLayers:\n",
        "            self.layers.append(nn.Linear(current_dim, hdim))\n",
        "            current_dim = hdim\n",
        "        self.layers.append(nn.Linear(current_dim, output_dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.layers[:-1]:\n",
        "            x = F.relu(layer(x))\n",
        "        out = F.relu(self.layers[-1](x))\n",
        "        return out "
      ],
      "metadata": {
        "id": "QS60F1Q3vBYi"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test dummy forward\n",
        "testMLP = MLP(6,8,4,50)\n",
        "testMLP(torch.tensor(X_train[0]).float())"
      ],
      "metadata": {
        "id": "Hr0F9U6hxA1O"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "kQJG0CJ8zRnY"
      },
      "outputs": [],
      "source": [
        "# instantiate the class\n",
        "#model = Net(hidden_size)  # will be instantiated inside HP loop\n",
        "\n",
        "# Define the loss function (mean square error)\n",
        "loss = nn.MSELoss()\n",
        "\n",
        "# Instantiate optimizer passing the net parameters as argument, and learning rate\n",
        "#optimizer = optim.Adam(model.parameters(), lr = 0.01)  # will be instantiated inside HP loop \n",
        "\n",
        "# list to store results\n",
        "train_losses , val_losses= [],[]\n",
        "\n",
        "# Try a dummy forward\n",
        "#model(torch.tensor(X_train[0]).float())  # beware that need to convert from double to float"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xO8OBaiRzRnY"
      },
      "source": [
        "# 2.1- Use the PyTorch DataLoader and Dataset utils.\n",
        "- DataLoader class combines a dataset and a sampler, and provides an iterable over the given dataset for training the model\n",
        "- Dataset: just an abstract class representing a :class:`Dataset`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "guVFdZgMzRnY"
      },
      "outputs": [],
      "source": [
        "class FatigueLoads_TrainSet(Dataset):\n",
        "\n",
        "    def __init__(self):\n",
        "        self.n_samples = X_train.shape[0]\n",
        "        self.x_data = torch.from_numpy(X_train) # size [n_samples, n_features]\n",
        "        self.y_data = torch.from_numpy(y_train) # size [n_samples, 1]\n",
        "\n",
        "    # support indexing such that dataset[i] can be used to get i-th sample\n",
        "    def __getitem__(self, index):\n",
        "        return self.x_data[index], self.y_data[index]\n",
        "\n",
        "    # we can call len(dataset) to return the size\n",
        "    def __len__(self):\n",
        "        return self.n_samples\n",
        "    \n",
        "class FatigueLoads_ValidationSet(Dataset):\n",
        "\n",
        "    def __init__(self):\n",
        "        self.n_samples = X_val.shape[0]\n",
        "        self.x_data = torch.from_numpy(X_val) # size [n_samples, n_features]\n",
        "        self.y_data = torch.from_numpy(y_val) # size [n_samples, 1]\n",
        "\n",
        "    # support indexing such that dataset[i] can be used to get i-th sample\n",
        "    def __getitem__(self, index):\n",
        "        return self.x_data[index], self.y_data[index]\n",
        "\n",
        "    # we can call len(dataset) to return the size\n",
        "    def __len__(self):\n",
        "        return self.n_samples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sp46xHZVzRnZ"
      },
      "source": [
        "### Get first sample and unpack. Note that the enviromental inputs are normalized using MinMaxScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zALPQBILzRnZ",
        "outputId": "c10a94d1-165c-45a6-f855-6aa786a19081"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.9213, 0.5395, 0.2971, 0.1574, 0.7369, 0.3075], dtype=torch.float64) tensor([ 7116.1376,   808.7441, 21966.9989, 16383.7768,  7347.3186,   807.4632,\n",
            "        18567.6736, 16935.8620], dtype=torch.float64)\n"
          ]
        }
      ],
      "source": [
        "train_dataset = FatigueLoads_TrainSet()\n",
        "valid_dataset = FatigueLoads_ValidationSet()\n",
        "\n",
        "first_data = train_dataset[0]\n",
        "features, loads = first_data\n",
        "print(features, loads)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "323d_UnszRnZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2628d3cc-ee5a-489c-d9f5-0da3c112e484"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batches train: 9\n",
            "batches test:  2\n"
          ]
        }
      ],
      "source": [
        "batch_size = 512\n",
        "num_epochs = 100\n",
        "\n",
        "num_batches_train = X_train.shape[0] // batch_size\n",
        "num_batches_test = X_val.shape[0] // batch_size\n",
        "print(f'batches train: {num_batches_train}')\n",
        "print(f'batches test:  {num_batches_test}')\n",
        "\n",
        "train_loader = DataLoader(dataset=train_dataset,\n",
        "                          batch_size=batch_size,\n",
        "                          shuffle=True,\n",
        "                          num_workers=0)\n",
        "\n",
        "valid_loader = DataLoader(dataset=valid_dataset,\n",
        "                          batch_size=batch_size,\n",
        "                          shuffle=True,\n",
        "                          num_workers=0)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import repeat\n",
        "algorithm = sherpa.algorithms.GridSearch(num_grid_points=3)\n",
        "\n",
        "# algorithm = sherpa.algorithms.RandomSearch(max_num_trials=15)\n",
        "\n",
        "# parameters = [sherpa.Discrete('hidden_size', [16,512]),\n",
        "#               sherpa.Continuous('lr',[0.001,0.1], scale='log')]\n",
        "\n",
        "parameters = [sherpa.Discrete('n_hidLayers', [2, 4]),\n",
        "              sherpa.Discrete('hidden_size', [16, 64])]"
      ],
      "metadata": {
        "id": "5KXldvsU08MV"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "study = sherpa.Study(parameters=parameters,\n",
        "                     algorithm=algorithm,\n",
        "                     lower_is_better=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXK9eWS81rHZ",
        "outputId": "ccf011ef-be24-4e6b-bfb4-ff95f6069992"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:sherpa.core:\n",
            "-------------------------------------------------------\n",
            "SHERPA Dashboard running. Access via\n",
            "http://172.28.0.2:8883 if on a cluster or\n",
            "http://localhost:8883 if running locally.\n",
            "-------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Serving Flask app \"sherpa.app.app\" (lazy loading)\n",
            " * Environment: production\n",
            "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
            "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
            " * Debug mode: on\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for trial in study:\n",
        "  print(\"Trial {}:\\t{}\".format(trial.id, trial.parameters))\n",
        "  # model = Net(trial.parameters['hidden_size'])\n",
        "  model=MLP(input_dim=6, \n",
        "            output_dim=8,\n",
        "            n_hidLayers=trial.parameters['n_hidLayers'],\n",
        "            hidden_size=trial.parameters['hidden_size'])    \n",
        "  #model.train()\n",
        "  model.to(device)\n",
        "  optimizer = optim.Adam(model.parameters(), lr = 0.01) # lr = trial.parameters['lr']\n",
        "  \n",
        "  for epoch in range(num_epochs):\n",
        "\n",
        "    epoch_loss = 0\n",
        "    model.train()\n",
        "    \n",
        "    for i, (inputs, loads) in enumerate(train_loader):\n",
        "        #print(features, loads)\n",
        "        \n",
        "        optimizer.zero_grad()                      # zeroize accumulated gradients in parameters             \n",
        "        \n",
        "        output = model(inputs.float().to(device))             # forwards pass       \n",
        "        batch_loss = loss(output, loads.float().to(device))   # compute loss for current batch\n",
        "        \n",
        "        batch_loss.backward()                      # compute the gradient of the loss wrt. model parameters\n",
        "        optimizer.step()                           # update weights according to the comptued gradients\n",
        "        \n",
        "    \n",
        "    epoch_loss_train = 0\n",
        "    epoch_loss_test = 0\n",
        "    model.eval()\n",
        "    \n",
        "    ##### Evaluate training\n",
        "    for i, (inputs, loads) in enumerate(train_loader):\n",
        "        \n",
        "        output = model(inputs.float().to(device))\n",
        "        \n",
        "        batch_loss_train = loss(output, loads.float().to(device))  # compute loss for the current batch\n",
        "        epoch_loss_train += batch_loss_train            # accumulate loss for the current epoch\n",
        "        \n",
        "        #print(f'Epoch: {epoch+1}/{num_epochs}  | Step {i+1}/{n_iterations}')\n",
        "    \n",
        "    ##### Evaluate validation    \n",
        "    for i, (inputs, loads) in enumerate(valid_loader):\n",
        "        \n",
        "        output = model(inputs.float().to(device))\n",
        "        \n",
        "        batch_loss_test = loss(output, loads.float().to(device))  # compute loss for the current batch\n",
        "        epoch_loss_test += batch_loss_test     # accumulate loss for the current epoch\n",
        "        \n",
        "        #print(f'Epoch: {epoch+1}/{num_epochs}  | Step {i+1}/{n_iterations}')\n",
        "    \n",
        "    if epoch % 20 == 0: \n",
        "        print(f'Epoch: {epoch+1}/{num_epochs} | Train loss: {0.001*epoch_loss_train/num_batches_train:.0f}       | Val loss {0.001*epoch_loss_test/num_batches_test:.0f}')\n",
        "\n",
        "        study.add_observation(trial=trial,\n",
        "                              iteration=epoch,\n",
        "                              objective=0.001*epoch_loss_test.cpu().detach().numpy()/num_batches_test)\n",
        "        \n",
        "    if study.should_trial_stop(trial):\n",
        "        break \n",
        "    # store in list for plotting the loss per epoch    \n",
        "    val_losses.append(0.001*epoch_loss_test.cpu().detach().numpy()/num_batches_test)  \n",
        "    train_losses.append(0.001*epoch_loss_train.cpu().detach().numpy()/num_batches_train)  \n",
        "  study.finalize(trial) \n",
        "\n",
        "  study.get_best_result()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GnOKI0762M9u",
        "outputId": "10003402-130b-4db7-de9e-dfe490a98cde"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 1:\t{'hidden_size': 16, 'n_hidLayers': 2}\n",
            "Epoch: 1/100 | Train loss: 205243       | Val loss 277206\n",
            "Epoch: 21/100 | Train loss: 134218       | Val loss 181991\n",
            "Epoch: 41/100 | Train loss: 123884       | Val loss 173445\n",
            "Epoch: 61/100 | Train loss: 123402       | Val loss 168850\n",
            "Epoch: 81/100 | Train loss: 122353       | Val loss 166193\n",
            "Trial 2:\t{'hidden_size': 16, 'n_hidLayers': 3}\n",
            "Epoch: 1/100 | Train loss: 205371       | Val loss 280380\n",
            "Epoch: 21/100 | Train loss: 63271       | Val loss 85990\n",
            "Epoch: 41/100 | Train loss: 51255       | Val loss 69289\n",
            "Epoch: 61/100 | Train loss: 50683       | Val loss 67741\n",
            "Epoch: 81/100 | Train loss: 50613       | Val loss 67963\n",
            "Trial 3:\t{'hidden_size': 16, 'n_hidLayers': 4}\n",
            "Epoch: 1/100 | Train loss: 205389       | Val loss 273660\n",
            "Epoch: 21/100 | Train loss: 62503       | Val loss 83379\n",
            "Epoch: 41/100 | Train loss: 56330       | Val loss 75322\n",
            "Epoch: 61/100 | Train loss: 56262       | Val loss 74865\n",
            "Epoch: 81/100 | Train loss: 55839       | Val loss 74378\n",
            "Trial 4:\t{'hidden_size': 40, 'n_hidLayers': 2}\n",
            "Epoch: 1/100 | Train loss: 205333       | Val loss 281878\n",
            "Epoch: 21/100 | Train loss: 160233       | Val loss 219994\n",
            "Epoch: 41/100 | Train loss: 160238       | Val loss 220265\n",
            "Epoch: 61/100 | Train loss: 160158       | Val loss 214947\n",
            "Epoch: 81/100 | Train loss: 159926       | Val loss 221305\n",
            "Trial 5:\t{'hidden_size': 40, 'n_hidLayers': 3}\n",
            "Epoch: 1/100 | Train loss: 204011       | Val loss 277760\n",
            "Epoch: 21/100 | Train loss: 138665       | Val loss 185729\n",
            "Epoch: 41/100 | Train loss: 137855       | Val loss 187389\n",
            "Epoch: 61/100 | Train loss: 138236       | Val loss 184500\n",
            "Epoch: 81/100 | Train loss: 138292       | Val loss 188434\n",
            "Trial 6:\t{'hidden_size': 40, 'n_hidLayers': 4}\n",
            "Epoch: 1/100 | Train loss: 205247       | Val loss 278193\n",
            "Epoch: 21/100 | Train loss: 137618       | Val loss 185563\n",
            "Epoch: 41/100 | Train loss: 137885       | Val loss 187150\n",
            "Epoch: 61/100 | Train loss: 136735       | Val loss 183347\n",
            "Epoch: 81/100 | Train loss: 136939       | Val loss 183913\n",
            "Trial 7:\t{'hidden_size': 64, 'n_hidLayers': 2}\n",
            "Epoch: 1/100 | Train loss: 205396       | Val loss 278617\n",
            "Epoch: 21/100 | Train loss: 69638       | Val loss 94451\n",
            "Epoch: 41/100 | Train loss: 61902       | Val loss 81722\n",
            "Epoch: 61/100 | Train loss: 57198       | Val loss 76874\n",
            "Epoch: 81/100 | Train loss: 56311       | Val loss 75218\n",
            "Trial 8:\t{'hidden_size': 64, 'n_hidLayers': 3}\n",
            "Epoch: 1/100 | Train loss: 205085       | Val loss 274122\n",
            "Epoch: 21/100 | Train loss: 58963       | Val loss 80028\n",
            "Epoch: 41/100 | Train loss: 57772       | Val loss 77146\n",
            "Epoch: 61/100 | Train loss: 57234       | Val loss 76949\n",
            "Epoch: 81/100 | Train loss: 56562       | Val loss 75572\n",
            "Trial 9:\t{'hidden_size': 64, 'n_hidLayers': 4}\n",
            "Epoch: 1/100 | Train loss: 203912       | Val loss 279166\n",
            "Epoch: 21/100 | Train loss: 19201       | Val loss 25962\n",
            "Epoch: 41/100 | Train loss: 18587       | Val loss 24657\n",
            "Epoch: 61/100 | Train loss: 18037       | Val loss 24439\n",
            "Epoch: 81/100 | Train loss: 17930       | Val loss 24603\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = 21.38242742\n",
        "print(f'{a:.0f}')"
      ],
      "metadata": {
        "id": "rpdUjrJQ0lcJ",
        "outputId": "9b0be09e-62c7-4fcc-eaf0-2e623c016f75",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "21\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "study.get_best_result()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mzhkLlRv29u0",
        "outputId": "68fdaaf5-3098-481d-9e97-d11be29d4f96"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Iteration': 80,\n",
              " 'Objective': 20207.992000000002,\n",
              " 'Trial-ID': 2,\n",
              " 'hidden_size': 16,\n",
              " 'n_hidLayers': 4}"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "study.get_best_result()"
      ],
      "metadata": {
        "id": "CSx8TyoulAS0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "study.results.Objective.describe()"
      ],
      "metadata": {
        "id": "3TJI1oV0jZnd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "study.get_best_result()"
      ],
      "metadata": {
        "id": "z_uF07KUG5uX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "study.results"
      ],
      "metadata": {
        "id": "yrrA661lhyJW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "diZ_MjPfzRna"
      },
      "source": [
        "# 3.1- Start training loop "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f6-pIKVRzRna"
      },
      "outputs": [],
      "source": [
        "for epoch in range(num_epochs):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    model.train()\n",
        "    \n",
        "    for i, (inputs, loads) in enumerate(train_loader):\n",
        "        #print(features, loads)\n",
        "        \n",
        "        optimizer.zero_grad()                      # zeroize accumulated gradients in parameters             \n",
        "        \n",
        "        output = model(inputs.float())             # forwards pass       \n",
        "        batch_loss = loss(output, loads.float())   # compute loss for current batch\n",
        "        \n",
        "        batch_loss.backward()                      # compute the gradient of the loss wrt. model parameters\n",
        "        optimizer.step()                           # update weights according to the comptued gradients\n",
        "        \n",
        "    \n",
        "    epoch_loss_train = 0\n",
        "    epoch_loss_test = 0\n",
        "    model.eval()\n",
        "    \n",
        "    ##### Evaluate training\n",
        "    for i, (inputs, loads) in enumerate(train_loader):\n",
        "        \n",
        "        output = model(inputs.float())\n",
        "        \n",
        "        batch_loss_train = loss(output, loads.float())  # compute loss for the current batch\n",
        "        epoch_loss_train += batch_loss_train            # accumulate loss for the current epoch\n",
        "        \n",
        "        #print(f'Epoch: {epoch+1}/{num_epochs}  | Step {i+1}/{n_iterations}')\n",
        "    \n",
        "    ##### Evaluate validation    \n",
        "    for i, (inputs, loads) in enumerate(valid_loader):\n",
        "        \n",
        "        output = model(inputs.float())\n",
        "        \n",
        "        batch_loss_test = loss(output, loads.float())  # compute loss for the current batch\n",
        "        epoch_loss_test += batch_loss_test     # accumulate loss for the current epoch\n",
        "        \n",
        "        #print(f'Epoch: {epoch+1}/{num_epochs}  | Step {i+1}/{n_iterations}')\n",
        "    \n",
        "    if epoch % 100 == 0: \n",
        "        print(f'Epoch: {epoch+1}/{num_epochs} | Train loss: {epoch_loss_train/num_batches_train}       | Val loss {epoch_loss_test/num_batches_test}')\n",
        "        \n",
        "    # store in list for plotting the loss per epoch    \n",
        "    val_losses.append(epoch_loss_test/num_batches_test)  \n",
        "    train_losses.append(epoch_loss_train/num_batches_train)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vgCFr4fIzRna"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize = (8,6))\n",
        "plt.plot(train_losses[50:])\n",
        "plt.plot(val_losses[50:], '-k')\n",
        "plt.title('Training-Validation loss', fontsize = 16)\n",
        "plt.grid()\n",
        "plt.legend(['Training loss','Validation loss'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSkpKZ2HzRna"
      },
      "source": [
        "# Evalute on the test data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6lcj4lLzRnb"
      },
      "source": [
        "### Scale the test data before evaluating the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e-TPXw0rzRnb"
      },
      "outputs": [],
      "source": [
        "Test_scaler_x = preprocessing.MinMaxScaler(feature_range=feature_range).fit(X_test) # maybe try another for X?\n",
        "X_Test_scaled = Test_scaler_x.transform(X_test)\n",
        "\n",
        "predictions = model(torch.tensor(X_Test_scaled).float()).detach().numpy()\n",
        "test_targets = y_test.values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ySZ9om3tzRnb"
      },
      "outputs": [],
      "source": [
        "for idx, ch in enumerate(df.columns.tolist()[8:]):\n",
        "  # issues with 2 and 7\n",
        "    plt.figure(idx, figsize = (8,6))\n",
        "    sns.scatterplot(predictions[:,idx],test_targets[:,idx])\n",
        "    plt.title(f'DELs: {ch}', fontsize = 20)\n",
        "    plt.xlabel('Predictions [kNm]', fontsize = 12)\n",
        "    plt.ylabel('Targets [kNm]')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Py30VR4zRnb"
      },
      "outputs": [],
      "source": [
        "mse_list = []\n",
        "r2_score_list = []\n",
        "for i in range(len(df.columns.tolist()[8:])):\n",
        "    #print(f'MSE {AllTargetData.columns[i]} Channel : \\n {mean_squared_error(AllTargetData.values[:,i], Yout[:,i])}')\n",
        "    mse_list.append(mean_squared_error(test_targets[:,i], predictions[:,i]))\n",
        "    r2_score_list.append(r2_score(test_targets[:,i], predictions[:,i]))\n",
        " \n",
        "\n",
        " # Compute the normalized mean square error:\n",
        "Norm_RMSE = np.sqrt(np.array(mse_list)) / y_test.describe().loc['mean'].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0MOqWtwxzRnb"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize= (14,4)) \n",
        "sns.barplot(x= df.columns.tolist()[8:], y= r2_score_list) \n",
        "plt.ylim([min(r2_score_list)*0.92,max(r2_score_list)*1.02])\n",
        "plt.ylabel('R2 score', fontsize = 20)\n",
        "plt.grid()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZR2NNMLYzRnc"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize= (10,4)) \n",
        "sns.barplot(x= df.columns.tolist()[8:], y= Norm_RMSE) \n",
        "plt.ylim([0,max(Norm_RMSE)*1.02])\n",
        "plt.ylabel('Normalized Root Mean Square Error', fontsize = 20)\n",
        "plt.grid()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7bJWGF9zRnc"
      },
      "source": [
        "# ============================================================\n",
        "# ==========================Section 2 ==========================\n",
        "# ============================================================\n",
        "### Compare versus the wind2loads neural net"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HdQbK_iozRnc"
      },
      "outputs": [],
      "source": [
        "error on purpose"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kvVzwFAqzRnc"
      },
      "outputs": [],
      "source": [
        "from w2l import neuralnets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qz34TBv6zRnc"
      },
      "outputs": [],
      "source": [
        "# define the model using the same net architecture, and train for the same number of epochs using the previously batch size as well.\n",
        "w2l_net = neuralnets.ann(layersizes = [input_size, num_hid_1, num_hid_2, output_channels],\n",
        "                       params = {'minibatchsize':64, 'nepochs':500}, \n",
        "                       output_style = 'None',\n",
        "                        testratios = [0.7, 0.3, 0.])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8f_LQU-gzRnd"
      },
      "outputs": [],
      "source": [
        "# train using the data from the first split\n",
        "Outdata = w2l_net.train(X.values,y.values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zKsFN84lzRnd"
      },
      "outputs": [],
      "source": [
        "w2l_net.params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8oSktNjXzRnd"
      },
      "outputs": [],
      "source": [
        "Outdata.keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4JWEI6izRnd"
      },
      "source": [
        "$\\color{red}{\\text{Why 500 epochs tho}}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nsafzKKMzRnd"
      },
      "outputs": [],
      "source": [
        "plt.plot(Outdata['Jhist'][50:])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k75YNlcxzRnd"
      },
      "outputs": [],
      "source": [
        "Yout = w2l_net.predict(X_test.values) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R5m1sXI1zRne"
      },
      "outputs": [],
      "source": [
        "for idx, ch in enumerate(df.columns.tolist()[8:]):\n",
        "  # issues with 2 and 7\n",
        "    plt.figure(idx, figsize = (8,6))\n",
        "    sns.scatterplot(Yout[:,idx],test_targets[:,idx])\n",
        "    sns.scatterplot(predictions[:,idx],test_targets[:,idx])\n",
        "    plt.title(f'DELs: {ch}', fontsize = 20)\n",
        "    plt.legend(['W2L','PyTorch'])\n",
        "    plt.xlabel('Predictions [kNm]', fontsize = 12)\n",
        "    plt.ylabel('Targets [kNm]')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uGzKSU3dzRne"
      },
      "outputs": [],
      "source": [
        "W2L_mse_list = []\n",
        "W2L_r2_score_list = []\n",
        "for i in range(len(df.columns.tolist()[8:])):\n",
        "    #print(f'MSE {AllTargetData.columns[i]} Channel : \\n {mean_squared_error(AllTargetData.values[:,i], Yout[:,i])}')\n",
        "    W2L_mse_list.append(mean_squared_error(test_targets[:,i], Yout[:,i]))\n",
        "    W2L_r2_score_list.append(r2_score(test_targets[:,i], Yout[:,i]))\n",
        "    \n",
        "W2L_Norm_RMSE = np.sqrt(np.array(W2L_mse_list)) / y_test.describe().loc['mean'].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OWxxa9MSzRne"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize= (12,6)) \n",
        "sns.barplot(x= df.columns.tolist()[8:], y= W2L_r2_score_list) \n",
        "plt.ylim([min(W2L_r2_score_list)*0.92,max(W2L_r2_score_list)*1.02])\n",
        "plt.ylabel('R2 score', fontsize = 20)\n",
        "plt.grid()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mczhs3xmzRne"
      },
      "source": [
        "# Compare Wind2Loads net versus PyTorch implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5k0G9KK1zRne"
      },
      "outputs": [],
      "source": [
        "barplot_lst = []\n",
        "for i,ch in enumerate(df.columns.tolist()[8:]):\n",
        "    barplot_lst.append(['pytorch',ch,r2_score_list[i]])\n",
        "for i,ch in enumerate(df.columns.tolist()[8:]):\n",
        "    barplot_lst.append(['W2L',ch,W2L_r2_score_list[i]])  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zCZCFJffzRne"
      },
      "outputs": [],
      "source": [
        "df_comparison = pd.DataFrame(barplot_lst,\n",
        "                  columns=['Model','channel','r2'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IIlsLG7FzRne"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12,6))\n",
        "df_comparison.pivot(\"channel\", \"Model\", \"r2\").plot(kind='bar')\n",
        "plt.ylim([min(r2_score_list)*0.98,max(r2_score_list)*1.02])\n",
        "plt.title('R2', fontsize = 18)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VA-jwSfqzRnf"
      },
      "outputs": [],
      "source": [
        "barplot_lst_NMSE = []\n",
        "for i,ch in enumerate(df.columns.tolist()[8:]):\n",
        "    barplot_lst_NMSE.append(['pytorch',ch , Norm_RMSE[i]])\n",
        "for i,ch in enumerate(df.columns.tolist()[8:]):\n",
        "    barplot_lst_NMSE.append(['W2L', ch, W2L_Norm_RMSE[i]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qsiebtcqzRnf"
      },
      "outputs": [],
      "source": [
        "df_NRMSE_comparison = pd.DataFrame(barplot_lst_NMSE,\n",
        "                  columns=['Model','channel','NRMSE'])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kZT_Jd4uzRnf"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(14,6))\n",
        "df_NRMSE_comparison.pivot(\"channel\", \"Model\", \"NRMSE\").plot(kind='bar')\n",
        "plt.ylim([0,max(W2L_Norm_RMSE)*1.02])\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "PyTorch_Model_v4_Sherpa.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "g6lcj4lLzRnb"
      ],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}